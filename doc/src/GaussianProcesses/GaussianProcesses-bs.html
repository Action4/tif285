<!--
Automatically generated HTML file from DocOnce source
(https://github.com/hplgit/doconce/)
-->
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="DocOnce: https://github.com/hplgit/doconce/" />
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<meta name="description" content="Learning from data: Gaussian processes">

<title>Learning from data: Gaussian processes</title>

<!-- Bootstrap style: bootstrap -->
<link href="https://netdna.bootstrapcdn.com/bootstrap/3.1.1/css/bootstrap.min.css" rel="stylesheet">
<!-- not necessary
<link href="https://netdna.bootstrapcdn.com/font-awesome/4.0.3/css/font-awesome.css" rel="stylesheet">
-->

<style type="text/css">

/* Add scrollbar to dropdown menus in bootstrap navigation bar */
.dropdown-menu {
   height: auto;
   max-height: 400px;
   overflow-x: hidden;
}

/* Adds an invisible element before each target to offset for the navigation
   bar */
.anchor::before {
  content:"";
  display:block;
  height:50px;      /* fixed header height for style bootstrap */
  margin:-50px 0 0; /* negative fixed header height */
}
</style>


</head>

<!-- tocinfo
{'highest level': 2,
 'sections': [('Inference using Gaussian processes', 2, None, '___sec0'),
              ('References:', 3, None, '___sec1'),
              ('Parametric approach', 2, None, '___sec2'),
              ('Proof', 3, None, '___sec3'),
              ('The covariance matrix as the central object',
               3,
               None,
               '___sec4'),
              ('Non-parametric approach: Mean and covariance functions',
               2,
               None,
               '___sec5'),
              ('Stationary kernels', 3, None, '___sec6'),
              ('GP models for regression', 2, None, '___sec7'),
              ('Optimizing the GP model hyperparameters', 3, None, '___sec8'),
              ('GP emulators', 2, None, '___sec9')]}
end of tocinfo -->

<body>



<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: {
     equationNumbers: {  autoNumber: "AMS"  },
     extensions: ["AMSmath.js", "AMSsymbols.js", "autobold.js", "color.js"]
  }
});
</script>
<script type="text/javascript" async
 src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>



    
<!-- Bootstrap navigation bar -->
<div class="navbar navbar-default navbar-fixed-top">
  <div class="navbar-header">
    <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-responsive-collapse">
      <span class="icon-bar"></span>
      <span class="icon-bar"></span>
      <span class="icon-bar"></span>
    </button>
    <a class="navbar-brand" href="GaussianProcesses-bs.html">Learning from data: Gaussian processes</a>
  </div>

  <div class="navbar-collapse collapse navbar-responsive-collapse">
    <ul class="nav navbar-nav navbar-right">
      <li class="dropdown">
        <a href="#" class="dropdown-toggle" data-toggle="dropdown">Contents <b class="caret"></b></a>
        <ul class="dropdown-menu">
     <!-- navigation toc: --> <li><a href="#___sec0" style="font-size: 80%;"><b>Inference using Gaussian processes</b></a></li>
     <!-- navigation toc: --> <li><a href="#___sec1" style="font-size: 80%;">&nbsp;&nbsp;&nbsp;References:</a></li>
     <!-- navigation toc: --> <li><a href="#___sec2" style="font-size: 80%;"><b>Parametric approach</b></a></li>
     <!-- navigation toc: --> <li><a href="#___sec3" style="font-size: 80%;">&nbsp;&nbsp;&nbsp;Proof</a></li>
     <!-- navigation toc: --> <li><a href="#___sec4" style="font-size: 80%;">&nbsp;&nbsp;&nbsp;The covariance matrix as the central object</a></li>
     <!-- navigation toc: --> <li><a href="#___sec5" style="font-size: 80%;"><b>Non-parametric approach: Mean and covariance functions</b></a></li>
     <!-- navigation toc: --> <li><a href="#___sec6" style="font-size: 80%;">&nbsp;&nbsp;&nbsp;Stationary kernels</a></li>
     <!-- navigation toc: --> <li><a href="#___sec7" style="font-size: 80%;"><b>GP models for regression</b></a></li>
     <!-- navigation toc: --> <li><a href="#___sec8" style="font-size: 80%;">&nbsp;&nbsp;&nbsp;Optimizing the GP model hyperparameters</a></li>
     <!-- navigation toc: --> <li><a href="#___sec9" style="font-size: 80%;"><b>GP emulators</b></a></li>

        </ul>
      </li>
    </ul>
  </div>
</div>
</div> <!-- end of navigation bar -->

<div class="container">

<p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p> <!-- add vertical space -->

<!-- ------------------- main content ---------------------- -->



<div class="jumbotron">
<center><h1>Learning from data: Gaussian processes</h1></center>  <!-- document title -->

<p>
<!-- author(s): Christian Forss&#233;n -->

<center>
<b>Christian Forss&#233;n</b> 
</center>

<p>
<!-- institution -->

<center><b>Department of Physics, Chalmers University of Technology, Sweden</b></center>
<br>
<p>
<center><h4>Oct 7, 2019</h4></center> <!-- date -->
<br>
<p>
<!-- potential-jumbotron-button -->
</div> <!-- end jumbotron -->

<!-- !split -->

<h2 id="___sec0" class="anchor">Inference using Gaussian processes </h2>

<p>
Assume that there is a set of input vectors with independent, predictor, variables
$$ \boldsymbol{X}_N \equiv \{ \boldsymbol{x}^{(n)}\}_{n=1}^N $$

and a set of target values
$$ \boldsymbol{t}_N \equiv \{ t^{(n)}\}_{n=1}^N. $$


<ul>
<li> Note that we will use the symbol \( t \) to denote the target, or response, variables in the context of Gaussian Processes.</li> 
<li> Furthermore, we will use the subscript \( N \) to denote a vector of \( N \) vectors (or scalars): \( \boldsymbol{X}_N \) (\( \boldsymbol{t}_N \))</li>
<li> While a single instance \( i \) is denoted by a superscript: \( \boldsymbol{x}^{(i)} \) (\( t^{(i)} \)).</li>
</ul>

<!-- !split -->
We will consider two different <em>inference problems</em>:

<ol>
<li> The prediction of <em>new target</em> \( t^{(N+1)} \) given a new input \( \boldsymbol{x}^{(N+1)} \)</li>
<li> The inference of a <em>function</em> \( y(\boldsymbol{x}) \) from the data.</li>
</ol>

<!-- !split -->
The former can be expressed with the pdf
$$ 
p\left( t^{(N+1)} | \boldsymbol{t}_N, \boldsymbol{X}_{N}, \boldsymbol{x}^{(N+1)} \right)
$$

while the latter can be written using Bayes' formula (in these notes we will not be including information \( I \) explicitly in the conditional probabilities)
$$ p\left( y(\boldsymbol{x}) | \boldsymbol{t}_N, \boldsymbol{X}_N \right)
= \frac{p\left( \boldsymbol{t}_N | y(\boldsymbol{x}), \boldsymbol{X}_N \right) p \left( y(\boldsymbol{x}) \right) }
{p\left( \boldsymbol{t}_N | \boldsymbol{X}_N \right) } $$

<p>
<!-- !split -->
The inference of a function will obviously also allow to make predictions for new targets. 
However, we will need to consider in particular the second term in the numerator, which is the <b>prior</b> distribution on functions assumed in the model.

<ul>
<li> This prior is implicit in parametric models with priors on the parameters.</li>
<li> The idea of Gaussian process modeling is to put a prior directly on the <b>space of functions</b> without parameterizing \( y(\boldsymbol{x}) \).</li>
<li> A Gaussian process can be thought of as a generalization of a Gaussian distribution over a finite vector space to a <b>function space of infinite dimension</b>.</li>
<li> Just as a Gaussian distribution is specified by its mean and covariance matrix, a Gaussian process is specified by a <b>mean and covariance function</b>.</li>
</ul>

<!-- !split -->
<div class="panel panel-primary">
  <div class="panel-heading">
  <h3 class="panel-title">Gaussian process</h3>
  </div>
<div class="panel-body">
<p> <!-- subsequent paragraphs come in larger fonts, so start with a paragraph -->
A Gaussian process is a stochastic process (a collection of random variables indexed by time or space), such that every finite collection of those random variables has a multivariate normal distribution
</div>
</div>


<p>
<!-- !split -->

<h3 id="___sec1" class="anchor">References: </h3>

<ol>
<li> <a href="http://www.gaussianprocess.org/gpml" target="_self">Gaussian Processes for Machine Learning</a>, Carl Edward Rasmussen and Chris Williams, the MIT Press, 2006, <a href="http://www.gaussianprocess.org/gpml/chapters" target="_self">online version</a>.</li>
<li> <a href="https://sheffieldml.github.io/GPy/" target="_self">GPy</a>: a Gaussian Process (GP) framework written in python, from the Sheffield machine learning group.</li>
</ol>

<!-- !split -->

<h2 id="___sec2" class="anchor">Parametric approach </h2>

<p>
Let us express \( y(\boldsymbol{x}) \) in terms of a model function \( y(\boldsymbol{x}; \boldsymbol{\theta}) \) that depends on a vector of model parameters \( \boldsymbol{\theta} \).

<p>
For example, using a set of basis functions \( \left\{ \phi^{(h)} (\boldsymbol{x}) \right\}_{h=1}^H \) with linear weights \( \boldsymbol{\theta}_H \) we have
$$
y (\boldsymbol{x}, \boldsymbol{\theta}) = \sum_{h=1}^H \theta^{(h)} \phi^{(h)} (\boldsymbol{x})
$$

<p>
<div class="panel panel-primary">
  <div class="panel-heading">
  <h3 class="panel-title">Notice</h3>
  </div>
<div class="panel-body">
<p> <!-- subsequent paragraphs come in larger fonts, so start with a paragraph -->
The basis functions can be non-linear such as Gaussians (aka <em>radial basis functions</em>)
$$
\phi^{(h)} (\boldsymbol{x}) = \exp \left[ -\frac{\left( \boldsymbol{x} - \boldsymbol{c}^{(h)} \right)^2}{2 (\sigma^{(h)})^2} \right].
$$

<p>
Still, this constitutes a linear model since \( y (\boldsymbol{x}, \boldsymbol{\theta}) \) depends linearly on the parameters \( \boldsymbol{\theta} \).
</div>
</div>


<p>
The inference of model parameters should be a well-known problem by now. We state it in terms of Bayes theorem
$$
p \left( \boldsymbol{\theta} | \boldsymbol{t}_N, \boldsymbol{X}_N \right)
= \frac{ p \left( \boldsymbol{t}_N | \boldsymbol{\theta}, \boldsymbol{X}_N \right) p \left( \boldsymbol{\theta} \right)}{p \left( \boldsymbol{t}_N | \boldsymbol{X}_N \right)}
$$

<p>
Having solved this inference problem (e.g. by linear regression) a prediction can be made through marginalization
$$
p\left( t^{(N+1)} | \boldsymbol{t}_N, \boldsymbol{X}_{N}, \boldsymbol{x}^{(N+1)} \right) 
= \int d^H \boldsymbol{\theta} 
p\left( t^{(N+1)} | \boldsymbol{\theta}, \boldsymbol{x}^{(N+1)} \right)
p \left( \boldsymbol{\theta} | \boldsymbol{t}_N, \boldsymbol{X}_N \right).
$$

Here it is important to note that the final answer does not make any explicit reference to our parametric representation of the unknown function \( y(\boldsymbol{x}) \).

<p>
Assuming that we have a fixed set of basis functions and Gaussian prior distributions (with zero mean) on the weights \( \boldsymbol{\theta} \) we will show that:

<ul>
<li> The joint pdf of the observed data given the model \( p( \boldsymbol{t}_N |  \boldsymbol{X}_N) \), is a multivariate Gaussian with mean zero and with a covariance matrix that is determined by the basis functions.</li>
<li> This implies that the conditional distribution \( p( t^{(N+1)} | \boldsymbol{t}_N, \boldsymbol{X}_{N+1}) \), is also a multivariate Gaussian whose mean depends linearly on \( \boldsymbol{t}_N \).</li>
</ul>

<h3 id="___sec3" class="anchor">Proof </h3>

To be added.

<h3 id="___sec4" class="anchor">The covariance matrix as the central object </h3>

To be added.

<p>
<!-- !split -->

<h2 id="___sec5" class="anchor">Non-parametric approach: Mean and covariance functions </h2>

<p>
Similarly to how a D-dimensional Gaussian is parameterized by its mean vector and its covariance matrix, a GP is parameterized by a mean <em>function</em> and a covariance <em>function</em>. To explain this, we'll assume (without loss of generality) that the mean function is \( \mu(x) = \mathbf{0} \). As for the covariance function, \( C(x,x') \), it is a function that receives as input two locations \( x,x' \) belonging to the input domain, i.e. \( x,x' \in \mathcal{X} \), and returns the value of their co-variance.

<p>
In this way, if we have a <em>finite</em> set of input locations we can evaluate the covariance function at every pair of locations and obtain a covariance matrix \( \mathbf{C} \). We write:
$$ \mathbf{C} = C(\mathbf{X}, \mathbf{X}), $$

where \( \mathbf{X} \) is the collection of training inputs.

<p>
We'll see below that the covariance function is what encodes our assumption about the GP. By selecting a covariance function, we are making implicit assumptions about the shape of the function we wish to encode with the GP, for example how smooth it is.

<p>
Even if the covariance function has a parametric form, combined with the GP it gives us a nonparametric model. In other words, the covariance function is specifying the general properties of the GP function we wish to encode, and not a specific parametric form for it.

<p>
<!-- !split -->

<h3 id="___sec6" class="anchor">Stationary kernels </h3>

To be added.

<p>
Below we define two very common covariance functions: The RBF (also known as Exponentiated Quadratic or Gaussian kernel) which is differentiable infinitely many times (hence, very smooth),
$$ k_{RBF}(\mathbf{x}_{i},\mathbf{x}_{j}) = \sigma^2 \exp \left( -\frac{1}{2\ell^2} \sum_{q=1}^Q (x_{i,q} - x_{j,q})^2\right) $$

where \( Q \) denotes the dimensionality of the input space. Its parameters are: the <em>lengthscale</em>, \( \ell \) and the variance \( \sigma^2 \).

<p>
Furthermore, the linear kernel:
$$ k_{lin}(\mathbf{x}_{i},\mathbf{x}_{j}) = \sigma^2 \mathbf{x}_{i}^T \mathbf{x}_{j} $$

<h2 id="___sec7" class="anchor">GP models for regression </h2>
To be added.

<h3 id="___sec8" class="anchor">Optimizing the GP model hyperparameters </h3>

To be added.

<h2 id="___sec9" class="anchor">GP emulators </h2>

<p>
To be added.

<!-- ------------------- end of main content --------------- -->

</div>  <!-- end container -->
<!-- include javascript, jQuery *first* -->
<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.10.2/jquery.min.js"></script>
<script src="https://netdna.bootstrapcdn.com/bootstrap/3.0.0/js/bootstrap.min.js"></script>

<!-- Bootstrap footer
<footer>
<a href="http://..."><img width="250" align=right src="http://..."></a>
</footer>
-->


<center style="font-size:80%">
<!-- copyright --> &copy; 2018-2019, Christian Forss&#233;n. Released under CC Attribution-NonCommercial 4.0 license
</center>


</body>
</html>
    

