TITLE: Learning from data: Assigning probabilities
AUTHOR: Christian Forssén {copyright, 2018-present|CC BY-NC} at Department of Physics, Chalmers University of Technology, Sweden
DATE: today

!split
======= Ignorance pdfs: Indifference and translation groups =======

* Consider a six-sided dice
* How do we assign $p(X_i|I)$, $i \in \{1, 2, 3, 4, 5, 6\}$?
* We do know $\sum_i p(X_i|I) = 1$
* Invariance under labeling $\Rightarrow p(X_i|I)=1/6$
  * provided that the prior information $I$ says nothing that breaks the symmetry.

!split
===== Location invariance =====
Indifference to a shift $x_0$ for a location parameter $x$ implies that
!bt
\[
p(x|I) dx \approx p(x+ x_0|I) d(x+x_0) =  p(x+ x_0|I) dx,
\]
!et
in the allowed range.

* Invariance under origin position $\Rightarrow p(x|I) =  p(x+ x_0|I)$, i.e., $p(x|I) = \mathrm{constant}$.
  * Provided that the prior information $I$ says nothing that breaks the symmetry.
* The pdf will be zero outside the allowed range (specified by $I$).

!split
===== Scale invariance =====

Indifference to a re-scaling $\lambda$ of a scale parameter $x$ implies that
!bt
\[
p(x|I) dx \approx p(\lambda x|I) d(\lambda x) =  \lambda p(\lambda x|I) dx,
\]
!et
in the allowed range.

* Invariance under re-scaling $\Rightarrow p(x|I) \propto 1/x$. 
  * Provided that the prior information $I$ says nothing that breaks the symmetry.
* The pdf will be zero outside the allowed range (specified by $I$).
* This prior is often called a *Jeffrey's prior*; it represents a complete ignorance of a scale parameter within an allowed range.
* It is equivalent to a uniform pdf for the logarithm: $p(\log(x)|I) = \mathrm{constant}$
  * as can be verified with a change of variable $y=\log(x)$, see lecture notes on error propagation.
  
=== Example: Straight-line model ===
Consider the theoretical model $y_\mathrm{th}(x) = \theta_1  x  + \theta_0$.

* Would you consider the intercept $\theta_0$ a location or a scale parameter, or something else?
* Would you consider the slope $\theta_1$ a location or a scale parameter, or something else?

Consider also the statistical model for the observed data $y_i = y_\mathrm{th}(x_i) + \epsilon_i$, where we assume independent, Gaussian noise $\epsilon_i \sim \mathcal{N}(0, \sigma^2)$.
* Would you consider the standard deviation $\sigma$ a location or a scale parameter, or something else?

!split
===== Symmetry invariance =====

* In fact, by symmetry indifference we could as well have written the linear model as $x_\mathrm{th}(y) = \theta_1'  y  + \theta_0'$

* We would then equate the probability elements for the two models $p(\theta_0, \theta_1 | I) d\theta_0 d\theta_1 = q(\theta_0', \theta_1' | I) d\theta_0' d\theta_1'$.

* The transformation gives $(\theta_0', \theta_1') = (-\theta_1^{-1}\theta_0, \theta_1^{-1})$.

This change of variables implies that
!bt
\[
q(\theta_0', \theta_1' | I) = p(\theta_0, \theta_1 | I) \left| \frac{d\theta_0 d\theta_1}{d\theta_0' d\theta_1'} \right|,
\]
!et
where the (absolute value of the) determinant of the Jacobian is
!bt
\[
\left| \frac{d\theta_0 d\theta_1}{d\theta_0' d\theta_1'} \right| 
= \mathrm{abs} \left( 
\begin{vmatrix}
\frac{\partial \theta_0}{\partial \theta_0'} & \frac{\partial \theta_0}{\partial \theta_1'} \\
\frac{\partial \theta_1}{\partial \theta_0'} & \frac{\partial \theta_1}{\partial \theta_1'} 
\end{vmatrix}
\right)
= \frac{1}{\left( \theta_1' \right)^3}.
\]
!et

* In summary we find that $\theta_1^3 p(\theta_0, \theta_1 | I) = p(-\theta_1^{-1}\theta_0, \theta_1^{-1}|I).$
* This functional equation is satisfied by
!bt
\[
p(\theta_0, \theta_1 | I) \propto \frac{1}{\left( 1 + \theta_1^2 \right)^{3/2}}.
\]
!et

!split
FIGURE:[fig/slope_priors.png, width=800 frac=0.8] 100 samples of straight lines with intercept 0 and slope from three different pdfs.


!split
======= The principle of maximum entropy =======

Having dealt with ignorance, let us move on to a more enlightened situation.

Consider a die with the usual six faces that was rolled a very large number of times. Suppose that we were only told that the average number of dots was 2.5. What (discrete) pdf would we assign? I.e. what are the probabilities $\{ p_i \}$ that the face on top had $i$ dots after a single throw?

!split
The available information can be summarized as follows
!bt
\[
\sum_{i=1}^6 p_i = 1, \qquad \sum_{i=1}^6 i p_i = 2.5
\]
!et
This is obviously not a normal die, with uniform probability $p_i=1/6$, since the average result would then be 3.5. But there are many candidate pdfs that would reproduce the given information. Which one should we prefer?

!split
It turns out that there are several different arguments that all point in a direction that is very familiar to people with a physics background. Namely that we should prefer the probability distribution that maximizes an entropy measure, while fulfilling the given constraints. 

!split
===== The entropy of Scandinavians =====

Let's consider another pdf assignment problem. This is originally the *kangaroo problem* (Gull and SKilling, 1984), but adapted here to a local context. The problem is stated as follows:

- Information: 70% of all Scandinavians have blonde hair, and 10% of all Scandinavians are left handed.

- Question: On the basis of this information alone, what proportion of kangaroos are both blonde and left handed?

!split
We note that for any one given Scandinavian there are four distinct possibilities: 
o Blonde and left handed (probability $p_1$).
o Blonde and right handed (probability $p_2$).
o Not blonde and left handed (probability $p_3$).
o Not blonde and right handed (probability $p_4$).

!split
The following 2x2 contingency table

  |--------------------------------------------------------|
  |                           | Left handed | Right handed |
  |--l----------------------c-----------c------------------|
  | Blonde     | $p_1$        | $p_2$                      |
  | Not blonde | $p_3$        | $p_4$                      |
  |--------------------------------------------------------|

!split
can be written in terms of a single variable $x$ due to the normalization condition $\sum_{i=1}^4 p_i = 1$, and the available information $p_1 + p_2 = 0.7$ and $p_1 + p_3 = 0.1$

  |--------------------------------------------------------|
  |                           | Left handed | Right handed |
  |--l----------------------c-----------c------------------|
  | Blonde     | $0 \le x \le 0.1$ | $0.7-x$               |
  | Not blonde | $0.1-x$           | $0.2+x$               |
  |--------------------------------------------------------|
  
But which choice of $x$ is preferred?

!split
===== The monkey argument =====

A model for assigning probabilities to $M$ different alternatives that satisfy some constraint as described by $I$: 
* Monkeys throwing $N$ balls into $M$ equally sized boxes.
* The normalization condition $N = \sum_{i=1}^M n_i$.
* The fraction of balls in each box gives a possible assignment for the corresponding probability $p_i = n_i / M$.
* The distribution of balls $\{ n_i \}$ is therefore a candidate pdf $\{ p_i \}$.
* The resulting pdf might not be consistent with the constraints of $I$, however, in which case it should be rejected as a possible candidate.
* After many such trials, some distributions will be found to come up more often than others. The one that appears most frequently (and satisfies $I$) would be a sensible choice for $p(\{p_i\}|I)$.

!split
* The number of micro-states, $W$, as a function of $\{p_i\}$ is
!bt
\[
\log(W(\{n_i\})) = \log(N!) − \sum_{i=1}^M \log(n_i!) 
\approx N\log(N) - \sum_{i=1}^M n_i\log(n_i),
\]
!et
  where we have used the Stirling approximation $\log(n!) \approx n\log(n) - n$ for large numbers, and a cancellation of two terms. 

!split
* There are $M^N$ different ways to distribute the balls.
* The micro-states $\{ n_i\}$ are connected to the pdf $\{ p_i \}$, so the frequency of a given pdf is given by
!bt
\[
\log(F(\{p_i\})) \approx -N \log(M) + N\log(N) - \sum_{i=1}^M n_i\log(n_i)
\]
!et

Substituting $p_i = n_i/N$, and using the normalization condition finally gives
!bt
\[
\log(F(\{p_i\})) \approx -N \log(M) - N \sum_{i=1}^M p_i\log(p_i)
\]
!et

!split
We note that $N$ and $M$ are constants so that the pdf is given by the $\{ p_i \}$ that maximizes
!bt
\[
S = - \sum_{i=1}^M p_i\log(p_i).
\]
!et
You might recognise this quantity as the *entropy* from statistical mechanics. The interpretation of entropy in statistical mechanics is the measure of uncertainty, which remains about a system after its observable macroscopic properties, such as temperature, pressure and volume, have been taken into account. For a given set of macroscopic variables, the entropy measures the degree to which the probability of the system is spread out over different possible microstates. Specifically, entropy is a logarithmic measure of the number of micro-states with significant probability of being occupied $S = -k_B \sum_i p_i \log(p_i)$, where $k_B$ is the Boltzmann constant.

!split
=== Why maximize the entropy? ===

* Information theory: maximum entropy=minimum information (Shannon, 1948).
* Logical consistency (Shore & Johnson, 1960).
* Uncorrelated assignments related monotonically to $S$ (Skilling, 1988).

Consider the third argument. Let us check it empirically to the problem of hair colour and handedness of Scandinavians. We are interested in determining $p_1 \equiv p(L,B|I) \equiv x$, the probability that a Scandinavian is both left-handed and blonde. However, in this simple example we can immediately realize that the assignment $p_1=0.07$ is the only one that implies no correlation between left-handedness and hair color. Any joint probability smaller than 0.07 implies that left-handed people are less likely to be blonde, and any larger vale indicates that left-handed people are more likely to be blonde.

!split
So unless you have specific information about the existence of such a correlation, you should better not build it into the assignment of the probability $p_1$.

_Question_: Can you show why $p_1 < 0.07$ and $p_1 > 0.07$ corresponds to left-handedness and blondeness being dependent variables?

!split
Let us now empirically consider a few variational functions of $\{ p_i \}$ and see if any of them gives a maximum that corresponds to the uncorrelated assignment $x=0.07$, which implies $p_1 = 0.07, \, p_2 = 0.63, \, p_3 = 0.03, \, p_4 = 0.27$. A few variational functions and their prediction for $x$ are shown in the following table.

  |--------------------------------------------------------|
  |Variational function  | Optimal x | Implied correlation |
  |--c----------------------c-----------c------------------|
  | $-\sum_i p_i \log(p_i)$     | 0.070  | None            |
  | $\sum_i \log(p_i)$          | 0.053  | Negative        |
  | $-\sum_i p_i^2 \log(p_i)$   | 0.100  | Positive        |
  | $-\sum_i \sqrt{p_i(1-p_i)}$ | 0.066  | Negative        |
  |--------------------------------------------------------|
  
FIGURE:[fig/scandinavian_entropy.png, width=800 frac=0.8] Four different variational functions $f\left( \{ p_i \} \right)$. The optimal $x$ are shown by circles. The uncorrelated assignment $x=0.07$ is shown by a vertical line.

=== Continuous case ===

Return to monkeys, but now with different probabilities for each bin.Then
!bt
\[
S= −\sum_{i=1}^M p_i \log \left( \frac{p_i}{m_i} \right),
\]
!et
which is often known as the *Shannon-Jaynes entropy*, or the *Kullback number*, or the *cross entropy* (with opposite sign).

In the continuous case
!bt
\[
S[p]= −\int p(x) \log \left( \frac{p(x)}{m(x)} \right).
\]
!et

===== Derivation of common pdfs using MaxEnt =====

=== Variance and the Gaussian pdf ===

=== Counting statistics and the Poisson distribution ===