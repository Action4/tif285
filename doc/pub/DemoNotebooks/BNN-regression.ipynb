{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demonstration Bayesian Neural Networks for regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f27c4016e4570c15",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# importing modules\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "\n",
    "import emcee\n",
    "import corner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(seed=2024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate data for regression problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DataGeneratingProcess(x, alpha=1., eps=0.1):\n",
    "    dy = rng.normal(loc=0, scale=eps, size=x.shape)\n",
    "    return np.exp(-alpha*x) + dy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha=1.\n",
    "dy=0.05\n",
    "xdata = np.sort(rng.uniform(size=10))\n",
    "ydata = DataGeneratingProcess(xdata, alpha=alpha, eps=dy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax =plt.subplots(1,1,)\n",
    "ax.plot(xdata,ydata,'o', label='data')\n",
    "xplot = np.linspace(0,2,100)\n",
    "ytrue = DataGeneratingProcess(xplot, alpha=alpha, eps=0.)\n",
    "ax.plot(xplot, ytrue,'k-',label='true')\n",
    "ax.set_xlabel(r'$x$')\n",
    "ax.set_ylabel(r'$y$')\n",
    "ax.legend(loc='best');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First approach: FFNN for regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct three copies of a simple FFNN with one hidden layer\n",
    "models={}\n",
    "numModels = 3\n",
    "for imodel in range(numModels):\n",
    "    models[imodel] = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Input(shape=(1,)),\n",
    "        tf.keras.layers.Dense(10, activation='sigmoid',),\n",
    "        tf.keras.layers.Dense(1, activation='linear',)\n",
    "    ])\n",
    "print(f'Initializing {numModels} identical FFNN models of the following architecture')\n",
    "models[0].summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for imodel in range(numModels):\n",
    "    models[imodel].compile(optimizer='adam',\n",
    "                           loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = {}\n",
    "for imodel in range(numModels):\n",
    "    print(f'Training model {imodel}')\n",
    "    history[imodel] = models[imodel].fit(xdata, ydata, epochs=5000,verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1)\n",
    "for imodel in range(numModels):\n",
    "    ax.semilogy(history[imodel].history['loss'])\n",
    "ax.set_xlabel('epoch')\n",
    "ax.set_ylabel('loss');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax =plt.subplots(1,1,)\n",
    "ax.plot(xdata,ydata,'o', label='data')\n",
    "xplot = np.linspace(0,2,100)\n",
    "ytrue = DataGeneratingProcess(xplot, alpha=alpha, eps=0.)\n",
    "ax.plot(xplot, ytrue,'k-',label='true')\n",
    "for imodel in range(numModels):\n",
    "    yFFNN = models[imodel].predict(xplot)\n",
    "    ax.plot(xplot, yFFNN,'--',label=f'FFNN #{imodel}')\n",
    "ax.set_xlabel(r'$x$')\n",
    "ax.set_ylabel(r'$y$')\n",
    "ax.legend(loc='best');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the different predictions of the FFNN models. All have the same architecture, the same activation functions, and the same learning algorithms. However, initialization of weights is random and the gradient-based learning leads to different local minima.\n",
    "\n",
    "How can we quantify the extrapolation error?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach 2: A Bayesian Neural Network with MCMC sampling of weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regWeight = 5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First create a hard-coded FFNN as defined above\n",
    "def sigmoid(z):\n",
    "    return 1. / (1. + np.exp(-z))\n",
    "\n",
    "def hardcodedFFNN(x, w):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        x: ndarray, len-Ndata\n",
    "            array of inputs\n",
    "        w: ndarray, shape=(31,)\n",
    "            weights of the neural network\n",
    "            w[0:9] = linear weights of the hidden layer\n",
    "            w[10:19] = biases of the hidden layer\n",
    "            w[20:29] = linear weights of the output layer\n",
    "            w[30] = bias of the output layer\n",
    "\n",
    "    Returns:\n",
    "        ndarray, shape=(Ndata,1)\n",
    "            array of outputs\n",
    "    \"\"\"\n",
    "    Ndata = len(x)\n",
    "    x = x.reshape(-1,1)\n",
    "    # activations of the hidden layer\n",
    "    z1 = w[10:20].reshape(1,-1) + np.dot(x, w[0:10].reshape(1,-1))\n",
    "    # outputs of the hidden layer\n",
    "    y1 = sigmoid(z1)\n",
    "    # activations of the output layer\n",
    "    zout = w[30] + np.dot(y1, w[20:30].reshape(-1,1))\n",
    "    # linear output\n",
    "    return zout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the functionality\n",
    "# weights of tf models\n",
    "weightsNN = {}\n",
    "for imodel in range(numModels):\n",
    "    weights = models[imodel].get_weights()\n",
    "    # set a weight array with these weights\n",
    "    weightsNN[imodel] = np.zeros(31)\n",
    "    weightsNN[imodel][0:10] = weights[0].flatten()\n",
    "    weightsNN[imodel][10:20] = weights[1].flatten()\n",
    "    weightsNN[imodel][20:30] = weights[2].flatten()\n",
    "    weightsNN[imodel][30] = weights[3].flatten()[0]\n",
    "    # make predictions with tf model and hard-coded one\n",
    "    modelPredict = models[imodel].predict(xplot)\n",
    "    hardcodedFFNNPredict = hardcodedFFNN(xplot, weightsNN[imodel])\n",
    "    if np.isclose(modelPredict,hardcodedFFNNPredict,atol=1e-7).all():\n",
    "        print(f'The FFNN can reproduce the predictions of model {imodel}')\n",
    "    else:\n",
    "        print(f'The tf and hard-code model are different for model {imodel}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the log prior, likelihood, posterior\n",
    "#\n",
    "def logLikelihood(w, xtrain, ytrain, dy=0.1):\n",
    "    \"\"\"\n",
    "    Log likelihood for the FFNN model and normal errors\n",
    "    \n",
    "    Args:\n",
    "        w: ndarray, len=31 \n",
    "            array with the FFNN's biases and weights as defined above.\n",
    "        xtrain: ndarray, shape (Ndata,1)\n",
    "            array of input data\n",
    "        ytrain: ndarray, shape (Ndata,1)\n",
    "            array of output data\n",
    "        dy: float (default 0.1)\n",
    "            stddev of normal error\n",
    "        \n",
    "    Return\n",
    "        log_likelihood \n",
    "    \"\"\"\n",
    "    residuals = hardcodedFFNN(xtrain,w) - ytrain.reshape(-1,1)\n",
    "    return -0.5 * np.linalg.norm(residuals)**2 / dy**2.\n",
    "\n",
    "def logPrior(w, dw=1.0):\n",
    "    \"\"\"\n",
    "    Log prior for the parameters of a single neuron binary classifier\n",
    "    \n",
    "    Args:\n",
    "        w: ndarray, len=31 \n",
    "            array with the FFNN's biases and weights as defined above.\n",
    "        dw: float (default=1.0)\n",
    "            stddev of the Gaussian prior\n",
    "        \n",
    "    Return\n",
    "        log_prior (gaussian) \n",
    "    \"\"\"\n",
    "    return - 0.5 * np.linalg.norm(w)**2 / dw**2\n",
    "\n",
    "def logPosterior(w, xtrain, ytrain, dy=0.1, dw=1.0):\n",
    "    return logPrior(w, dw=dw) + logLikelihood(w, xtrain, ytrain, dy=dy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the emcee sampler\n",
    "ndim = 31  # number of parameters in the model\n",
    "nwalkers = 100  # number of MCMC walkers\n",
    "nburn = 1000  # \"burn-in\" period to let chains stabilize\n",
    "nsteps = 10000  # number of MCMC steps to take\n",
    "\n",
    "# we'll start at random locations within the prior volume\n",
    "p0 = rng.normal(loc=0.0, scale=regWeight, size=(nwalkers, ndim))\n",
    "\n",
    "print(\"MCMC sampling using emcee (affine-invariant ensamble sampler) with {0} walkers\".format(nwalkers))\n",
    "sampler = emcee.EnsembleSampler(nwalkers, ndim, logPosterior, args=[xdata, ydata], kwargs={'dy': dy, 'dw': regWeight})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"burn-in\" period; save final positions and then reset\n",
    "pos, prob, state = sampler.run_mcmc(p0, nburn,)\n",
    "sampler.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampling period\n",
    "sampler.run_mcmc(pos, nsteps)\n",
    "\n",
    "print(\"Mean acceptance fraction: {0:.3f} (in total {1} steps)\"\n",
    "                .format(np.mean(sampler.acceptance_fraction),nwalkers*nsteps))\n",
    "\n",
    "# discard burn-in points and flatten the walkers; the shape of samples is (nwalkers*nsteps, ndim)\n",
    "samples = sampler.chain.reshape((-1, ndim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a corner plot with the posterior distribution\n",
    "fig = corner.corner(samples[::100,(0,10,1,11, 20, 21, 30)], labels=[r\"$w^{(1)}_{1,1}$\", r\"$w^{(1)}_{0,1}$\", \\\n",
    "                        r\"$w^{(1)}_{1,2}$\", r\"$w^{(1)}_{0,2}$\", r\"$w^{(2)}_{1}$\", r\"$w^{(2)}_{2}$\", r\"$w^{(2)}_{0}$\"],\n",
    "                    truths=weightsNN[0][[0,10,1,11, 20, 21, 30]], quantiles=[0.16, 0.5, 0.84],\n",
    "                    show_titles=True, title_kwargs={\"fontsize\": 10})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axs = plt.subplots(8,4, sharey=True, sharex=True, figsize=(6,12))\n",
    "histRange=(-regWeight,regWeight)\n",
    "\n",
    "# Here we look at the traces for different walkers, but...\n",
    "# ... NOTE that walkers are not independent in an ensamble sampler\n",
    "for ipar in range(31):\n",
    "    ax = axs.flatten()[ipar]\n",
    "    for ichain in range(5):\n",
    "        ax.plot(samples[ichain*nsteps:(ichain+1)*nsteps,ipar])\n",
    "    ax.set_ylim(bottom=histRange[0], top=histRange[1])\n",
    "axs.flatten()[-1].set_visible(False)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axs = plt.subplots(8,4, sharey=True, sharex=True, figsize=(6,12))\n",
    "histRange=(-6,6)\n",
    "\n",
    "for ipar in range(31):\n",
    "    ax = axs.flatten()[ipar]\n",
    "    ax.hist(samples[:,ipar],density=True,histtype='step',range=histRange,bins=20)\n",
    "    for imodel in range(numModels):\n",
    "        ax.axvline(x=weightsNN[imodel][ipar],color='r',ls='--')\n",
    "    ax.set_xlim(left=histRange[0], right=histRange[1])\n",
    "axs.flatten()[-1].set_visible(False)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For simplicity we will just take one PPD sample per walker\n",
    "samplesPPD = np.zeros((nwalkers,len(xplot)))\n",
    "for iwalker in range(nwalkers):\n",
    "    samplePosterior = sampler.chain[iwalker,int(nsteps/2),:]\n",
    "    samplesPPD[iwalker,:] = hardcodedFFNN(xplot, samplePosterior).flatten()\n",
    "(PPDlo, PPDmed, PPDhi) = np.quantile(samplesPPD,(0.16, 0.5, 0.84),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax =plt.subplots(1,1,)\n",
    "ax.plot(xdata,ydata,'o', label='data')\n",
    "ax.fill_between(xplot, PPDlo, PPDhi,color='r', alpha=0.1, label='BNN')\n",
    "ax.plot(xplot, PPDmed, color='r')\n",
    "ax.plot(xplot, PPDlo, color='r', lw=0.5)\n",
    "ax.plot(xplot, PPDhi, color='r', lw=0.5)\n",
    "xplot = np.linspace(0,2,100)\n",
    "ytrue = DataGeneratingProcess(xplot, alpha=alpha, eps=0.)\n",
    "ax.plot(xplot, ytrue,'k-',label='true')\n",
    "for imodel in range(numModels):\n",
    "    yFFNN = models[imodel].predict(xplot)\n",
    "    ax.plot(xplot, yFFNN,'--',label=f'FFNN #{imodel}')\n",
    "ax.set_xlabel(r'$x$')\n",
    "ax.set_ylabel(r'$y$')\n",
    "ax.legend(loc='best');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach 3: A Bayesian Neural Network with Variational Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First create a hard-coded FFNN as defined above\n",
    "# In order to employ the power of jax, we have to use jax.numpy\n",
    "# for all numpy operations\n",
    "def jsigmoid(z):\n",
    "    return 1. / (1. + jnp.exp(-z))\n",
    "\n",
    "def jhardcodedFFNN(x, w):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        x: ndarray, len-Ndata\n",
    "            array of inputs\n",
    "        w: ndarray, shape=(31,)\n",
    "            weights of the neural network\n",
    "            w[0:9] = linear weights of the hidden layer\n",
    "            w[10:19] = biases of the hidden layer\n",
    "            w[20:29] = linear weights of the output layer\n",
    "            w[30] = bias of the output layer\n",
    "\n",
    "    Returns:\n",
    "        ndarray, shape=(Ndata,1)\n",
    "            array of outputs\n",
    "    \"\"\"\n",
    "    Ndata = len(x)\n",
    "    x = x.reshape(-1,1)\n",
    "    # activations of the hidden layer\n",
    "    z1 = w[10:20].reshape(1,-1) + jnp.dot(x, w[0:10].reshape(1,-1))\n",
    "    # outputs of the hidden layer\n",
    "    y1 = jsigmoid(z1)\n",
    "    # activations of the output layer\n",
    "    zout = w[30] + jnp.dot(y1, w[20:30].reshape(-1,1))\n",
    "    # linear output\n",
    "    return zout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# jax.grad takes a function and returns a function\n",
    "jax.grad(jsigmoid)(0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the functionality\n",
    "# weights of tf models\n",
    "weightsNN = {}\n",
    "for imodel in range(numModels):\n",
    "    weights = models[imodel].get_weights()\n",
    "    # set a weight array with these weights\n",
    "    weightsNN[imodel] = np.zeros(31)\n",
    "    weightsNN[imodel][0:10] = weights[0].flatten()\n",
    "    weightsNN[imodel][10:20] = weights[1].flatten()\n",
    "    weightsNN[imodel][20:30] = weights[2].flatten()\n",
    "    weightsNN[imodel][30] = weights[3].flatten()[0]\n",
    "    # make predictions with tf model and hard-coded one\n",
    "    modelPredict = models[imodel].predict(xplot)\n",
    "    hardcodedFFNNPredict = jhardcodedFFNN(xplot, weightsNN[imodel])\n",
    "    if np.isclose(modelPredict,hardcodedFFNNPredict,atol=1e-7).all():\n",
    "        print(f'The jax FFNN can reproduce the predictions of model {imodel}')\n",
    "    else:\n",
    "        print(f'The tf and jax FFNN are different for model {imodel}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the log prior, likelihood, posterior\n",
    "#\n",
    "def jlogLikelihood(w, xtrain, ytrain, dy=0.1):\n",
    "    \"\"\"\n",
    "    Log likelihood for the FFNN model and normal errors\n",
    "    \n",
    "    Args:\n",
    "        w: ndarray, len=31 \n",
    "            array with the FFNN's biases and weights as defined above.\n",
    "        xtrain: ndarray, shape (Ndata,1)\n",
    "            array of input data\n",
    "        ytrain: ndarray, shape (Ndata,1)\n",
    "            array of output data\n",
    "        dy: float (default 0.1)\n",
    "            stddev of normal error\n",
    "        \n",
    "    Return\n",
    "        log_likelihood \n",
    "    \"\"\"\n",
    "    residuals = jhardcodedFFNN(xtrain,w) - ytrain.reshape(-1,1)\n",
    "    return -0.5 * jnp.linalg.norm(residuals)**2 / dy**2.\n",
    "\n",
    "def jlogPrior(w, dw=1.0):\n",
    "    \"\"\"\n",
    "    Log prior for the parameters of a single neuron binary classifier\n",
    "    \n",
    "    Args:\n",
    "        w: ndarray, len=31 \n",
    "            array with the FFNN's biases and weights as defined above.\n",
    "        dw: float (default=1.0)\n",
    "            stddev of the Gaussian prior\n",
    "        \n",
    "    Return\n",
    "        log_prior (gaussian) \n",
    "    \"\"\"\n",
    "    return - 0.5 * jnp.linalg.norm(w)**2 / dw**2\n",
    "\n",
    "def jlogPosterior(w, xtrain, ytrain, dy=0.1, dw=1.0):\n",
    "    return jlogPrior(w, dw=dw) + jlogLikelihood(w, xtrain, ytrain, dy=dy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time the original posterior\n",
    "%timeit logPosterior(weightsNN[0],xdata, ydata, dy=dy, dw=regWeight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time the jax-evaluated posterior\n",
    "%timeit jlogPosterior(weightsNN[0],xdata, ydata, dy=dy, dw=regWeight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The gradient with respect to the first argument\n",
    "def gradLogPosterior(w):\n",
    "    return jax.grad(jlogPosterior, argnums=0)(w,xdata, ydata, dy=dy, dw=regWeight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the gradient comes at a computational cost\n",
    "%timeit gradLogPosterior(weightsNN[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following we use the Doubly Stochastical Variational Inference (DSVI) approach by Titsias and Lazaro-Gredilla (2014). Specifically, we implement their Algorithm 1 with\n",
    "- $\\Phi(\\mathbf{z})$ is the standard normal distribution (independent components)\n",
    "- $\\mathbf{w} = \\mathbf{C} \\mathbf{z} + \\boldsymbol{\\mu}$\n",
    "- We neglect correlations between weights by considering $\\mathbf{C} = \\text{diag}(\\boldsymbol{\\sigma})$\n",
    "- Together with $\\boldsymbol{\\mu}$ we have $2 N_\\mathrm{par}$ variational parameters.\n",
    "\n",
    "The variational distribution \n",
    "$$\n",
    "q\\left( \\boldsymbol{\\theta} \\vert \\boldsymbol{\\mu}, \\mathbf{C}\\right) = \\frac{1}{|\\mathbf{C}|} \\Phi \\left( \\mathbf{C}^{-1}(\\boldsymbol{\\theta} - \\boldsymbol{\\mu}) \\right),\n",
    "$$\n",
    "will be optimized to approximate the posterior $p(\\boldsymbol{\\theta} \\vert \\mathbf{y})$ via the KL divergence \n",
    "$$\n",
    "D_\\mathrm{KL} \\left( q\\left( \\boldsymbol{\\theta} \\vert \\boldsymbol{\\mu}, \\mathbf{C}\\right) \\, \\vert\\vert \\, p(\\boldsymbol{\\theta} \\vert \\mathbf{y}) \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
