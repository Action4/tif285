
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>16. Frequentist hypothesis testing &#8212; Learning from data</title>
    
  <link href="_static/css/theme.css" rel="stylesheet" />
  <link href="_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="_static/sphinx-book-theme.acff12b8f9c144ce68a297486a2fa670.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="_static/js/index.1c5a1a01449ed65a7b51.js">

    <script id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/togglebutton.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="1. Inference using Gaussian processes" href="GaussianProcesses.html" />
    <link rel="prev" title="15. Assigning probabilities" href="demo-MaxEnt.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="index.html">
      
      <img src="_static/logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Learning from data</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="welcome.html">
   Course aim
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Introduction
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="Intro.html">
   1. Learning from data: Inference
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="demo-Intro.html">
   2. Reading Data and fitting
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="exercise_Jupyter_Python_intro_01.html">
   4. Python and Jupyter notebooks: part 01
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="exercise_Jupyter_Python_intro_02.html">
   5. Python and Jupyter notebooks: part 02
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Linear regression
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="LinearRegression.html">
   1. Linear regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="ModelValidation.html">
   2. Model validation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="demo-ModelValidation.html">
   4. Linear Regression and Model Validation demonstration
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="exercise_LinearRegression.html">
   5. Linear Regression exercise
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Bayesian statistics
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="BayesianBasics.html">
   1. Statistical inference
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="BayesianRecipe.html">
   2. The Bayesian recipe
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="demo-BayesianBasics.html">
   3. Interactive Bayesian Coin Tossing
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="exercise_sum_product_rule.html">
   4. Checking the sum and product rules, and their consequences
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="BayesianParameterEstimation.html">
   5. Inference With Parametric Models
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="demo-BayesianParameterEstimation.html">
   6. Bayesian parameter estimation demonstration
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="exercise_BayesianParameterEstimation.html">
   7. Exercise: Bayesian parameter estimation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="MCMC.html">
   8. Why MCMC
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="demo-MCMC.html">
   9. Metropolis-Hasting MCMC sampling of a Poisson distribution
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="ErrorPropagation.html">
   10. Why Bayes is Better
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="demo-ErrorPropagation.html">
   11. Learning from Data: Error propagation and nuisance parameters (demonstration)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="MaxEnt.html">
   14. Ignorance pdfs: Indifference and translation groups
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="demo-MaxEnt.html">
   15. Assigning probabilities
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   16. Frequentist hypothesis testing
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Machine learning---A Bayesian perspective
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="GaussianProcesses.html">
   1. Inference using Gaussian processes
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="demo-GaussianProcesses.html">
   2. Gaussian processes demonstration
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="exercise_GP.html">
   3. Exercise: Gaussian processes
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="LogReg.html">
   4. Logistic Regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="NeuralNet.html">
   6. Neural networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="demo-NeuralNet.html">
   7. Neural network classifier demonstration
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="exercises_LogReg_NeuralNet.html">
   8. Exercise: Logistic Regression and neural networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="cnn.html">
   9. Convolutional Neural Networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="demo-cnn.html">
   10. Convolutional Neural Network
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="bnn.html">
   13. Bayesian neural networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="demo-bnn.html">
   14. Variational Inference: Bayesian Neural Networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="exercises_BNN.html">
   15. Exercise: Bayesian neural networks
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="_sources/ModelSelection.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#">
   16. Frequentist hypothesis testing
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#basic-idea">
     16.1. Basic idea
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#hypothesis-testing-with-the-chi-squared-statistic">
       16.1.1. Hypothesis testing with the chi-squared statistic
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#bayesian-model-selection">
   17. Bayesian model selection
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#the-story-of-dr-a-and-prof-b">
     17.1. The Story of Dr. A and Prof. B
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#one-adjustable-parameter-each-different-prior-ranges">
       17.1.1. One adjustable parameter each; different prior ranges
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#comparison-with-parameter-estimation">
     17.2. Comparison with parameter estimation
    </a>
   </li>
  </ul>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <p>Copyright 2018-2021, Christian Forssén. Released under CC Attribution-NonCommercial 4.0 license</p>
<!-- !split -->
<p>So far, we have been concerned with the problem of parameter estimation. In studying the linear relationship between two quantities, for example, we discussed how to infer the slope and the offset of the associated straight-line model. Often, however, there is a question as to whether another functional form (such as quadratic or cubic) might be a more appropriate model. In this lecture, we will consider the broad class of scientific problems when there is uncertainty as to which one of a set of alternative models is most suitable. In the Bayesian terminology these can be labeled as <strong>Model Selection</strong> problems and we will discuss them in some depth.</p>
<!-- !split -->
<p>One of the main objectives in science is that of inferring the truth of one or more hypotheses about how some aspect of nature works. Because we are always in a state of incomplete information, we can never prove any hypothesis (theory) is true.</p>
<!-- !split -->
<p>We will start, however, with a brief discussion on sampling theory and the frequentist approach to <strong>hypothesis testing</strong>. This will involve the introduction of the <span class="math notranslate nohighlight">\(P\)</span>-value or significance measure—quantities that are often misinterpreted even by scientists themselves. See, for example, the following comment published in Nature (March 20, 2019): <a class="reference external" href="https://www.nature.com/articles/d41586-019-00857-9">Scientists rise up against statistical significance</a>.</p>
<!-- !split -->
<div class="section" id="frequentist-hypothesis-testing">
<h1><span class="section-number">16. </span>Frequentist hypothesis testing<a class="headerlink" href="#frequentist-hypothesis-testing" title="Permalink to this headline">¶</a></h1>
<p>Recall that in frequentist statistics, probability statements are restricted to random variables. A hypothesis can not be considered a random variable, and therefore we are restricted to a much more indirect approach when trying to infer its truth, or rather when attempting to falsify it.</p>
<!-- !split -->
<div class="section" id="basic-idea">
<h2><span class="section-number">16.1. </span>Basic idea<a class="headerlink" href="#basic-idea" title="Permalink to this headline">¶</a></h2>
<p>The standard sampling theory approach to hypothesis testing is to construct a statistical test. The basic idea is the following:</p>
<p><em>Frequentist hypothesis testing.</em>
The sampling theory hypothesis test is designed to compare a selected statistic from the measured data with expected results from a very large number of hypothetical repeated measurements under the assumption that a chosen null hypothesis (<span class="math notranslate nohighlight">\(\mathcal{H}_0\)</span>) is true.</p>
<!-- !split -->
<ul class="simple">
<li><p>The null hypothesis is accepted or rejected purely on the basis of how unexpected the data were to <span class="math notranslate nohighlight">\(\mathcal{H}_0\)</span>, not on how much better the alternative hypothesis (<span class="math notranslate nohighlight">\(\mathcal{H}_A\)</span>) predicted the data.</p></li>
</ul>
<!-- !bpop -->
<ul class="simple">
<li><p>The degree of ‘’unexpectedness’’ is based on a statistic, such as the sample mean or the <span class="math notranslate nohighlight">\(\chi^2\)</span> statistic.</p></li>
<li><p>The statistic is a random variable and it is chosen so that its distribution can be easily computed given the truth of the null hypothesis. In other words, this is the distribution of the chosen statistic for a very large number of hypothetical repeated measurements under the assumption that the null hypothesis is true.</p></li>
<li><p>This statistic is then computed for the observed data set and its value is compared with the distribution that is associated with the truth of the null hypothesis.</p></li>
<li><p>If the statistic from the observed data falls in a very unlikely spot on this distribution (the threshold is to be defined beforehand) we choose to reject the null hypothesis at some confidence level on the basis of the measured data set.</p></li>
</ul>
<!-- !epop -->
<!-- !split -->
<div class="section" id="hypothesis-testing-with-the-chi-squared-statistic">
<h3><span class="section-number">16.1.1. </span>Hypothesis testing with the chi-squared statistic<a class="headerlink" href="#hypothesis-testing-with-the-chi-squared-statistic" title="Permalink to this headline">¶</a></h3>
<p>A very common statistic to use is the <span class="math notranslate nohighlight">\(\chi^2\)</span> measure. A good example is found in Gregory, ch 7.2.1, with the measurements of flux density from a distant galaxy over a period of 6000 days. The main steps of the presented analysis are the following:</p>
<ul class="simple">
<li><p>Choose as a null hypothesis that the galaxy has an unknown, but constant, flux density. If we can reject this hypothesis at e.g. the 95% confidence level, then this provides indirect evidence(?) for the alternative hypothesis that the radio emission is variable.</p></li>
<li><p>In this example, it is assumed that the measurement errors are independent and identically distributed (<strong>iid</strong>) according to a normal distribution with a fixed standard deviation <span class="math notranslate nohighlight">\(\sigma\)</span> that is known beforehand.</p></li>
<li><p>The <span class="math notranslate nohighlight">\(\chi^2\)</span> statistic from the data set is evaluated (<span class="math notranslate nohighlight">\(x_i\)</span> is the data and <span class="math notranslate nohighlight">\(\bar{x}\)</span> is the average from the sample)</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\chi^2 = \sum_{i=1}^n \frac{(x_i - \bar{x})^2}{\sigma^2}.\]</div>
<!-- !split -->
<ul class="simple">
<li><p>In our example we had 15 data points, but we are using them first to estimate the mean <span class="math notranslate nohighlight">\(\mu\)</span>. Therefore, we lose one degree of freedom and are left with 14. This number will determine the form of the <span class="math notranslate nohighlight">\(\chi^2\)</span> distribution that will be used for comparison with our actual <span class="math notranslate nohighlight">\(\chi^2\)</span> statistic.</p></li>
<li><p>The question of how unlikely is this value of <span class="math notranslate nohighlight">\(\chi^2\)</span> is by convention interpreted in terms of the area in the tail of the <span class="math notranslate nohighlight">\(\chi^2\)</span> distribution beyond this line. This is called the <span class="math notranslate nohighlight">\(P\)</span>-value or significance.</p></li>
<li><p>In some cases, a two-sided statistic should be considered instead.</p></li>
</ul>
<!-- !split -->
<!-- <img src="fig/gregory_7_2.png" width=600><p><em>The $\chi^2$ distribution for 14 degrees of freedom. The value computed from the measurements of flux density from a galaxy is indicated by a vertical line. The shaded area is the $P$-value. It is 2% in this particular example so we would reject the null hypothesis with 98% confidence. (Gregory, Fig. 7.2)</em></p> -->
<p><img alt="The  distribution for 14 degrees of freedom. The value computed from the measurements of flux density from a galaxy is indicated by a vertical line. The shaded area is the -value. It is 2% in this particular example so we would reject the null hypothesis with 98% confidence. (Gregory, Fig. 7.2)" src="fig/gregory_7_2.png" /></p>
<!-- !split -->
<p>At the point of performing this comparison, and making a final statement, the sampling theory school divides itself into two camps:</p>
<!-- !bpop -->
<ol class="simple">
<li><p>One camp uses the following protocol: first, before looking at the data, pick the significance level of the test (e.g. 5%), and determine the critical value of <span class="math notranslate nohighlight">\(\chi^2\)</span> above which the null hypothesis will be rejected. (The significance level is the fraction of times that the statistic <span class="math notranslate nohighlight">\(\chi^2\)</span> would exceed the critical value, if the null hypothesis were true.) Then, compare the actual <span class="math notranslate nohighlight">\(\chi^2\)</span> with the critical value, and declare the outcome of the test, and its significance level (which was fixed beforehand).</p></li>
<li><p>The second camp looks at the data, finds <span class="math notranslate nohighlight">\(\chi^2\)</span>, then looks in the table of <span class="math notranslate nohighlight">\(\chi^2\)</span>-distributions for the significance level, <span class="math notranslate nohighlight">\(P\)</span>, for which the observed value of <span class="math notranslate nohighlight">\(\chi^2\)</span> would be the critical value. The result of the test is then reported by giving this value of <span class="math notranslate nohighlight">\(p\)</span>, which is the fraction of times that a result as extreme as the one observed, or more extreme, would be expected to arise if the null hypothesis were true.</p></li>
</ol>
<!-- !epop -->
<!-- !split -->
</div>
</div>
</div>
<div class="section" id="bayesian-model-selection">
<h1><span class="section-number">17. </span>Bayesian model selection<a class="headerlink" href="#bayesian-model-selection" title="Permalink to this headline">¶</a></h1>
<!-- !split -->
<!-- ======= The Story of Dr. A and Prof. B ======= -->
<div class="section" id="the-story-of-dr-a-and-prof-b">
<h2><span class="section-number">17.1. </span>The Story of Dr. A and Prof. B<a class="headerlink" href="#the-story-of-dr-a-and-prof-b" title="Permalink to this headline">¶</a></h2>
<p>[Reproduced, with some modifications, from Sivia, 2006].</p>
<blockquote>
<div><p><em>Dr. A has a theory; Prof. B also has a theory, but with an adjustable parameter <span class="math notranslate nohighlight">\(\lambda\)</span>. Whose theory should we prefer on the basis of data <span class="math notranslate nohighlight">\(D\)</span>?</em>— Jefferys (1939), Gull (1988), Sivia (2006)</p>
</div></blockquote>
<!-- !split -->
<p>It is clear that we need to evaluate the posterior probabilities for A and B being correct to ascertain the relative merit of the two theories. If the ratio of the posterior probabilities,
$$</p>
<div class="amsmath math notranslate nohighlight" id="equation-47359e1c-c680-4267-a2ee-18aa0488f713">
<span class="eqno">(17.1)<a class="headerlink" href="#equation-47359e1c-c680-4267-a2ee-18aa0488f713" title="Permalink to this equation">¶</a></span>\[\begin{equation}
\text{posterior ratio} = \frac{p(A |D, I )}{p(B|D,I)}
\label{eq:sivia_41}
\end{equation}\]</div>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}
is very much greater than one, then we will prefer A’s theory; if it is very much less than one, then we prefer that of B; and if it is of order unity, then the current data are insufficient to make an informed judgement.\\To estimate the odds, let us start by applying Bayes’ theorem to both the numerator and the denominator; this gives
\end{aligned}\end{align} \]</div>
<div class="amsmath math notranslate nohighlight" id="equation-0eb58bd0-5443-488e-a6e3-9a9cde9af751">
<span class="eqno">(17.2)<a class="headerlink" href="#equation-0eb58bd0-5443-488e-a6e3-9a9cde9af751" title="Permalink to this equation">¶</a></span>\[\begin{equation}
\frac{p(A|D,I)}{p(B|D,I)} = \frac{p(D|A,I) p(A|I)}{p(D|B,I) p(B|I)}
\label{eq:sivia_42}
\end{equation}\]</div>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}
because the term $p(D|I)$ cancels out, top and bottom. As usual, probability theory warns us immediately that the answer to our question depends partly on what we thought about the two theories before the analysis of the data. To be fair, we might take the ratio of the prior terms, on the far right of Eq. ([eq:sivia_42](#eq:sivia_42)), to be unity; a harsher assignment could be based on the past track records of the theorists! To assign the probabilities involving the experimental measurements, $p(D|A,I)$ and $p(D|B,I)$, we need to be able to compare the data with the predictions of A and B: the larger the mismatch, the lower the corresponding probability. This calculation is straightforward for Dr A, but not for Prof B; the latter cannot make predictions without a value for $\lambda$.\\To circumvent this difficulty, we can use the sum and product rule to relate the probability we require to other pdfs which might be easier to assign. In particular, marginalization and the product rule allow us to express $p(D | B , I )$ as
\end{aligned}\end{align} \]</div>
<div class="amsmath math notranslate nohighlight" id="equation-70dd2cac-806d-41d2-9926-8964972bcc13">
<span class="eqno">(17.3)<a class="headerlink" href="#equation-70dd2cac-806d-41d2-9926-8964972bcc13" title="Permalink to this equation">¶</a></span>\[\begin{equation}
p(D|B,I) = \int d\lambda p(D,\lambda|B,I) = 
\int d\lambda p(D|\lambda,B,I) p(\lambda|B,I). 
\label{eq:sivia_43}
\end{equation}\]</div>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}
The first term in the integral $p(D | \lambda, B , I )$, where the value of $\lambda$ is given, is now just an ordinary likelihood function; as such, it is on a par with $p(D|A,I)$. The second term is B’s prior pdf for $\lambda$; the onus is, therefore, on the theorist to articulate his or her state of knowledge, or ignorance, before getting access to the data.\\To proceed further analytically, let us make some simplifying approximations. Assume that, a priori, Prof B is only prepared to say that $\lambda$ must lie between the limits $\lambda_\mathrm{min}$ and $\lambda_\mathrm{max}$; we can then naively assign a uniform prior within this range:
\end{aligned}\end{align} \]</div>
<div class="amsmath math notranslate nohighlight" id="equation-af7804a0-d642-495b-9bdd-0cb48ec76439">
<span class="eqno">(17.4)<a class="headerlink" href="#equation-af7804a0-d642-495b-9bdd-0cb48ec76439" title="Permalink to this equation">¶</a></span>\[\begin{equation}
p(\lambda|B,I) = \frac{1}{\lambda_\mathrm{max}-\lambda_\mathrm{min}} \quad \text{for } \lambda_\mathrm{min} \leq \lambda \leq \lambda_\mathrm{max}, 
\label{eq:sivia_44}
\end{equation}\]</div>
<div class="math notranslate nohighlight">
\[
and zero otherwise. Let us also take it that there is a value $\lambda_0$ which yields the closest agreement with the measurements; the corresponding probability $p(D|\lambda_0,B,I)$ will be the maximum of B’s likelihood function. As long as this adjustable parameter lies in the neighbourhood of the optimal value, $\lambda_0 \pm (\delta\lambda)$, we would expect a reasonable fit to the data; this can be represented by the Gaussian pdf
\]</div>
<div class="amsmath math notranslate nohighlight" id="equation-028cc3eb-c48b-44cc-98ee-f4944fff7c1e">
<span class="eqno">(17.5)<a class="headerlink" href="#equation-028cc3eb-c48b-44cc-98ee-f4944fff7c1e" title="Permalink to this equation">¶</a></span>\[\begin{equation}
p(D|\lambda,B,I) = p(D|\lambda_0,B,I) \exp \left[ − \frac{(\lambda−\lambda_0)^2}{2(\delta\lambda)^2} \right]. 
\label{eq:sivia_45}
\end{equation}\]</div>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}
The assignments of the prior ([eq:sivia_44](#eq:sivia_44)) and the likelihood ([eq:sivia_45](#eq:sivia_45)) are illustrated in Fig. [fig:sivia_41](#fig:sivia_41). We may note that, unlike the prior pdf $p(\lambda|B,I)$, B’s likelihood function need not be normalized with respect to $\lambda$; in other words, $p(D|\lambda_0,B,I)$ need not equal $1/ (\delta\lambda) \sqrt{2\pi}$ . This is because the $\lambda$ in $p(D|\lambda,B,I)$ appears in the conditioning statement, whereas the normalization requirement applies to quantities to the left of the ‘|’ symbol.\\&lt;!-- &lt;img src=&quot;fig/fig41.png&quot; width=500&gt;&lt;p&gt;&lt;em&gt;A schematic representation of the prior pdf (dashed line) and the likelihood function (solid line) for the parameter $\lambda$ in Prof B’s theory. &lt;div id=&quot;fig:sivia_41&quot;&gt;&lt;/div&gt;&lt;/em&gt;&lt;/p&gt; --&gt;
![&lt;p&gt;&lt;em&gt;A schematic representation of the prior pdf (dashed line) and the likelihood function (solid line) for the parameter $\lambda$ in Prof B’s theory. &lt;div id=&quot;fig:sivia_41&quot;&gt;&lt;/div&gt;&lt;/em&gt;&lt;/p&gt;](fig/fig41.png)\\In the evaluation of $p(D | B , I )$, we can make use of the fact that the prior does not depend explicitly on $\lambda$; this enables us to take $p(\lambda|B,I)$ outside the integral in Eq. ([eq:sivia_43](#eq:sivia_43))
\end{aligned}\end{align} \]</div>
<div class="amsmath math notranslate nohighlight" id="equation-b5bcb501-a213-45f8-b4d8-ff520df2af20">
<span class="eqno">(17.6)<a class="headerlink" href="#equation-b5bcb501-a213-45f8-b4d8-ff520df2af20" title="Permalink to this equation">¶</a></span>\[\begin{equation}
p(D|B,I) = \frac{1}{\lambda_\mathrm{max} - \lambda_\mathrm{min}} \int_{\lambda_\mathrm{min}}^{\lambda_\mathrm{max}} d\lambda
p(D|\lambda,B,I),
\label{eq:sivia_46}
\end{equation}\]</div>
<div class="math notranslate nohighlight">
\[
having set the limits according to the specified range. Assuming that the sharp cut-offs
do not cause a significant truncation of the Gaussian pdf of the likelihood,
its integral will be equal to $(\delta\lambda) \sqrt{2\pi}$ times $p(D|\lambda_0,B,I)$. The troublesome term then reduces to
\]</div>
<p>p(D|B,I) = \frac{1}{\lambda_\mathrm{max} - \lambda_\mathrm{min}} p(D|\lambda_0,B,I) (\delta\lambda) \sqrt{2\pi}.
$<span class="math notranslate nohighlight">\(
Substituting this into Eq. ([eq:sivia_42](#eq:sivia_42)), we finally see that the ratio of the posteriors required to answer our original question decomposes into the product of three terms:
\)</span>$</p>
<div class="amsmath math notranslate nohighlight" id="equation-02f5971f-12ee-4538-9303-1b2246037367">
<span class="eqno">(17.7)<a class="headerlink" href="#equation-02f5971f-12ee-4538-9303-1b2246037367" title="Permalink to this equation">¶</a></span>\[\begin{equation}
\frac{p(A|D,I)}{p(B|D,I)} =  \frac{p(A|I)}{p(B|I)} \times \frac{p(D|A,I)}{p(D|\lambda_0,B,I)} \times \frac{\lambda_\mathrm{max} - \lambda_\mathrm{min}}{(\delta\lambda) \sqrt{2\pi}}. 
\label{eq:sivia_48}
\end{equation}\]</div>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}
The first term on the right-hand side reflects our relative prior preference for the alternative theories; to be fair, we can take it to be unity. The second term is a measure of how well the best predictions from each of the models agree with the data; with the added flexibility of his adjustable parameter, this maximum likelihood ratio can only favour B. The goodness-of-fit, however, cannot be the only thing that matters; if it was, we would always prefer more complicated explanations. Probability theory tells us that there is, indeed, another term to be considered. As assumed earlier in the evaluation of the marginal integral of Eq. ([eq:sivia_43](#eq:sivia_43)), the prior range $\lambda_\mathrm{max} - \lambda_\mathrm{min}$ will generally be much larger than the uncertainty $\pm(\delta\lambda)$ permitted by the data. As such, the final term in Eq. ([eq:sivia_48](#eq:sivia_48)) acts to penalize B for the additional parameter; for this reason, it is often called an Ockham factor. That is to say, we have naturally encompassed the spirit of Ockham’s Razor: *‘Frustra fit per plura quod potest fieri per pauciora’* or, in English, *‘it is vain to do with more what can be done with fewer’* .\\Although it is satisfying to quantify the everyday guiding principle attributed to the thirteenth-century Franciscan monk William of Ockham (or Occam, in Latin), that we should prefer the simplest theory which agrees with the empirical evidence, we should not get too carried away by it. After all, what do we mean by the simpler theory if alternative models have the same number of adjustable parameters? In the choice between Gaussian and Lorentzian peak shapes, for example, both are defined by the position of the maximum and their width. All that we are obliged to do, and have done, in addressing such questions is to adhere to the rules of probability.\\While accepting the clear logic leading to Eq ([eq:sivia_48](#eq:sivia_48)), many people rightly worry about the question of the limits $\lambda_\mathrm{min}$ and $\lambda_\mathrm{max}$. Jeffreys (1939) himself was concerned and pointed out that there would be an infinite penalty for any new parameter if the range was allowed to go to $\pm\infty$. Stated in the abstract, this would appear to be a severe limitation. In practice, however, it is not generally such a problem: since the analysis is always used in specific contexts, a suitable choice can usually be made on the basis of the relevant background information. Even in uncharted territory, a small amount of thought soon reveals that our state of ignorance is always far from the $\pm\infty$ scenario. If $\lambda$ was the coupling constant (or strength) for a possible fifth force, for example, then we could put an upper bound on its magnitude because everybody would have noticed it by now if it had been large enough! We should also not lose sight of the fact that the precise form of Eq ([eq:sivia_48](#eq:sivia_48)) stems from our stated simplifying approximations; if these are not appropriate, then eqns ([eq:sivia_42](#eq:sivia_42)) and ([eq:sivia_43](#eq:sivia_43)) will lead us to a somewhat different formula.\\In most cases, our relative preference for A or B is dominated by the goodness of the fit to the data; that is to say, the maximum likelihood ratio in eqn ([eq:sivia_48](#eq:sivia_48)) tends to overwhelm the contributions of the other two terms. The Ockham factor can play a crucial role, however, when both theories give comparably good agreement with the measurements. Indeed, it becomes increasingly important if B’s theory fails to give a significantly better fit as the quality of the data improves. In that case, $(\delta\lambda)$ continues to become smaller but the ratio of best-fit likelihoods remains close to unity; according to Eq. ([eq:sivia_48](#eq:sivia_48)), therefore, A’s theory is favoured ever more strongly. By the same token, the Ockham effect disappears if the data are either few in number, of poor quality or just fail to shed new light on the problem at hand. This is simply because the posterior ratio of Eq. ([eq:sivia_48](#eq:sivia_48)) is then roughly equal to the complementary prior one, since the empirical evidence is very weak; hence, there is no inherent preference for A’s theory unless it is explicitly encoded in $p(A|I)/p(B|I)$. This property can be verified formally by going back to Eqs. ([eq:sivia_42](#eq:sivia_42)), ([eq:sivia_45](#eq:sivia_45)) and ([eq:sivia_46](#eq:sivia_46)), and considering the poor-data limit in which 
$(\delta\lambda) \gg \lambda_\mathrm{max}-\lambda_\mathrm{min}$ and $p(D|\lambda_0,B,I) \approx p(D|A,I)$.\\&lt;!-- !split --&gt;
#### One adjustable parameter each\\Some further interesting features arise when we consider the case where Dr A also has one adjustable parameter; call it $\mu$. If we make the same sort of probability assignments, and simplifying approximations, as for Prof B, then we find that
\end{aligned}\end{align} \]</div>
<div class="amsmath math notranslate nohighlight" id="equation-20217d4e-d026-4626-a914-4d630093e227">
<span class="eqno">(17.8)<a class="headerlink" href="#equation-20217d4e-d026-4626-a914-4d630093e227" title="Permalink to this equation">¶</a></span>\[\begin{equation}
\frac{p(A|D,I)}{p(B|D,I)} =  \frac{p(A|I)}{p(B|I)} \times \frac{p(D|\mu_0,A,I)}{p(D|\lambda_0,B,I)} \times \frac{\delta\mu(\lambda_\mathrm{max} - \lambda_\mathrm{min})}{\delta\lambda(\mu_\mathrm{max} - \mu_\mathrm{min})}. 
\label{eq:sivia_49}
\end{equation}\]</div>
<div class="math notranslate nohighlight">
\[&lt;!-- !split --&gt;
This could represent the situation where we have to choose between a Gaussian and Lorentzian shape for a signal peak, but one associated parameter is not known. The position of the maximum may be fixed at the origin by theory, for example, and the amplitude constrained by the normalization of the data; A and B could then be the hypotheses favouring the alternative lineshapes, where $\delta\mu$ and $\delta\lambda$ are their related full-width-half-maxima. If we give equal weight to A and B before the analysis, and assign a similar large prior range for both $\mu$ and $\lambda$, then Eq. ([eq:sivia_49](#eq:sivia_49)) reduces to
\]</div>
<p>\frac{p(A|D,I)}{p(B|D,I)} \approx  \frac{p(D|\mu_0,A,I)}{p(D|\lambda_0,B,I)} \times \frac{\delta\mu}{\delta\lambda}.
$$</p>
<!-- !split -->
<p>For data of good quality, the dominant factor will tend to be the best-fit likelihood ratio. If both give comparable agreement with the measurements, however, then the shape with the larger error-bar for its associated parameter will be favoured. At first sight, it might seem rather odd that the less discriminating theory can gain the upper hand. It appears less strange once we realize that, in the context of model selection, a larger ‘error-bar’ means that more parameter values are consistent with the given hypothesis; hence its preferential treatment.</p>
<!-- !split -->
<div class="section" id="one-adjustable-parameter-each-different-prior-ranges">
<h3><span class="section-number">17.1.1. </span>One adjustable parameter each; different prior ranges<a class="headerlink" href="#one-adjustable-parameter-each-different-prior-ranges" title="Permalink to this headline">¶</a></h3>
<p>Finally, we can also consider the situation where Mr A and Mr B have the same physical theory but assign a different prior range for <span class="math notranslate nohighlight">\(\lambda\)</span> (or <span class="math notranslate nohighlight">\(\mu\)</span>). Although Eq. (<a class="reference external" href="#eq:sivia_48">eq:sivia_48</a>) can be seen as representing the case when <span class="math notranslate nohighlight">\((\mu_\mathrm{max} - \mu_\mathrm{min})\)</span> is infinitesimally small, so that A has no flexibility, Eq. (<a class="reference external" href="#eq:sivia_49">eq:sivia_49</a>) is more appropriate when the limits set by both theorists are large enough to encompass all the parameter values giving a reasonable fit to the data. With equal initial weighting towards A and B, the latter reduces to
$<span class="math notranslate nohighlight">\(
\frac{p(A|D,I)}{p(B|D,I)} =  \frac{\lambda_\mathrm{max} - \lambda_\mathrm{min}}{\mu_\mathrm{max} - \mu_\mathrm{min}}. 
\)</span><span class="math notranslate nohighlight">\(
because the best-fit likelihood ratio will be unity (since \)</span>\lambda_0 = \mu_0<span class="math notranslate nohighlight">\() and \)</span>\delta\lambda = \delta\mu$. Thus, our analysis will lead us to prefer the theorist who gives the narrower prior range; this is not unreasonable as he must have had some additional insight to be able to predict the value of the parameter more accurately.</p>
<!-- !split -->
</div>
</div>
<div class="section" id="comparison-with-parameter-estimation">
<h2><span class="section-number">17.2. </span>Comparison with parameter estimation<a class="headerlink" href="#comparison-with-parameter-estimation" title="Permalink to this headline">¶</a></h2>
<p>The dependence of the result in Eq. (<a class="reference external" href="#eq:sivia_48">eq:sivia_48</a>) on the prior range <span class="math notranslate nohighlight">\((\lambda_\mathrm{max} - \lambda_\mathrm{min})\)</span> can seem a little strange, since we haven’t encountered such behaviour in the preceding chapters. It is instructive, therefore, to compare the model selection analysis with parameter estimation. To infer the value of <span class="math notranslate nohighlight">\(\lambda\)</span> from the data, given that B’s theory is correct, we use Bayes’ theorem:
$$</p>
<div class="amsmath math notranslate nohighlight" id="equation-d791a57a-d684-41be-a480-36d25dc6a677">
<span class="eqno">(17.9)<a class="headerlink" href="#equation-d791a57a-d684-41be-a480-36d25dc6a677" title="Permalink to this equation">¶</a></span>\[\begin{equation}
p(\lambda|D,B,I) = \frac{p(D|\lambda,B,I) p(\lambda|B,I)}{p(D|B,I)}. 
\label{eq:sivia_410}
\end{equation}\]</div>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}
The numerator is the familiar product of a prior and likelihood, and the denominator is usually omitted since it does not depend explicitly on $\lambda$; hence this relationship is often written as a proportionality. From the story of Dr A and Prof B, however, we find that the neglected term on the bottom plays a crucial role in ascertaining the merit of B’s theory relative to a competing alternative. \\&lt;!-- !split --&gt;
* In recognition of its new-found importance, the denominator in Bayes’ theorem is sometimes called the **‘evidence’** for B; it is also referred to as the ‘marginal likelihood’, the ‘global likelihood’ and the ‘prior predictive’. 
* Since all the components necessary for both parameter estimation and model selection appear in Eq. ([eq:sivia_410](#eq:sivia_410)), we are not dealing with any new principles; the only thing that sets them apart is that we are asking different questions of the data.\\&lt;!-- !split --&gt;
A simple way to think about the difference between parameter estimation and model selection is to note that, to a good approximation, the former requires the location of the maximum of the likelihood function whereas the latter entails the calculation of its average value. As long as $\lambda_\mathrm{min}$ and $\lambda_\mathrm{max}$ encompass the significant region of $p(D|\lambda,B,I)$ around $\lambda_0$, the precise bounds do not matter for estimating the optimal parameter and need not be specified. Since the prior range defines the domain over which the mean likelihood is computed, due thought is necessary when dealing with model selection. Indeed, it is precisely this act of comparing ‘average’ likelihoods rather than ‘maximum’ ones which introduces the desired Ockham balance to the goodness- of-fit criterion. Any likelihood gain from a better agreement with the data, allowed by the greater flexibility of a more complicated model, has to be weighed against the additional cost of averaging it over a larger parameter space.\\&lt;!-- !split --&gt;
## Evidence calculations
The actual computation of Bayesian evidences can be a challenging task. Recall that we often have knowledge of the posterior distribution only through sampling. In many cases, the simple Laplace method can be used to compute the evidence approximately, while in other cases we have to rely on special sampling algorithms such as nested sampling or parallel tempering with thermodynamic integration.\\&lt;!-- ===== Laplace's method ===== --&gt;
&lt;!-- !split --&gt;
### Laplace's method
The idea behind the Laplace approximation is simple. We assume that an unnormalized probability density $P^*(\theta)$ has a peak at a point $\theta_0$. We are interested in the evidence, $Z_P$, which is given by the normalizing constant
\end{aligned}\end{align} \]</div>
<p>Z_P = \int P^*(\theta) d^K\theta,
$<span class="math notranslate nohighlight">\( 
where we consider the general case in which \)</span>\theta<span class="math notranslate nohighlight">\( is in a \)</span>K$-dimensional space.</p>
<!-- !split -->
<p>We Taylor-expand the logarithm <span class="math notranslate nohighlight">\(\log P^*\)</span> around the peak:
$<span class="math notranslate nohighlight">\(
\log P^*(\theta) = \log P^*(\theta_0) - \frac{1}{2} (\theta - \theta_0)^T \Sigma^{-1} (\theta - \theta_0) + \ldots,
\)</span><span class="math notranslate nohighlight">\(
where \)</span>\Sigma^{-1} = H<span class="math notranslate nohighlight">\( is the (Hessian) matrix of second derivatives at the maximum
\)</span><span class="math notranslate nohighlight">\(
H_{ij} = - \left. \frac{\partial^2}{\partial \theta_i \partial \theta_j}  \log P^*(\theta)\right|_{\theta=\theta_0}.
\)</span>$</p>
<!-- !split -->
<p>We then approximate <span class="math notranslate nohighlight">\(P^*(\theta)\)</span> by an unnormalized Gaussian,
$<span class="math notranslate nohighlight">\(
Q^*(\theta) \equiv P^*(\theta_0) \exp \left[ - \frac{1}{2}(\theta - \theta_0)^T \Sigma^{-1} (\theta - \theta_0) \right],
\)</span><span class="math notranslate nohighlight">\(
and we approximate the normalizing constant \)</span>Z_P<span class="math notranslate nohighlight">\( by the normalizing constant
of this Gaussian,
\)</span><span class="math notranslate nohighlight">\(
Z_P \approx Z_Q = P^*(\theta_0) \sqrt{\frac{(2\pi)^K}{\det\Sigma^{-1}}}.
\)</span>$
Predictions can then be made using this approximation. Physicists also call this widely-used approximation the saddle-point approximation.</p>
<!-- !split -->
<p>Note, in particular, that if we consider a chi-squared pdf: <span class="math notranslate nohighlight">\(P^*(\theta) = \exp \left( -\frac{1}{2} \chi^2(\theta)\right)\)</span>, then we get
$<span class="math notranslate nohighlight">\(
Z_P \approx \exp \left( -\frac{1}{2} \chi^2(\theta_0)\right) \sqrt{\frac{(4\pi)^K}{\det\Sigma^{-1}}},
\)</span><span class="math notranslate nohighlight">\(
where there is a factor \)</span>2^{K/2}<span class="math notranslate nohighlight">\( that comes from the extra factor \)</span>1/2<span class="math notranslate nohighlight">\( multiplying the covariance matrix \)</span>\Sigma^{-1}<span class="math notranslate nohighlight">\( and therefore appearing in all \)</span>K$ eigenvalues.</p>
<!-- !split -->
<p>The fact that the normalizing constant of a Gaussian is given by
$<span class="math notranslate nohighlight">\(
\int d^K\theta \exp \left[ - \frac{1}{2}\theta^T \Sigma^{-1} \theta \right] = \sqrt{\frac{(2\pi)^K}{\det\Sigma^{-1}}},
\)</span><span class="math notranslate nohighlight">\(
can be proved by making an orthogonal transformation into the basis \)</span>u<span class="math notranslate nohighlight">\( in which \)</span>\Sigma<span class="math notranslate nohighlight">\( is transformed into a diagonal matrix. The integral then separates into a product of one-dimensional integrals, each of the form
\)</span><span class="math notranslate nohighlight">\(
\int du_i \exp \left[ -\frac{1}{2} \lambda_i u_i^2 \right] = \sqrt{\frac{2\pi}{\lambda_i}}
\)</span><span class="math notranslate nohighlight">\(
The product of the eigenvalues \)</span>\lambda_i<span class="math notranslate nohighlight">\( is the determinant of \)</span>\Sigma^{-1}$.</p>
<!-- !split -->
<p>Note that the Laplace approximation is basis-dependent: if <span class="math notranslate nohighlight">\(\theta\)</span> is transformed to a nonlinear function <span class="math notranslate nohighlight">\(u(\theta)\)</span> and the density is transformed to <span class="math notranslate nohighlight">\(P(u) = P(\theta) |d\theta/du|\)</span> then in general the approximate normalizing constants <span class="math notranslate nohighlight">\(Z_Q\)</span> will be different. This can be viewed as a defect—since the true value <span class="math notranslate nohighlight">\(Z_P\)</span> is basis-independent in this approximation—or an opportunity, because we can hunt for a choice of basis in which the Laplace approximation is most accurate.</p>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
        <div class='prev-next-bottom'>
            
    <a class='left-prev' id="prev-link" href="demo-MaxEnt.html" title="previous page"><span class="section-number">15. </span>Assigning probabilities</a>
    <a class='right-next' id="next-link" href="GaussianProcesses.html" title="next page"><span class="section-number">1. </span>Inference using Gaussian processes</a>

        </div>
        
        </div>
    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Christian Forssén<br/>
        
            &copy; Copyright 2020.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  
  </body>
</html>