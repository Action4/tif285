
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>3. Exercise: Gaussian processes &#8212; Learning from data</title>
    
  <link href="_static/css/theme.css" rel="stylesheet" />
  <link href="_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="_static/sphinx-book-theme.acff12b8f9c144ce68a297486a2fa670.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="_static/js/index.1c5a1a01449ed65a7b51.js">

    <script id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/togglebutton.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="4. Logistic Regression" href="LogReg.html" />
    <link rel="prev" title="2. Gaussian processes demonstration" href="demo-GaussianProcesses.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="index.html">
      
      <img src="_static/logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Learning from data</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="welcome.html">
   Course aim
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Introduction
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="Intro.html">
   1. Learning from data: Inference
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="demo-Intro.html">
   2. Reading Data and fitting
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="exercise_Jupyter_Python_intro_01.html">
   4. Python and Jupyter notebooks: part 01
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="exercise_Jupyter_Python_intro_02.html">
   5. Python and Jupyter notebooks: part 02
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Linear regression
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="LinearRegression.html">
   1. Linear regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="ModelValidation.html">
   2. Model validation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="demo-ModelValidation.html">
   4. Linear Regression and Model Validation demonstration
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="exercise_LinearRegression.html">
   5. Linear Regression exercise
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Bayesian statistics
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="BayesianBasics.html">
   1. Statistical inference
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="BayesianRecipe.html">
   2. The Bayesian recipe
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="demo-BayesianBasics.html">
   3. Interactive Bayesian Coin Tossing
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="exercise_sum_product_rule.html">
   4. Checking the sum and product rules, and their consequences
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="BayesianParameterEstimation.html">
   5. Inference With Parametric Models
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="demo-BayesianParameterEstimation.html">
   6. Bayesian parameter estimation demonstration
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="exercise_BayesianParameterEstimation.html">
   7. Exercise: Bayesian parameter estimation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="MCMC.html">
   8. Why MCMC
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="demo-MCMC.html">
   10. Metropolis-Hasting MCMC sampling of a Poisson distribution
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="ErrorPropagation.html">
   11. Why Bayes is Better
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="demo-ErrorPropagation.html">
   12. Learning from Data: Error propagation and nuisance parameters (demonstration)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="MaxEnt.html">
   15. Ignorance pdfs: Indifference and translation groups
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="demo-MaxEnt.html">
   16. Assigning probabilities
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="ModelSelection.html">
   17. Frequentist hypothesis testing
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Machine learning---A Bayesian perspective
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="GaussianProcesses.html">
   1. Inference using Gaussian processes
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="demo-GaussianProcesses.html">
   2. Gaussian processes demonstration
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   3. Exercise: Gaussian processes
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="LogReg.html">
   4. Logistic Regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="NeuralNet.html">
   6. Neural networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="demo-NeuralNet.html">
   7. Neural network classifier demonstration
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="exercises_LogReg_NeuralNet.html">
   8. Exercise: Logistic Regression and neural networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="cnn.html">
   9. Convolutional Neural Networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="demo-cnn.html">
   10. Convolutional Neural Network
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="bnn.html">
   13. Bayesian neural networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="demo-bnn.html">
   14. Variational Inference: Bayesian Neural Networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="exercises_BNN.html">
   15. Exercise: Bayesian neural networks
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="_sources/exercise_GP.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#import-modules">
   3.1. Import modules
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#part-1">
   3.2. Part 1
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#getting-started-the-covariance-function">
     3.2.1. Getting started: The Covariance Function
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#setting-covariance-function-parameters">
     3.2.2. Setting Covariance Function Parameters
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#exercise-1">
     3.2.3. Exercise 1
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#covariance-functions-in-gpy">
     3.2.4. Covariance Functions in GPy
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#computing-the-covariance-function-given-the-input-data-mathbf-x">
     3.2.5. Computing the Covariance Function given the Input Data,
     <span class="math notranslate nohighlight">
      \(\mathbf{X}\)
     </span>
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#question">
     3.2.6. Question
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#combining-covariance-functions">
     3.2.7. Combining Covariance Functions
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#exercise-2">
     3.2.8. Exercise 2
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#combining-covariance-functions-in-gpy">
     3.2.9. Combining Covariance Functions in GPy
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#part-2">
   3.3. Part 2
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#sampling-from-a-gaussian-process">
     3.3.1. Sampling from a Gaussian Process
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#exercise-3">
     3.3.2. Exercise 3
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#part-3">
   3.4. Part 3
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#a-gaussian-process-regression-model">
     3.4.1. A Gaussian Process Regression Model
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#exercise-4">
     3.4.2. Exercise 4
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#covariance-function-parameter-estimation">
     3.4.3. Covariance Function Parameter Estimation
    </a>
   </li>
  </ul>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="exercise-gaussian-processes">
<h1><span class="section-number">3. </span>Exercise: Gaussian processes<a class="headerlink" href="#exercise-gaussian-processes" title="Permalink to this headline">¶</a></h1>
<p>Last revised: 09-Oct-2019 by Christian Forssén [christian.forssen&#64;chalmers.se]</p>
<p><em>Adapted from the Gaussian Process Summer School (written by Nicolas Durrande, Neil Lawrence and James Hensman)</em></p>
<p>The aim of this exercise is to illustrate the concepts of Gaussian processes. We will focus on three aspects of GPs: the kernel, the random sample paths and the GP regression model.</p>
<p>We will use the well known <a class="reference external" href="https://sheffieldml.github.io/GPy/">GPy package</a> by the Sheffield ML group.</p>
<p>The current draft of the online documentation of GPy is available from <a class="reference external" href="http://gpy.readthedocs.org/en/latest/">this page</a>.</p>
<div class="section" id="import-modules">
<h2><span class="section-number">3.1. </span>Import modules<a class="headerlink" href="#import-modules" title="Permalink to this headline">¶</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%</span><span class="k">matplotlib</span> inline
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="nn">GPy</span>

<span class="c1"># Not really needed, but nicer plots</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="n">sns</span><span class="o">.</span><span class="n">set</span><span class="p">()</span>
<span class="n">sns</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="s2">&quot;talk&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="part-1">
<h2><span class="section-number">3.2. </span>Part 1<a class="headerlink" href="#part-1" title="Permalink to this headline">¶</a></h2>
<div class="section" id="getting-started-the-covariance-function">
<h3><span class="section-number">3.2.1. </span>Getting started: The Covariance Function<a class="headerlink" href="#getting-started-the-covariance-function" title="Permalink to this headline">¶</a></h3>
<p>Let’s start with defining an exponentiated quadratic covariance function (also known as squared exponential or rbf or Gaussian) in one dimension:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">d</span> <span class="o">=</span> <span class="mi">1</span>          <span class="c1"># input dimension</span>
<span class="n">var</span> <span class="o">=</span> <span class="mf">1.</span>       <span class="c1"># variance</span>
<span class="n">theta</span> <span class="o">=</span> <span class="mf">0.2</span>    <span class="c1"># lengthscale</span>
<span class="n">k</span> <span class="o">=</span> <span class="n">GPy</span><span class="o">.</span><span class="n">kern</span><span class="o">.</span><span class="n">RBF</span><span class="p">(</span><span class="n">d</span><span class="p">,</span> <span class="n">variance</span><span class="o">=</span><span class="n">var</span><span class="p">,</span> <span class="n">lengthscale</span><span class="o">=</span><span class="n">theta</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>A summary of the kernel can be obtained using the command <code class="docutils literal notranslate"><span class="pre">print</span> <span class="pre">k</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">k</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>It is also possible to plot the kernel as a function of one of its inputs (whilst fixing the other) with <code class="docutils literal notranslate"><span class="pre">k.plot()</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span><span class="mi">6</span><span class="p">))</span>
<span class="n">k</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">);</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="setting-covariance-function-parameters">
<h3><span class="section-number">3.2.2. </span>Setting Covariance Function Parameters<a class="headerlink" href="#setting-covariance-function-parameters" title="Permalink to this headline">¶</a></h3>
<p>The value of the covariance function parameters can be accessed and modified using <code class="docutils literal notranslate"><span class="pre">k['.*var']</span></code> where the string in bracket is a regular expression matching the parameter name as it appears in <code class="docutils literal notranslate"><span class="pre">print(k)</span></code>. Let’s use this to get an insight into the effect of the parameters on the shape of the covariance function.</p>
<p>We’ll now use to set the lengthscale of the covariance to different values, and then plot the resulting covariance using the <code class="docutils literal notranslate"><span class="pre">k.plot()</span></code> method.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">k</span> <span class="o">=</span> <span class="n">GPy</span><span class="o">.</span><span class="n">kern</span><span class="o">.</span><span class="n">RBF</span><span class="p">(</span><span class="n">d</span><span class="p">)</span>     <span class="c1"># By default, the parameters are set to 1.</span>
<span class="n">theta</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">([</span><span class="mf">0.2</span><span class="p">,</span><span class="mf">0.5</span><span class="p">,</span><span class="mf">1.</span><span class="p">,</span><span class="mf">2.</span><span class="p">,</span><span class="mf">4.</span><span class="p">])</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span><span class="mi">6</span><span class="p">))</span>
<span class="k">for</span> <span class="n">it</span><span class="p">,</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">theta</span><span class="p">):</span>
    <span class="n">k</span><span class="o">.</span><span class="n">lengthscale</span><span class="o">=</span><span class="n">t</span>
    <span class="n">k</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;C</span><span class="si">{</span><span class="n">it</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span><span class="n">title</span><span class="o">=</span><span class="s1">&#39;lengthscale&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="exercise-1">
<h3><span class="section-number">3.2.3. </span>Exercise 1<a class="headerlink" href="#exercise-1" title="Permalink to this headline">¶</a></h3>
<p>a) What is the effect of the lengthscale parameter on the covariance function?</p>
<p>b) Now change the code used above for plotting the covariances associated with the length scale to see the influence of the variance parameter. What is the effect of the the variance parameter on the covariance function?</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Exercise 1 b) answer</span>
<span class="c1"># insert code here</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="covariance-functions-in-gpy">
<h3><span class="section-number">3.2.4. </span>Covariance Functions in GPy<a class="headerlink" href="#covariance-functions-in-gpy" title="Permalink to this headline">¶</a></h3>
<p>Many covariance functions are already implemented in GPy. Instead of rbf, try constructing and plotting the following  covariance functions: <code class="docutils literal notranslate"><span class="pre">Exponential</span></code>, <code class="docutils literal notranslate"><span class="pre">Matern32</span></code>, <code class="docutils literal notranslate"><span class="pre">Matern52</span></code>, <code class="docutils literal notranslate"><span class="pre">Brownian</span></code>, <code class="docutils literal notranslate"><span class="pre">Linear</span></code>, <code class="docutils literal notranslate"><span class="pre">Bias</span></code>,
<code class="docutils literal notranslate"><span class="pre">StdPeriodic</span></code>, <code class="docutils literal notranslate"><span class="pre">PeriodicExponential</span></code>, etc. Some of these covariance functions have more hyperparameters than just a variance and a lengthscale. Furthermore, not all kernels are stationary (i.e., they can’t all be written as <span class="math notranslate nohighlight">\(k ( x, y) = f ( x − y)\)</span>, see for example the Brownian
covariance function). For plotting, it may be interesting to change the value of the fixed input:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">kb1</span> <span class="o">=</span> <span class="n">GPy</span><span class="o">.</span><span class="n">kern</span><span class="o">.</span><span class="n">PeriodicExponential</span><span class="p">(</span><span class="n">input_dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">variance</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">lengthscale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">period</span><span class="o">=</span><span class="mf">6.28</span><span class="p">)</span>
<span class="n">kb2</span> <span class="o">=</span> <span class="n">GPy</span><span class="o">.</span><span class="n">kern</span><span class="o">.</span><span class="n">StdPeriodic</span><span class="p">(</span><span class="n">input_dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">variance</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">lengthscale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">period</span><span class="o">=</span><span class="mf">6.28</span><span class="p">)</span>
<span class="n">inputs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">2.</span><span class="p">,</span> <span class="mf">4.</span><span class="p">])</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span><span class="mi">6</span><span class="p">))</span>
<span class="k">for</span> <span class="n">ix</span><span class="p">,</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">inputs</span><span class="p">):</span>
    <span class="n">kb1</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">plot_limits</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">5</span><span class="p">],</span><span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;C</span><span class="si">{</span><span class="n">ix</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="n">kb2</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">plot_limits</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">5</span><span class="p">],</span><span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;C</span><span class="si">{</span><span class="n">ix</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span><span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="computing-the-covariance-function-given-the-input-data-mathbf-x">
<h3><span class="section-number">3.2.5. </span>Computing the Covariance Function given the Input Data, <span class="math notranslate nohighlight">\(\mathbf{X}\)</span><a class="headerlink" href="#computing-the-covariance-function-given-the-input-data-mathbf-x" title="Permalink to this headline">¶</a></h3>
<p>Let <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> be a <span class="math notranslate nohighlight">\(n\)</span> × <span class="math notranslate nohighlight">\(d\)</span> numpy array. Given a kernel <span class="math notranslate nohighlight">\(k\)</span>, the covariance matrix associated to
<span class="math notranslate nohighlight">\(\mathbf{X}\)</span> is obtained with <code class="docutils literal notranslate"><span class="pre">C</span> <span class="pre">=</span> <span class="pre">k.K(X,X)</span></code> . The positive semi-definiteness of <span class="math notranslate nohighlight">\(k\)</span> ensures that <code class="docutils literal notranslate"><span class="pre">C</span></code>
is a positive semi-definite (psd) matrix regardless of the initial points <span class="math notranslate nohighlight">\(\mathbf{X}\)</span>. This can be
checked numerically by looking at the eigenvalues:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">k</span> <span class="o">=</span> <span class="n">GPy</span><span class="o">.</span><span class="n">kern</span><span class="o">.</span><span class="n">Matern52</span><span class="p">(</span><span class="n">input_dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span>       <span class="c1"># 50*2 matrix of iid standard Gaussians</span>
<span class="n">C</span> <span class="o">=</span> <span class="n">k</span><span class="o">.</span><span class="n">K</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">X</span><span class="p">)</span>
<span class="n">eigvals</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">eigvals</span><span class="p">(</span><span class="n">C</span><span class="p">)</span>           <span class="c1"># Computes the eigenvalues of a matrix</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span><span class="mi">6</span><span class="p">))</span>
<span class="n">ax</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">eigvals</span><span class="p">)),</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">eigvals</span><span class="p">))</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">ylabel</span><span class="o">=</span><span class="s1">&#39;log(eig)&#39;</span><span class="p">,</span><span class="n">title</span><span class="o">=</span><span class="s1">&#39;Eigenvalues of the Matern 5/2 Covariance&#39;</span><span class="p">);</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="question">
<h3><span class="section-number">3.2.6. </span>Question<a class="headerlink" href="#question" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>Check this property for some other kernel and for a different set of points</p></li>
</ul>
</div>
<div class="section" id="combining-covariance-functions">
<h3><span class="section-number">3.2.7. </span>Combining Covariance Functions<a class="headerlink" href="#combining-covariance-functions" title="Permalink to this headline">¶</a></h3>
</div>
<div class="section" id="exercise-2">
<h3><span class="section-number">3.2.8. </span>Exercise 2<a class="headerlink" href="#exercise-2" title="Permalink to this headline">¶</a></h3>
<p>a) A matrix, <span class="math notranslate nohighlight">\(\mathbf{K}\)</span>, is positive semi-definite if the matrix inner product, <span class="math notranslate nohighlight">\(\mathbf{x}^\top \mathbf{K}\mathbf{x}\)</span> is greater than or equal to zero regardless of the values in <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>. Given this it should be easy to see that the sum of two positive semi-definite matrices is also positive semi-definite. In the context of Gaussian processes, this is the sum of two covariance functions. What does this mean from a modelling perspective?</p>
<p><em>Hint</em>: there are actually two related interpretations for this. Think about the properties of a Gaussian distribution, and where the sum of Gaussian variances arises.</p>
<p>b) What about the element-wise product of two covariance functions? In other words if we define</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
k(\mathbf{x}, \mathbf{x}^\prime) = k_1(\mathbf{x}, \mathbf{x}^\prime) k_2(\mathbf{x}, \mathbf{x}^\prime)
\end{align*}\]</div>
<p>then is <span class="math notranslate nohighlight">\(k(\mathbf{x}, \mathbf{x}^\prime)\)</span> a valid covariance function?</p>
</div>
<div class="section" id="combining-covariance-functions-in-gpy">
<h3><span class="section-number">3.2.9. </span>Combining Covariance Functions in GPy<a class="headerlink" href="#combining-covariance-functions-in-gpy" title="Permalink to this headline">¶</a></h3>
<p>In GPy you can easily combine covariance functions you have created using the sum and product operators, <code class="docutils literal notranslate"><span class="pre">+</span></code> and <code class="docutils literal notranslate"><span class="pre">*</span></code>. So, for example, if we wish to combine an exponentiated quadratic covariance with a Matern 5/2 then we can write</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">kern1</span> <span class="o">=</span> <span class="n">GPy</span><span class="o">.</span><span class="n">kern</span><span class="o">.</span><span class="n">RBF</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">variance</span><span class="o">=</span><span class="mf">1.</span><span class="p">,</span> <span class="n">lengthscale</span><span class="o">=</span><span class="mf">2.</span><span class="p">)</span>
<span class="n">kern2</span> <span class="o">=</span> <span class="n">GPy</span><span class="o">.</span><span class="n">kern</span><span class="o">.</span><span class="n">Matern52</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">variance</span><span class="o">=</span><span class="mf">2.</span><span class="p">,</span> <span class="n">lengthscale</span><span class="o">=</span><span class="mf">4.</span><span class="p">)</span>
<span class="n">kern</span> <span class="o">=</span> <span class="n">kern1</span> <span class="o">+</span> <span class="n">kern2</span>
<span class="nb">print</span><span class="p">(</span><span class="n">kern</span><span class="p">)</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span><span class="mi">6</span><span class="p">))</span>
<span class="n">kern</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">);</span>
</pre></div>
</div>
</div>
</div>
<p>Or if we wanted to multiply them we can write</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">kern</span> <span class="o">=</span> <span class="n">kern1</span><span class="o">*</span><span class="n">kern2</span>
<span class="nb">print</span><span class="p">(</span><span class="n">kern</span><span class="p">)</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span><span class="mi">6</span><span class="p">))</span>
<span class="n">kern</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">);</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="section" id="part-2">
<h2><span class="section-number">3.3. </span>Part 2<a class="headerlink" href="#part-2" title="Permalink to this headline">¶</a></h2>
<div class="section" id="sampling-from-a-gaussian-process">
<h3><span class="section-number">3.3.1. </span>Sampling from a Gaussian Process<a class="headerlink" href="#sampling-from-a-gaussian-process" title="Permalink to this headline">¶</a></h3>
<p>The Gaussian process provides a prior over an infinite dimensional function. It is defined by a covariance <em>function</em> and a mean <em>function</em>. When we compute the covariance matrix using <code class="docutils literal notranslate"><span class="pre">kern.K(X,</span> <span class="pre">X)</span></code> we are computing a covariance <em>matrix</em> between the values of the function that correspond to the input locations in the matrix <code class="docutils literal notranslate"><span class="pre">X</span></code>. If we want to have a look at the type of functions that arise from a particular Gaussian process we can never generate all values of the function, because there are infinite values. However, we can generate samples from a Gaussian <em>distribution</em> based on a covariance matrix associated with a particular matrix of input locations <code class="docutils literal notranslate"><span class="pre">X</span></code>. If these locations are chosen appropriately then they give us a good idea of the underlying function. For example, for a one dimensional function, if we choose <code class="docutils literal notranslate"><span class="pre">X</span></code> to be uniformly spaced across part of the real line, and the spacing is small enough, we’ll get an idea of the underlying function. We will now use this trick to draw sample paths from a Gaussian process.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">k</span> <span class="o">=</span> <span class="n">GPy</span><span class="o">.</span><span class="n">kern</span><span class="o">.</span><span class="n">RBF</span><span class="p">(</span><span class="n">input_dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="n">lengthscale</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mf">0.</span><span class="p">,</span><span class="mf">1.</span><span class="p">,</span><span class="mi">500</span><span class="p">)</span> <span class="c1"># define X to be 500 points evenly spaced over [0,1]</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:,</span><span class="kc">None</span><span class="p">]</span> <span class="c1"># reshape X to make it n*p --- GPy uses &#39;design matrices&#39;</span>

<span class="n">mu</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">))</span> <span class="c1"># vector of the means --- we could use a mean function here, but here it is just zero.</span>
<span class="n">C</span> <span class="o">=</span> <span class="n">k</span><span class="o">.</span><span class="n">K</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">X</span><span class="p">)</span> <span class="c1"># compute the covariance matrix associated with inputs X</span>

<span class="c1"># Generate &#39;nsamples&#39; separate samples paths from a Gaussian with mean mu and covariance C</span>
<span class="n">nsamples</span><span class="o">=</span><span class="mi">10</span>
<span class="n">Z</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">multivariate_normal</span><span class="p">(</span><span class="n">mu</span><span class="p">,</span><span class="n">C</span><span class="p">,</span><span class="n">nsamples</span><span class="p">)</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span><span class="mi">6</span><span class="p">))</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">nsamples</span><span class="p">):</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X</span><span class="p">[:],</span><span class="n">Z</span><span class="p">[</span><span class="n">i</span><span class="p">,:]);</span>
</pre></div>
</div>
</div>
</div>
<p>Our choice of <code class="docutils literal notranslate"><span class="pre">X</span></code> means that the points are close enough together to look like functions. We can see the structure of the covariance matrix we are plotting from if we visualize C.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span><span class="mi">8</span><span class="p">))</span>
<span class="n">ax</span><span class="o">.</span><span class="n">matshow</span><span class="p">(</span><span class="n">C</span><span class="p">);</span>
</pre></div>
</div>
</div>
</div>
<p>Now try a range of different covariance functions and values and plot the corresponding sample paths for each using the same approach given above.</p>
</div>
<div class="section" id="exercise-3">
<h3><span class="section-number">3.3.2. </span>Exercise 3<a class="headerlink" href="#exercise-3" title="Permalink to this headline">¶</a></h3>
<p>Modify the code below so that it plots the sampled paths from the nine different covariance structures that are generated.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">figure</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span><span class="mi">12</span><span class="p">),</span> <span class="n">tight_layout</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">kerns</span> <span class="o">=</span> <span class="p">[</span><span class="n">GPy</span><span class="o">.</span><span class="n">kern</span><span class="o">.</span><span class="n">RBF</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">GPy</span><span class="o">.</span><span class="n">kern</span><span class="o">.</span><span class="n">Exponential</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">GPy</span><span class="o">.</span><span class="n">kern</span><span class="o">.</span><span class="n">Matern32</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">GPy</span><span class="o">.</span><span class="n">kern</span><span class="o">.</span><span class="n">Matern52</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">GPy</span><span class="o">.</span><span class="n">kern</span><span class="o">.</span><span class="n">Brownian</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">GPy</span><span class="o">.</span><span class="n">kern</span><span class="o">.</span><span class="n">Bias</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">GPy</span><span class="o">.</span><span class="n">kern</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">GPy</span><span class="o">.</span><span class="n">kern</span><span class="o">.</span><span class="n">PeriodicExponential</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">GPy</span><span class="o">.</span><span class="n">kern</span><span class="o">.</span><span class="n">White</span><span class="p">(</span><span class="mi">1</span><span class="p">)]</span>


<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mf">0.</span><span class="p">,</span><span class="mf">1.</span><span class="p">,</span><span class="mi">500</span><span class="p">)</span> 
<span class="n">X</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:,</span><span class="kc">None</span><span class="p">]</span> 
<span class="n">mu</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">))</span> <span class="c1"># vector of the means --- we could use a mean function here, but here it is just zero.</span>
<span class="n">nsamples</span><span class="o">=</span><span class="mi">10</span>    
    
<span class="k">for</span> <span class="n">k</span><span class="p">,</span><span class="n">a</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">kerns</span><span class="p">,</span> <span class="n">axes</span><span class="o">.</span><span class="n">flatten</span><span class="p">()):</span>
    <span class="n">C</span> <span class="o">=</span> <span class="n">k</span><span class="o">.</span><span class="n">K</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">X</span><span class="p">)</span> <span class="c1"># compute the covariance matrix associated with inputs X</span>
    <span class="n">Z</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">multivariate_normal</span><span class="p">(</span><span class="n">mu</span><span class="p">,</span><span class="n">C</span><span class="p">,</span><span class="n">nsamples</span><span class="p">)</span>
    <span class="n">a</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="n">k</span><span class="o">.</span><span class="n">name</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s1">&#39;_&#39;</span><span class="p">,</span> <span class="s1">&#39; &#39;</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Enter code here</span>
<span class="c1">#</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="section" id="part-3">
<h2><span class="section-number">3.4. </span>Part 3<a class="headerlink" href="#part-3" title="Permalink to this headline">¶</a></h2>
<div class="section" id="a-gaussian-process-regression-model">
<h3><span class="section-number">3.4.1. </span>A Gaussian Process Regression Model<a class="headerlink" href="#a-gaussian-process-regression-model" title="Permalink to this headline">¶</a></h3>
<p>We will now combine the Gaussian process prior with some data to form a GP regression model with GPy. We will generate data from the function <span class="math notranslate nohighlight">\(f ( x ) = − \cos(\pi x ) + \sin(4\pi x )\)</span> over <span class="math notranslate nohighlight">\([0, 1]\)</span>, adding some noise to give <span class="math notranslate nohighlight">\(y(x) = f(x) + \epsilon\)</span>, with the noise being Gaussian distributed, <span class="math notranslate nohighlight">\(\epsilon \sim \mathcal{N}(0, 0.01)\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mf">0.05</span><span class="p">,</span><span class="mf">0.95</span><span class="p">,</span><span class="mi">10</span><span class="p">)[:,</span><span class="kc">None</span><span class="p">]</span>
<span class="n">Y</span> <span class="o">=</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="o">*</span><span class="n">X</span><span class="p">)</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="mi">4</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="o">*</span><span class="n">X</span><span class="p">)</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span> 
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span><span class="mi">6</span><span class="p">))</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">Y</span><span class="p">,</span><span class="s1">&#39;kx&#39;</span><span class="p">,</span><span class="n">mew</span><span class="o">=</span><span class="mf">1.5</span><span class="p">);</span>
</pre></div>
</div>
</div>
</div>
<p>A GP regression model based on an exponentiated quadratic covariance function can be defined by first defining a covariance function,</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">k</span> <span class="o">=</span> <span class="n">GPy</span><span class="o">.</span><span class="n">kern</span><span class="o">.</span><span class="n">RBF</span><span class="p">(</span><span class="n">input_dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">variance</span><span class="o">=</span><span class="mf">1.</span><span class="p">,</span> <span class="n">lengthscale</span><span class="o">=</span><span class="mf">1.</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>And then combining it with the data to form a Gaussian process model,</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">m</span> <span class="o">=</span> <span class="n">GPy</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">GPRegression</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">Y</span><span class="p">,</span><span class="n">k</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Just as for the covariance function object, we can find out about the model using the command <code class="docutils literal notranslate"><span class="pre">print(m)</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">m</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Note that by default the model includes some observation noise
with variance 1. We can see the posterior mean prediction and visualize the marginal posterior variances using <code class="docutils literal notranslate"><span class="pre">m.plot()</span></code>.</p>
<p><strong>Note:</strong> The <code class="docutils literal notranslate"><span class="pre">plot</span></code> command shows the mean of the GP model as well as the 95% confidence region.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">m</span><span class="o">.</span><span class="n">plot</span><span class="p">();</span>
</pre></div>
</div>
</div>
</div>
<p>The actual predictions of the model for a set of points <code class="docutils literal notranslate"><span class="pre">Xnew</span></code> can be computed using</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">Ynew</span><span class="p">,</span> <span class="n">Yvar</span> <span class="o">=</span> <span class="n">m</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">Xnew</span><span class="p">)</span>
</pre></div>
</div>
<p>Note that <code class="docutils literal notranslate"><span class="pre">Xnew</span></code> is an <span class="math notranslate nohighlight">\(m \times p\)</span> array, where <span class="math notranslate nohighlight">\(m\)</span> is the number of points that we want to predict and <span class="math notranslate nohighlight">\(p\)</span> is the dimensionality of the parameter space.</p>
<p>We can also extract the predictive quantiles around the prediction at <code class="docutils literal notranslate"><span class="pre">Xnew</span></code>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">Ylo95</span><span class="p">,</span> <span class="n">Yhi95</span> <span class="o">=</span> <span class="n">m</span><span class="o">.</span><span class="n">predict_quantiles</span><span class="p">(</span><span class="n">Xnew</span><span class="p">)</span>
</pre></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">Xnew</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span><span class="mf">1.5</span><span class="p">,</span><span class="mi">4</span><span class="p">)[:,</span><span class="kc">None</span><span class="p">]</span>
<span class="p">(</span><span class="n">Ymean</span><span class="p">,</span><span class="n">Yvar</span><span class="p">)</span> <span class="o">=</span> <span class="n">m</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">Xnew</span><span class="p">)</span>
<span class="c1"># quantiles=(2.5, 97.5) are default</span>
<span class="p">(</span><span class="n">Ylo95</span><span class="p">,</span><span class="n">Yhi95</span><span class="p">)</span> <span class="o">=</span> <span class="n">m</span><span class="o">.</span><span class="n">predict_quantiles</span><span class="p">(</span><span class="n">Xnew</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;  X   mean Y   var Y   lo95</span><span class="si">%   hi</span><span class="s2">95%&quot;</span><span class="p">)</span>
<span class="k">for</span> <span class="p">(</span><span class="n">Xi</span><span class="p">,</span><span class="n">Ymeani</span><span class="p">,</span><span class="n">Yvari</span><span class="p">,</span><span class="n">Yloi</span><span class="p">,</span><span class="n">Yhii</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">Xnew</span><span class="p">,</span><span class="n">Ymean</span><span class="p">,</span><span class="n">Yvar</span><span class="p">,</span><span class="n">Ylo95</span><span class="p">,</span><span class="n">Yhi95</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">Xi</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="s2">5.2f</span><span class="si">}</span><span class="s2">  </span><span class="si">{</span><span class="n">Ymeani</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="s2">5.2f</span><span class="si">}</span><span class="s2">   </span><span class="si">{</span><span class="n">Yvari</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="s2">5.2f</span><span class="si">}</span><span class="s2">   </span><span class="si">{</span><span class="n">Yloi</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="s2">5.2f</span><span class="si">}</span><span class="s2">   </span><span class="si">{</span><span class="n">Yhii</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="s2">5.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="exercise-4">
<h3><span class="section-number">3.4.2. </span>Exercise 4<a class="headerlink" href="#exercise-4" title="Permalink to this headline">¶</a></h3>
<p>a) What do you think about this first fit? Does the prior given by the GP seem to be
adapted?</p>
<p>b) The parameters of the models can be modified using a regular expression matching the parameters names (for example <code class="docutils literal notranslate"><span class="pre">m['Gaussian_noise.variance']</span> <span class="pre">=</span> <span class="pre">0.001</span></code> ). Change the values of the parameters to obtain a better fit.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Exercise 4 b) </span>
<span class="c1"># make a plot for a better fit here</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># KEY</span>
<span class="n">m</span><span class="p">[</span><span class="s1">&#39;Gaussian_noise.variance&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.01</span>
<span class="n">m</span><span class="p">[</span><span class="s1">&#39;rbf.lengthscale&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.1</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span><span class="mi">6</span><span class="p">))</span>
<span class="n">m</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">);</span>
</pre></div>
</div>
</div>
</div>
<p>c) As in Section 2, random sample paths from the conditional GP can be obtained using
<code class="docutils literal notranslate"><span class="pre">np.random.multivariate_normal(mu[:,0],C)</span></code> where the mean vector and covariance
matrix <code class="docutils literal notranslate"><span class="pre">mu</span></code>, <code class="docutils literal notranslate"><span class="pre">C</span></code> are obtained through the predict function <code class="docutils literal notranslate"><span class="pre">mu,</span> <span class="pre">C</span> <span class="pre">=</span> <span class="pre">m.predict(Xp,full_cov=True)</span></code>. Obtain 20 samples from the posterior sample and plot them alongside the data below. Compare the random sample paths to the 95% confidence region that is shown with the <code class="docutils literal notranslate"><span class="pre">m.plot()</span></code> command.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Exercise 4 c) answer</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="covariance-function-parameter-estimation">
<h3><span class="section-number">3.4.3. </span>Covariance Function Parameter Estimation<a class="headerlink" href="#covariance-function-parameter-estimation" title="Permalink to this headline">¶</a></h3>
<p>The kernel parameter values can be estimated by maximizing the <em>likelihood</em> of the observations. Since we don’t want one of the variances to become negative during the optimization, we can constrain all parameters to be positive before running the optimisation.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">m</span><span class="o">.</span><span class="n">constrain_positive</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<p>The warnings are because the parameters are already constrained by default, the software is warning us that they are being reconstrained.</p>
<p>Now we can optimize the model using the <code class="docutils literal notranslate"><span class="pre">m.optimize()</span></code> method.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">m</span><span class="o">.</span><span class="n">optimize</span><span class="p">()</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span><span class="mi">6</span><span class="p">))</span>
<span class="n">m</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">m</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>The parameters obtained after optimisation can be compared with the values selected by hand above. As previously, you can modify the kernel used for building the model to investigate its influence on the model.</p>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
        <div class='prev-next-bottom'>
            
    <a class='left-prev' id="prev-link" href="demo-GaussianProcesses.html" title="previous page"><span class="section-number">2. </span>Gaussian processes demonstration</a>
    <a class='right-next' id="next-link" href="LogReg.html" title="next page"><span class="section-number">4. </span>Logistic Regression</a>

        </div>
        
        </div>
    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Christian Forssén<br/>
        
            &copy; Copyright 2020.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  
  </body>
</html>