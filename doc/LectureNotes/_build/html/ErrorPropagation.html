
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>10. Why Bayes is Better &#8212; Learning from data</title>
    
  <link href="_static/css/theme.css" rel="stylesheet" />
  <link href="_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="_static/sphinx-book-theme.acff12b8f9c144ce68a297486a2fa670.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="_static/js/index.1c5a1a01449ed65a7b51.js">

    <script id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/togglebutton.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="11. Learning from Data: Error propagation and nuisance parameters (demonstration)" href="demo-ErrorPropagation.html" />
    <link rel="prev" title="9. Metropolis-Hasting MCMC sampling of a Poisson distribution" href="demo-MCMC.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="index.html">
      
      <img src="_static/logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Learning from data</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="welcome.html">
   Course aim
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Introduction
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="Intro.html">
   1. Learning from data: Inference
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="demo-Intro.html">
   2. Reading Data and fitting
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="exercise_Jupyter_Python_intro_01.html">
   4. Python and Jupyter notebooks: part 01
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="exercise_Jupyter_Python_intro_02.html">
   5. Python and Jupyter notebooks: part 02
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Linear regression
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="LinearRegression.html">
   1. Linear regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="ModelValidation.html">
   2. Model validation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="demo-ModelValidation.html">
   4. Linear Regression and Model Validation demonstration
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="exercise_LinearRegression.html">
   5. Linear Regression exercise
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Bayesian statistics
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="BayesianBasics.html">
   1. Statistical inference
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="BayesianRecipe.html">
   2. The Bayesian recipe
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="demo-BayesianBasics.html">
   3. Interactive Bayesian Coin Tossing
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="exercise_sum_product_rule.html">
   4. Checking the sum and product rules, and their consequences
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="BayesianParameterEstimation.html">
   5. Inference With Parametric Models
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="demo-BayesianParameterEstimation.html">
   6. Bayesian parameter estimation demonstration
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="exercise_BayesianParameterEstimation.html">
   7. Exercise: Bayesian parameter estimation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="MCMC.html">
   8. Why MCMC
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="demo-MCMC.html">
   9. Metropolis-Hasting MCMC sampling of a Poisson distribution
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   10. Why Bayes is Better
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="demo-ErrorPropagation.html">
   11. Learning from Data: Error propagation and nuisance parameters (demonstration)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="IgnorancePDF.html">
   14. Ignorance pdfs: Indifference and translation groups
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="MaxEnt.html">
   15. The principle of maximum entropy
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="demo-MaxEnt.html">
   16. Assigning probabilities
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="ModelSelection.html">
   17. Frequentist hypothesis testing
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Machine learning---A Bayesian perspective
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="GaussianProcesses.html">
   1. Inference using Gaussian processes
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="demo-GaussianProcesses.html">
   2. Gaussian processes demonstration
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="exercise_GP.html">
   3. Exercise: Gaussian processes
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="LogReg.html">
   4. Logistic Regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="NeuralNet.html">
   6. Neural networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="demo-NeuralNet.html">
   7. Neural network classifier demonstration
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="exercises_LogReg_NeuralNet.html">
   8. Exercise: Logistic Regression and neural networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="cnn.html">
   9. Convolutional Neural Networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="demo-cnn.html">
   10. Convolutional Neural Network
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="bnn.html">
   13. Bayesian neural networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="demo-bnn.html">
   14. Variational Inference: Bayesian Neural Networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="exercises_BNN.html">
   15. Exercise: Bayesian neural networks
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="_sources/ErrorPropagation.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#quotes-from-one-pioneering-and-one-renaissance-bayesian-authority">
   10.1. Quotes from one pioneering and one renaissance Bayesian authority
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#advantages-of-the-bayesian-approach">
   10.2. Advantages of the Bayesian approach
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#occam-s-razor">
     10.2.1. Occam’s razor
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#nuisance-parameters">
   10.3. Nuisance parameters
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#nuisance-parameters-i-bayesian-billiard">
     10.3.1. Nuisance parameters (I): Bayesian Billiard
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#nuisance-parameters-ii-marginal-distributions">
     10.3.2. Nuisance parameters (II): marginal distributions
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#error-propagation">
   10.4. Error propagation
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#error-propagation-i-marginalization">
     10.4.1. Error propagation (I): marginalization
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#error-propagation-ii-changing-variables-and-prior-information">
     10.4.2. Error propagation (II): changing variables and prior information
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#a-useful-short-cut">
     10.4.3. A useful short cut
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#example-taking-the-square-root-of-a-number">
     10.4.4. Example: Taking the square root of a number
    </a>
   </li>
  </ul>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <!-- !split -->
<div class="section" id="why-bayes-is-better">
<h1><span class="section-number">10. </span>Why Bayes is Better<a class="headerlink" href="#why-bayes-is-better" title="Permalink to this headline">¶</a></h1>
<div class="section" id="quotes-from-one-pioneering-and-one-renaissance-bayesian-authority">
<h2><span class="section-number">10.1. </span>Quotes from one pioneering and one renaissance Bayesian authority<a class="headerlink" href="#quotes-from-one-pioneering-and-one-renaissance-bayesian-authority" title="Permalink to this headline">¶</a></h2>
<blockquote class="epigraph">
<div><blockquote>
<div><p><em>“Probability theory is nothing but common sense reduced to calculation.”</em></p>
</div></blockquote>
<p class="attribution">—Laplace</p>
</div></blockquote>
<blockquote class="epigraph">
<div><blockquote>
<div><p><em>“Bayesian inference probabilities are a measure of our state of knowledge about nature, not a measure of nature itself.”</em></p>
</div></blockquote>
<p class="attribution">—Sivia</p>
</div></blockquote>
<!-- !split -->
</div>
<div class="section" id="advantages-of-the-bayesian-approach">
<h2><span class="section-number">10.2. </span>Advantages of the Bayesian approach<a class="headerlink" href="#advantages-of-the-bayesian-approach" title="Permalink to this headline">¶</a></h2>
<!-- !bpop -->
<ol class="simple">
<li><p>Provides an elegantly simple and rational approach for answering, in an optimal way, any scientific question for a given state of information. This contrasts to the recipe or cookbook approach of conventional statistical analysis. The procedure is well-defined:</p>
<ul class="simple">
<li><p>Clearly state your question and prior information.</p></li>
<li><p>Apply the sum and product rules. The starting point is always Bayes’ theorem.</p></li>
</ul>
</li>
<li><p>For some problems, a Bayesian analysis may simply lead to a familiar statistic. Even in this situation it often provides a powerful new insight concerning the interpretation of the statistic.</p></li>
<li><p>Incorporates relevant prior (e.g., known signal model or known theory model expansion) information through Bayes’ theorem. This is one of the great strengths of Bayesian analysis.</p>
<ul class="simple">
<li><p>For data with a small signal-to-noise ratio, a Bayesian analysis can frequently yield many orders of magnitude improvement in model parameter estimation, through the incorporation of relevant prior information about the signal model.</p></li>
</ul>
</li>
<li><p>Provides a way of eliminating nuisance parameters through marginalization. For some problems, the marginalization can be performed analytically, permitting certain calculations to become computationally tractable.</p></li>
<li><p>Provides a way for incorporating the effects of systematic errors arising from both the measurement operation and theoretical model predictions.</p></li>
<li><p>Calculates probability of hypothesis directly: <span class="math notranslate nohighlight">\(p(H_i|D, I)\)</span>.</p></li>
<li><p>Provides a more powerful way of assessing competing theories at the forefront of science by automatically quantifying Occam’s razor.</p></li>
</ol>
<!-- !epop -->
<!-- !split -->
<p>The Bayesian quantitative Occam’s razor can also save a lot of time that might otherwise be spent chasing noise artifacts that masquerade as possible detections of real phenomena.</p>
<!-- !split -->
<div class="section" id="occam-s-razor">
<h3><span class="section-number">10.2.1. </span>Occam’s razor<a class="headerlink" href="#occam-s-razor" title="Permalink to this headline">¶</a></h3>
<p>Occam’s razor is a principle attributed to the medieval philosopher William of Occam (or Ockham). The principle states that one should not make more assumptions than the minimum needed. It underlies all scientific modeling and theory building. It cautions us to choose from a set of otherwise equivalent models of a given phenomenon the simplest one. In any given model, Occam’s razor helps us to “shave off” those variables that are not really needed to explain the phenomenon. It was previously thought to be only a qualitative principle.</p>
<!-- !split -->
<!-- <img src="fig/ErrorPropagation/Leprechaun_or_Clurichaun.png" width=500><p><em>Did the Leprechaun drink your wine, or is there a simpler explanation?</em></p> -->
<p><img alt="Did the Leprechaun drink your wine, or is there a simpler explanation?" src="_images/Leprechaun_or_Clurichaun.png" /></p>
<!-- !split -->
</div>
</div>
<div class="section" id="nuisance-parameters">
<h2><span class="section-number">10.3. </span>Nuisance parameters<a class="headerlink" href="#nuisance-parameters" title="Permalink to this headline">¶</a></h2>
<div class="section" id="nuisance-parameters-i-bayesian-billiard">
<h3><span class="section-number">10.3.1. </span>Nuisance parameters (I): Bayesian Billiard<a class="headerlink" href="#nuisance-parameters-i-bayesian-billiard" title="Permalink to this headline">¶</a></h3>
<p>See demonstration notebook: A Bayesian Billiard game</p>
<!-- !split -->
</div>
<div class="section" id="nuisance-parameters-ii-marginal-distributions">
<h3><span class="section-number">10.3.2. </span>Nuisance parameters (II): marginal distributions<a class="headerlink" href="#nuisance-parameters-ii-marginal-distributions" title="Permalink to this headline">¶</a></h3>
<p>Assume that we have a model with two parameters, <span class="math notranslate nohighlight">\(\theta_0,\theta_1\)</span>, although only one of them (say <span class="math notranslate nohighlight">\(\theta_1\)</span>) is of physical relevance (the other one is them labeled a nuisance parameter). Through a Bayesian data analysis we have the joint, posterior pdf</p>
<div class="math notranslate nohighlight">
\[p(\theta_0, \theta_1 | D, I).\]</div>
<p>The marginal posterior pdf <span class="math notranslate nohighlight">\(p(\theta_1 | D, I)\)</span> is obtained via marginalization</p>
<div class="math notranslate nohighlight">
\[p(\theta_1 | D, I) = \int p(\theta_0, \theta_1 | D, I) d\theta_0.\]</div>
<p>Assume that we have <span class="math notranslate nohighlight">\(N\)</span> samples from the joint pdf. This might be the Markov Chain from an MCMC sampler: <span class="math notranslate nohighlight">\(\left\{ (\theta_0, \theta_1)_i \right\}_{i=0}^{N-1}\)</span>. Then the marginal distribution of <span class="math notranslate nohighlight">\(\theta_1\)</span> will be given by the same chain by simply ignoring the <span class="math notranslate nohighlight">\(\theta_0\)</span> column, i.e., <span class="math notranslate nohighlight">\(\left\{ \theta_{1,i} \right\}_{i=0}^{N-1}\)</span>.</p>
<p>See the interactive demos created by Chi Feng for an illustration of this: <a class="reference external" href="https://chi-feng.github.io/mcmc-demo/">The Markov-chain Monte Carlo Interactive Gallery</a>.</p>
<!-- !split -->
</div>
</div>
<div class="section" id="error-propagation">
<h2><span class="section-number">10.4. </span>Error propagation<a class="headerlink" href="#error-propagation" title="Permalink to this headline">¶</a></h2>
<div class="section" id="error-propagation-i-marginalization">
<h3><span class="section-number">10.4.1. </span>Error propagation (I): marginalization<a class="headerlink" href="#error-propagation-i-marginalization" title="Permalink to this headline">¶</a></h3>
<p>The Bayesian approach offers a straight-forward approach for dealing with (known) systematic uncertainties; namely by marginalization. Let us demonstrate this with an example \n</p>
<!-- !split -->
<p><strong>Inferring galactic distances with an imprecise knowledge of the Hubble constant</strong>
The Hubble constant acts as a galactic ruler as it is used to measure astronomical distances according to <span class="math notranslate nohighlight">\(v = H_0 x\)</span>. An error in this ruler will therefore correspond to a systematic uncertainty in such measurements.</p>
<p>Here we use marginalization to obtain the desired posterior pdf <span class="math notranslate nohighlight">\(p(x|D,I)\)</span> from the joint distribution of <span class="math notranslate nohighlight">\(p(x,H_0|D,I)\)</span></p>
<div class="amsmath math notranslate nohighlight" id="equation-810417cd-ae51-4054-b0b7-b6ffceee2f63">
<span class="eqno">(10.1)<a class="headerlink" href="#equation-810417cd-ae51-4054-b0b7-b6ffceee2f63" title="Permalink to this equation">¶</a></span>\[\begin{equation}
p(x|D,I) = \int_{-\infty}^\infty dH_0 p(x,H_0|D,I).
\label{eq:marginalization}
\end{equation}\]</div>
<p>Using Bayes’ rule: <span class="math notranslate nohighlight">\(p(x,H_0|D,I) \propto p(D|x,H_0,I) p(x,H_0|I)\)</span>, the product rule: <span class="math notranslate nohighlight">\(p(x,H_0|I) = p(H_0|x,I)p(x|I)\)</span>, and the fact that <span class="math notranslate nohighlight">\(H_0\)</span> is independent of <span class="math notranslate nohighlight">\(x\)</span>: <span class="math notranslate nohighlight">\(p(H_0|x,I) = p(H_0|I)\)</span>, we find that</p>
<div class="math notranslate nohighlight">
\[p(x|D,I) \propto p(x|I) \int dH_0 p(H_0|I) p(D|x,H_0,I),\]</div>
<p>which means that we have expressed the quantity that we want (the posterior of <span class="math notranslate nohighlight">\(x\)</span>) in terms of quantities that we know.</p>
<p>Assume that the pdf <span class="math notranslate nohighlight">\(p(H_0 | I)\)</span> is known via its <span class="math notranslate nohighlight">\(N\)</span> samples <span class="math notranslate nohighlight">\(\{H_{i}\}_{i=0}^{N-1}\)</span> generated by the MCMC sampler.</p>
<p>This means that we can approximate</p>
<div class="math notranslate nohighlight">
\[p(x |D,I) \propto \int dH_0 p(H_0|I) p(D|x,H_0,I) \approx \frac{1}{N} \sum_{i=1}^N p(D | x, H_i, I)\]</div>
<p>where we have used a uniform prior for the distance <span class="math notranslate nohighlight">\(p(x|I) \propto 1\)</span>.</p>
<!-- !split -->
</div>
<div class="section" id="error-propagation-ii-changing-variables-and-prior-information">
<h3><span class="section-number">10.4.2. </span>Error propagation (II): changing variables and prior information<a class="headerlink" href="#error-propagation-ii-changing-variables-and-prior-information" title="Permalink to this headline">¶</a></h3>
<p>(Based on Sivia, ch 3.6.)</p>
<p>Assume that we have measured parameter <span class="math notranslate nohighlight">\(X = 10 \pm 3\)</span> and <span class="math notranslate nohighlight">\(Y=7 \pm 2\)</span>; what can we say about the difference <span class="math notranslate nohighlight">\(X-Y\)</span> or the raio <span class="math notranslate nohighlight">\(X/Y\)</span>, or the sum of their squares <span class="math notranslate nohighlight">\(X^2+Y^2\)</span>, etc? In essence, the problem is nothing more than an exercise in the change of variables: given the joint pdf <span class="math notranslate nohighlight">\(p(X,Y|I)\)</span>, where the information <span class="math notranslate nohighlight">\(I\)</span> might include the data if the pdf is a posterior from a data analysis, we need the corresponding pdf <span class="math notranslate nohighlight">\(p(Z|I)\)</span>, where <span class="math notranslate nohighlight">\(Z=X-Y\)</span>, or <span class="math notranslate nohighlight">\(Z=X/Y\)</span>, or whatever as appropriate.</p>
<!-- !split -->
<p>Let us start with a single variable <span class="math notranslate nohighlight">\(X\)</span> and a function <span class="math notranslate nohighlight">\(Y=f(X)\)</span>. How is <span class="math notranslate nohighlight">\(p(X|I)\)</span> related to <span class="math notranslate nohighlight">\(p(Y|I)\)</span>?</p>
<p>Consider a point <span class="math notranslate nohighlight">\(X^*\)</span> and a small interval <span class="math notranslate nohighlight">\(\delta X\)</span> around it. The probability that <span class="math notranslate nohighlight">\(X\)</span> lies within that interval can be written</p>
<div class="math notranslate nohighlight">
\[p \left( X^* - \frac{\delta X}{2} \le X &lt; X^* + \frac{\delta X}{2} \big| I \right) 
\approx p(X=X^*|I) \delta X.\]</div>
<!-- !split -->
<p>Assume now that the function <span class="math notranslate nohighlight">\(f\)</span> will map the point <span class="math notranslate nohighlight">\(X=X^*\)</span> uniquely onto <span class="math notranslate nohighlight">\(Y=Y^*=f(X^*)\)</span>. Then there must be an interval <span class="math notranslate nohighlight">\(\delta Y\)</span> around <span class="math notranslate nohighlight">\(Y^*\)</span> so that the probability is conserved</p>
<div class="math notranslate nohighlight">
\[p(X=X^*|I) \delta X = p(Y=Y^*|I) \delta Y.\]</div>
<!-- !split -->
<p>In the limit of infinitesimally small intervals, and with the realization that this should be true for any point <span class="math notranslate nohighlight">\(X\)</span>, we obtain the relationship</p>
<div class="amsmath math notranslate nohighlight" id="equation-a31455bb-7d6d-4cd7-95ad-0ef7b99244a0">
<span class="eqno">(10.2)<a class="headerlink" href="#equation-a31455bb-7d6d-4cd7-95ad-0ef7b99244a0" title="Permalink to this equation">¶</a></span>\[\begin{equation}
p(X|I) = p(Y=Y|I) \left| \frac{dY}{dX} \right|,
\label{eq:transformation}
\end{equation}\]</div>
<p>where the term on the far right is called the <em>Jacobian</em>.</p>
<!-- !split -->
<p>The generalization to several variables, relating the pdf for <span class="math notranslate nohighlight">\(M\)</span> variables <span class="math notranslate nohighlight">\(\{ X_j \}\)</span> in terms of the same number of quantities <span class="math notranslate nohighlight">\(\{ Y_j \}\)</span> related to them, is</p>
<div class="amsmath math notranslate nohighlight" id="equation-d2fe4f07-e4fc-4795-8d28-195cca41c749">
<span class="eqno">(10.3)<a class="headerlink" href="#equation-d2fe4f07-e4fc-4795-8d28-195cca41c749" title="Permalink to this equation">¶</a></span>\[\begin{equation}
p(\{X_j\}|I) = p(\{Y_j\}|I) \left| \frac{\partial (Y_1, Y_2, \ldots, Y_M)}{\partial (X_1, X_2, \ldots, X_M)} \right|,
\label{eq:multivariate-transformation}
\end{equation}\]</div>
<p>where the multivariate Jacobian is given by the determinant of the <span class="math notranslate nohighlight">\(M \times M\)</span> matrix of partial derivatives <span class="math notranslate nohighlight">\(\partial Y_i / \partial X_j\)</span>.</p>
<!-- !split -->
<p><em>Summary.</em>
We have now seen the basic ingredients required for the propagation of errors: it either involves a transformation in the sense of Eq. (<a class="reference external" href="#eq:multivariate-transformation">eq:multivariate-transformation</a>) or an integration as in Eq. (<a class="reference external" href="#eq:marginalization">eq:marginalization</a>).</p>
<!-- !split -->
</div>
<div class="section" id="a-useful-short-cut">
<h3><span class="section-number">10.4.3. </span>A useful short cut<a class="headerlink" href="#a-useful-short-cut" title="Permalink to this headline">¶</a></h3>
<p>For practical purposes, we are often satisfied to approximate pdfs with Gaussians. Within such limits there is an easier method that is often used for error propagation. Note, however, that there are instances when this method fails miserably as will be shown in the example further down.</p>
<p>Suppose that we have summarized the pdfs <span class="math notranslate nohighlight">\(p(X|I)\)</span> and <span class="math notranslate nohighlight">\(p(Y|I)\)</span> as two Gaussians with mean and standard deviation <span class="math notranslate nohighlight">\(x_0, \sigma_x\)</span> and <span class="math notranslate nohighlight">\(y_0, \sigma_y\)</span>, respectively. Assume further that these two variables are not correlated, i.e., <span class="math notranslate nohighlight">\(p(X,Y|I) = p(X|I) p(Y|I)\)</span>.</p>
<!-- !split -->
<p>Suppose now that we are interested in <span class="math notranslate nohighlight">\(Z=X-Y\)</span>. Intuitively, we might guess that the best estimate <span class="math notranslate nohighlight">\(z_0 = x_0 - y_0\)</span>, but the error bar <span class="math notranslate nohighlight">\(\sigma_z\)</span> requires some more thought. Differentiate the relation</p>
<div class="math notranslate nohighlight">
\[\delta Z = \delta X - \delta Y.\]</div>
<p>Square both sides and integrate to get the expectation value</p>
<div class="math notranslate nohighlight">
\[\langle \delta Z^2 \rangle = \langle \delta X^2 + \delta Y^2 - 2 \delta x \delta Y \rangle = \langle \delta X^2 \rangle + \langle  \delta Y^2 \rangle - 2 \langle \delta X \delta Y \rangle,\]</div>
<p>where we have employed the linear property for an integral over a sum of terms.</p>
<!-- !split -->
<p>Since we assumed that the pdfs for <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> were described by independent Gaussians we have</p>
<div class="amsmath math notranslate nohighlight" id="equation-e873e37f-43e5-4d64-ac04-d906d9cf3cb0">
<span class="eqno">(10.4)<a class="headerlink" href="#equation-e873e37f-43e5-4d64-ac04-d906d9cf3cb0" title="Permalink to this equation">¶</a></span>\[\begin{equation}
\langle \delta X^2 \rangle = \sigma_x^2; \qquad \langle \delta Y^2 \rangle = \sigma_y^2; \qquad \langle \delta X \delta Y \rangle = 0,
\label{eq:stddev}
\end{equation}\]</div>
<p>and we find that</p>
<div class="math notranslate nohighlight">
\[\sigma_z = \sqrt{ \langle \delta Z^2 \rangle } = \sqrt{ \sigma_x^2 + \sigma_y^2 }.\]</div>
<!-- !split -->
<p>Consider, as a second example, the ratio of two parameters <span class="math notranslate nohighlight">\(Z = X/Y\)</span>. Differentiation gives</p>
<div class="math notranslate nohighlight">
\[\delta Z = \frac{Y \delta X - X \delta Y}{Y^2} \quad \Leftrightarrow \quad \frac{\delta Z}{Z} = \frac{\delta X}{X} - \frac{\delta Y}{Y}.\]</div>
<p>Squaring both sides and taking the expectation values, we obtain</p>
<div class="math notranslate nohighlight">
\[\frac{\langle \delta Z^2 \rangle}{z_0^2} = \frac{\langle \delta X^2 \rangle}{x_0^2} + \frac{\langle \delta Y^2 \rangle}{y_0^2} - 2 \frac{\langle \delta X \rangle \langle \delta ZY \rangle}{x_0 y_0},\]</div>
<p>where the <span class="math notranslate nohighlight">\(X\)</span>, <span class="math notranslate nohighlight">\(Y\)</span> and <span class="math notranslate nohighlight">\(Z\)</span> in the denominator have been replaced by the constants <span class="math notranslate nohighlight">\(x_0\)</span>, <span class="math notranslate nohighlight">\(y_0\)</span> and <span class="math notranslate nohighlight">\(z_0 = x_0 / y_0\)</span> because we are interested in deviations from the peak of the pdf.</p>
<!-- !split -->
<p>Finally, substituting the information for the pdfs of <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> as summarized in Eq. (<a class="reference external" href="#eq:stddev">eq:stddev</a>) we finally obtain the propagated error for the ratio</p>
<div class="math notranslate nohighlight">
\[\frac{\sigma_z}{z_0} = \sqrt{ \left( \frac{\sigma_x}{x_0} \right)^2 + \left( \frac{\sigma_y}{y_0} \right)^2}.\]</div>
<!-- !split -->
<p>Despite its virtues, let us end our discussion of error-propagation with a salutary warning against the blind use of this nifty short cut.</p>
<!-- !split -->
</div>
<div class="section" id="example-taking-the-square-root-of-a-number">
<h3><span class="section-number">10.4.4. </span>Example: Taking the square root of a number<a class="headerlink" href="#example-taking-the-square-root-of-a-number" title="Permalink to this headline">¶</a></h3>
<p>(Example 3.6.2 in Sivia)</p>
<ul class="simple">
<li><p>Assume that the amplitude of a Bragg peak is measured with an uncertainty <span class="math notranslate nohighlight">\(A = A_0 \pm \sigma_A\)</span> from a least-squares fit to experimental data.</p></li>
<li><p>The Bragg peak amplitude is proportional to the square of a complex structure function: <span class="math notranslate nohighlight">\(A = |F|^2 \equiv f^2\)</span>.</p></li>
<li><p>What is <span class="math notranslate nohighlight">\(f = f_0 \pm \sigma_f\)</span>?</p></li>
</ul>
<p>Obviously, we have that <span class="math notranslate nohighlight">\(f_0 = \sqrt{A_0}\)</span>. Differentiate the relation, square and take the expectation value</p>
<div class="math notranslate nohighlight">
\[\langle \delta A^2 \rangle = 4 f_0^2 \langle \delta f^2 \rangle \quad 
\Leftrightarrow \quad 
\sigma_f = \frac{\sigma_A}{2 \sqrt{A_0}},\]</div>
<p>where we have used the Gaussian approximation for the pdfs.</p>
<p>But what happens if the best fit gives <span class="math notranslate nohighlight">\(A_0 &lt; 0\)</span>, which would not be impossible if we have weak and strongly overlapping peaks. The above equation obviously does not work since <span class="math notranslate nohighlight">\(f_0\)</span> would be a complex number.</p>
<!-- !split -->
<p>We have made two mistakes:</p>
<ol class="simple">
<li><p>Likelihood is not posterior!</p></li>
<li><p>The Gaussian approximation around the peak does not always work.</p></li>
</ol>
<!-- !split -->
<p>Consider first the best fit of the signal peak. It implies that the likelihood can be approximated by</p>
<div class="math notranslate nohighlight">
\[p(D | A, I) \propto \exp \left[ -\frac{(A-A_0)^2}{2\sigma_A^2} \right].\]</div>
<p>However, the posterior for <span class="math notranslate nohighlight">\(A\)</span> is <span class="math notranslate nohighlight">\(p(A|D,I) \propto p(D|A,I) p(A|I)\)</span> and we should use the fact that we know that <span class="math notranslate nohighlight">\(A \ge 0\)</span>.</p>
<!-- !split -->
<p>We will incorporate this information through a simple step-function prior</p>
<div class="math notranslate nohighlight">
\[\begin{split}p(A|I) = \left\{
\begin{array}{ll}
\frac{1}{A_\mathrm{max}}, &amp; 0 \le A \le A_\mathrm{max}, \\
0, &amp; \mathrm{otherwise}.
\end{array}
\right.\end{split}\]</div>
<p>This implies that the posterior will be a truncated Gaussian, and its maximum will always be above zero.</p>
<!-- !split -->
<p>This also implies that we cannot use the Gaussian approximation. Instead we will do the proper calculation using the transformation (<a class="reference external" href="#eq:transformation">eq:transformation</a>)</p>
<div class="math notranslate nohighlight">
\[p(f|D,I) = p(A|D,I) \left| \frac{dA}{df} \right| = 2 f p(A|D,I)\]</div>
<p>In the end we find the proper Bayesian error propagation given by the pdf</p>
<div class="math notranslate nohighlight">
\[\begin{split}p(f|D,I) \propto \left\{
\begin{array}{ll}
f \exp \left[ -\frac{(A-A_0)^2}{2\sigma_A^2} \right], &amp; 0 \le f \le \sqrt{A_\mathrm{max}}, \\
0, &amp; \mathrm{otherwise}.
\end{array}
\right.\end{split}\]</div>
<!-- !split -->
<p>Let us visualize the difference between the Bayesian and the naive error propagation for a few scenarios.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">A_posterior</span><span class="p">(</span><span class="n">A</span><span class="p">,</span><span class="n">A0</span><span class="p">,</span><span class="n">sigA</span><span class="p">):</span>
    <span class="n">pA</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="p">(</span><span class="n">A</span><span class="o">-</span><span class="n">A0</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="o">/</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">sigA</span><span class="o">**</span><span class="mi">2</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">pA</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">pA</span><span class="p">)</span>

<span class="c1"># Wrong analysis</span>
<span class="k">def</span> <span class="nf">f_likelihood</span><span class="p">(</span><span class="n">f</span><span class="p">,</span><span class="n">A0</span><span class="p">,</span><span class="n">sigA</span><span class="p">):</span>
    <span class="n">sigf</span> <span class="o">=</span> <span class="n">sigA</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">A0</span><span class="p">))</span>
    <span class="n">pf</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="p">(</span><span class="n">f</span><span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">A0</span><span class="p">))</span><span class="o">**</span><span class="mi">2</span><span class="o">/</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">sigf</span><span class="o">**</span><span class="mi">2</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">pf</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">pf</span><span class="p">)</span>

<span class="c1"># Correct error propagation</span>
<span class="k">def</span> <span class="nf">f_posterior</span><span class="p">(</span><span class="n">f</span><span class="p">,</span><span class="n">A0</span><span class="p">,</span><span class="n">sigA</span><span class="p">):</span>
    <span class="n">pf</span> <span class="o">=</span> <span class="n">f</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="p">(</span><span class="n">f</span><span class="o">**</span><span class="mi">2</span><span class="o">-</span><span class="n">A0</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="o">/</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">sigA</span><span class="o">**</span><span class="mi">2</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">pf</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">pf</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="p">(</span><span class="n">A0</span><span class="p">,</span><span class="n">sigA</span><span class="p">)</span> <span class="ow">in</span> <span class="p">[(</span><span class="mi">9</span><span class="p">,</span><span class="mi">1</span><span class="p">),(</span><span class="mi">1</span><span class="p">,</span><span class="mi">9</span><span class="p">),(</span><span class="o">-</span><span class="mi">20</span><span class="p">,</span><span class="mi">9</span><span class="p">)]:</span>
    <span class="n">maxA</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">A0</span><span class="p">,</span><span class="mi">3</span><span class="o">*</span><span class="n">sigA</span><span class="p">)</span>
    <span class="n">A_arr</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mf">0.01</span><span class="p">,</span><span class="n">maxA</span><span class="p">)</span>
    <span class="n">f_arr</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">A_arr</span><span class="p">)</span>
    <span class="n">fig</span><span class="p">,</span><span class="n">ax</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>
    <span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">A_arr</span><span class="p">,</span><span class="n">A_posterior</span><span class="p">(</span><span class="n">A_arr</span><span class="p">,</span><span class="n">A0</span><span class="p">,</span><span class="n">sigA</span><span class="p">))</span>
    <span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">f_arr</span><span class="p">,</span><span class="n">f_posterior</span><span class="p">(</span><span class="n">f_arr</span><span class="p">,</span><span class="n">A0</span><span class="p">,</span><span class="n">sigA</span><span class="p">),</span><span class="n">label</span><span class="o">=</span><span class="s1">&#39;Bayesian&#39;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">A0</span><span class="o">&gt;</span><span class="mi">0</span><span class="p">:</span>
        <span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">f_arr</span><span class="p">,</span><span class="n">f_likelihood</span><span class="p">(</span><span class="n">f_arr</span><span class="p">,</span><span class="n">A0</span><span class="p">,</span><span class="n">sigA</span><span class="p">),</span><span class="s1">&#39;--&#39;</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s1">&#39;Naive&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">xlabel</span><span class="o">=</span><span class="s1">&#39;A&#39;</span><span class="p">,</span><span class="n">ylabel</span><span class="o">=</span><span class="s1">&#39;p(A|D,I)&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mf">0.55</span><span class="p">,</span><span class="mf">0.8</span><span class="p">,</span><span class="sa">f</span><span class="s1">&#39;$A=</span><span class="si">{</span><span class="n">A0</span><span class="si">}</span><span class="s1">$, $\sigma_A=</span><span class="si">{</span><span class="n">sigA</span><span class="si">}</span><span class="s1">$&#39;</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">transAxes</span><span class="p">,</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">xlabel</span><span class="o">=</span><span class="s1">&#39;f&#39;</span><span class="p">,</span><span class="n">ylabel</span><span class="o">=</span><span class="s1">&#39;p(f|D,I)&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">&#39;best&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
<!-- !split -->
<!-- <img src="fig/ErrorPropagation/error_square_root_9_1.png" width=700><p><em>The left-hand panels show the posterior pdf for the amplitude of a Bragg peak in three different scenarios. The right-hand plots are the corresponding pdfs for the modulus of the structure factor $f=\sqrt{A}$. The solid lines correspond to a full bayesian error propagation, while the dashed lines are obtained with the short-cut error propagation.</em></p> -->
<p><img alt="The left-hand panels show the posterior pdf for the amplitude of a Bragg peak in three different scenarios. The right-hand plots are the corresponding pdfs for the modulus of the structure factor . The solid lines correspond to a full bayesian error propagation, while the dashed lines are obtained with the short-cut error propagation." src="_images/error_square_root_9_1.png" /></p>
<!-- !split -->
<!-- <img src="fig/ErrorPropagation/error_square_root_1_9.png" width=700> -->
<p><img alt="" src="_images/error_square_root_1_9.png" /></p>
<!-- !split -->
<!-- <img src="fig/ErrorPropagation/error_square_root_-20_9.png" width=700> -->
<p><img alt="" src="_images/error_square_root_-20_9.png" /></p>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
        <div class='prev-next-bottom'>
            
    <a class='left-prev' id="prev-link" href="demo-MCMC.html" title="previous page"><span class="section-number">9. </span>Metropolis-Hasting MCMC sampling of a Poisson distribution</a>
    <a class='right-next' id="next-link" href="demo-ErrorPropagation.html" title="next page"><span class="section-number">11. </span>Learning from Data: Error propagation and nuisance parameters (demonstration)</a>

        </div>
        
        </div>
    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Christian Forssén<br/>
        
            &copy; Copyright 2020.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  
  </body>
</html>