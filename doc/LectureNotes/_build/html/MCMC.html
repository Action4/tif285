
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>8. Why MCMC &#8212; Learning from data</title>
    
  <link href="_static/css/theme.css" rel="stylesheet" />
  <link href="_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="_static/sphinx-book-theme.acff12b8f9c144ce68a297486a2fa670.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="_static/js/index.1c5a1a01449ed65a7b51.js">

    <script id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/togglebutton.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="9. Metropolis-Hasting MCMC sampling of a Poisson distribution" href="demo-MCMC.html" />
    <link rel="prev" title="7. Exercise: Bayesian parameter estimation" href="exercise_BayesianParameterEstimation.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="index.html">
      
      <img src="_static/logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Learning from data</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="welcome.html">
   Course aim
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Introduction
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="Intro.html">
   1. Learning from data: Inference
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="demo-Intro.html">
   2. Reading Data and fitting
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="exercise_Jupyter_Python_intro_01.html">
   4. Python and Jupyter notebooks: part 01
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="exercise_Jupyter_Python_intro_02.html">
   5. Python and Jupyter notebooks: part 02
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Linear regression
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="LinearRegression.html">
   1. Linear regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="ModelValidation.html">
   2. Model validation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="demo-ModelValidation.html">
   4. Linear Regression and Model Validation demonstration
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="exercise_LinearRegression.html">
   5. Linear Regression exercise
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Bayesian statistics
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="BayesianBasics.html">
   1. Statistical inference
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="BayesianRecipe.html">
   2. The Bayesian recipe
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="demo-BayesianBasics.html">
   3. Interactive Bayesian Coin Tossing
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="exercise_sum_product_rule.html">
   4. Checking the sum and product rules, and their consequences
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="BayesianParameterEstimation.html">
   5. Inference With Parametric Models
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="demo-BayesianParameterEstimation.html">
   6. Bayesian parameter estimation demonstration
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="exercise_BayesianParameterEstimation.html">
   7. Exercise: Bayesian parameter estimation
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   8. Why MCMC
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="demo-MCMC.html">
   9. Metropolis-Hasting MCMC sampling of a Poisson distribution
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="ErrorPropagation.html">
   10. Why Bayes is Better
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="demo-ErrorPropagation.html">
   11. Learning from Data: Error propagation and nuisance parameters (demonstration)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="IgnorancePDF.html">
   14. Ignorance pdfs: Indifference and translation groups
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="MaxEnt.html">
   15. The principle of maximum entropy
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="demo-MaxEnt.html">
   16. Assigning probabilities
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="ModelSelection.html">
   17. Model Selection
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Machine learning-A Bayesian perspective
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="GaussianProcesses.html">
   1. Inference using Gaussian processes
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="demo-GaussianProcesses.html">
   2. Gaussian processes demonstration
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="exercise_GP.html">
   3. Exercise: Gaussian processes
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="LogReg.html">
   4. Logistic Regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="NeuralNet.html">
   6. Neural networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="demo-NeuralNet.html">
   7. Neural network classifier demonstration
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="exercises_LogReg_NeuralNet.html">
   8. Exercise: Logistic Regression and neural networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="cnn.html">
   9. Convolutional Neural Networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="demo-cnn.html">
   10. Convolutional Neural Network
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="bnn.html">
   13. Bayesian neural networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="demo-bnn.html">
   14. Variational Inference: Bayesian Neural Networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="exercises_BNN.html">
   15. Exercise: Bayesian neural networks
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="_sources/MCMC.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://physics-chalmers.github.io/tif285/doc/LectureNotes/_build/html/index.html"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://physics-chalmers.github.io/tif285/doc/LectureNotes/_build/html/index.html/issues/new?title=Issue%20on%20page%20%2FMCMC.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        <a class="edit-button" href="https://github.com/html/index.html/edit/master/MCMC.md"><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Edit this page"><i class="fas fa-pencil-alt"></i>suggest edit</button></a>
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#monte-carlo-integration">
   8.1. Monte Carlo integration
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#example-one-dimensional-integration">
     8.1.1. Example: One-dimensional integration
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#slow-convergence">
     8.1.2. Slow convergence
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#markov-chain-monte-carlo">
   8.2. Markov Chain Monte Carlo
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#the-metropolis-hastings-algorithm">
   8.3. The Metropolis Hastings algorithm
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#visualizations-of-mcmc">
   8.4. Visualizations of MCMC
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#challenges-in-mcmc-sampling">
   8.5. Challenges in MCMC sampling
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <!-- !split -->
<div class="section" id="why-mcmc">
<h1><span class="section-number">8. </span>Why MCMC<a class="headerlink" href="#why-mcmc" title="Permalink to this headline">¶</a></h1>
<p>We have been emphasizing that everything is a pdf in the Bayesian approach. In particular, we studied parameter estimation for which we were interested in the posterior pdf of parameters <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span> in a model <span class="math notranslate nohighlight">\(M\)</span> given data <span class="math notranslate nohighlight">\(D\)</span> and other information <span class="math notranslate nohighlight">\(I\)</span></p>
<div class="math notranslate nohighlight">
\[
p(\boldsymbol{\theta} | D, I) \equiv p(\boldsymbol{\theta}).
\]</div>
<!-- !split -->
<p>Suppose that this parametrized model can make predictions for some quantity <span class="math notranslate nohighlight">\(y = f(\boldsymbol{\theta})\)</span> and that we would like to compute the expectation value of this prediction given our knowledge of the model parameters</p>
<div class="math notranslate nohighlight">
\[
\langle f(\boldsymbol{\theta}) \rangle = \int f(\boldsymbol{\theta}) p(\boldsymbol{\theta} | D,I) d \boldsymbol{\theta} \equiv \int g( \boldsymbol{\theta} ) d\boldsymbol{\theta}.
\]</div>
<p>The function <span class="math notranslate nohighlight">\(f(\boldsymbol{\theta})\)</span> might represent the prediction of some new data that was not part of the original data set <span class="math notranslate nohighlight">\(D\)</span> used to constrain the model.</p>
<!-- !split -->
<p>Note that this is more involved than traditional calculations in which we would use a single vector of parameters, e.g. denoted <span class="math notranslate nohighlight">\(\boldsymbol{\theta}^*\)</span>, that we might have found by maximizing a likelihood (minimizing a chi-squared function). Instead, <span class="math notranslate nohighlight">\(\langle f( \boldsymbol{\theta} ) \rangle\)</span> means that we do a multidimensional integral over the full range of possible <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span> values, weighted by the probability density function, <span class="math notranslate nohighlight">\(p(\boldsymbol{\theta} |D,I)\)</span> that we have worked out.</p>
<!-- !split -->
<ul class="simple">
<li><p>This is a lot more work!</p></li>
<li><p>The same sort of multidimensional integrals appear when we want to marginalize over a subset of parameters <span class="math notranslate nohighlight">\(\boldsymbol{\theta}_B\)</span> to find a pdf for the rest, <span class="math notranslate nohighlight">\(\boldsymbol{\theta}_A\)</span>. E.g., when extracting the masses of binary black holes from gravitational-wave signals there are many (nuisance) parameters that characterize, e.g.,   background noise that should be integrated out.</p></li>
<li><p>Therefore, in the Bayesian approach we will frequently encounter these multidimensional integrals. However, conventional methods for low dimensions (Gaussian quadrature or Simpson’s rule) become inadequate rapidly with the increase of dimension.</p></li>
<li><p>In particular, the integrals are problematic because the posterior pdfs are usually very small in much of the integration volume so that the relevant region has a very complicated shape.</p></li>
</ul>
<!-- !split -->
<div class="section" id="monte-carlo-integration">
<h2><span class="section-number">8.1. </span>Monte Carlo integration<a class="headerlink" href="#monte-carlo-integration" title="Permalink to this headline">¶</a></h2>
<p>To approximate such integrals one turns to Monte Carlo (MC) methods. The straight and naive version of MC integration evaluates the integral by randomly distributing <span class="math notranslate nohighlight">\(n\)</span> points in the multidimensional volume <span class="math notranslate nohighlight">\(V\)</span> of possible parameter values <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span>. These points have to cover the regions where <span class="math notranslate nohighlight">\(p( \boldsymbol{\theta} |D,I)\)</span> is significantly different from zero. Then</p>
<!-- !split -->
<div class="math notranslate nohighlight">
\[\langle f( \boldsymbol{\theta} ) \rangle = \int_V g( \boldsymbol{\theta} ) d\boldsymbol{\theta} \approx V \langle g( \boldsymbol{\theta} ) \rangle 
\pm V \sqrt{ \frac{\langle g^2( \boldsymbol{\theta} ) \rangle - \langle g( \boldsymbol{\theta} ) \rangle^2 }{n} },\]</div>
<p>where</p>
<div class="math notranslate nohighlight">
\[\langle g( \boldsymbol{\theta} ) \rangle = \frac{1}{n} \sum_{i=0}^{n-1} g(\boldsymbol{\theta}_i )\]</div>
<div class="math notranslate nohighlight">
\[\langle g^2( \boldsymbol{\theta} ) \rangle = \frac{1}{n} \sum_{i=0}^{n-1} g^2(\boldsymbol{\theta}_i )\]</div>
<!-- !split -->
<div class="section" id="example-one-dimensional-integration">
<h3><span class="section-number">8.1.1. </span>Example: One-dimensional integration<a class="headerlink" href="#example-one-dimensional-integration" title="Permalink to this headline">¶</a></h3>
<p>The average of a function <span class="math notranslate nohighlight">\(g(\theta)\)</span> on <span class="math notranslate nohighlight">\(\theta \in [a,b]\)</span> is</p>
<div class="math notranslate nohighlight">
\[\overline{g(\theta)} = \frac{1}{b-a} \int_a^b g(\theta) d\theta,\]</div>
<p>from calculus. However, we can estimate <span class="math notranslate nohighlight">\(\bar{g(\theta)}\)</span> by averaging over a set of random samples</p>
<div class="math notranslate nohighlight">
\[\overline{g(\theta)} \approx \frac{1}{n} \sum_{i=0}^{n-1} g(\theta_i).\]</div>
<p>Let us consider the integral</p>
<div class="math notranslate nohighlight">
\[\langle f(\theta) \rangle = \int_a^b g(\theta) d\theta \approx 
\frac{b-a}{n} \sum_{i=0}^{n-1} g(\theta_i),\]</div>
<p>where <span class="math notranslate nohighlight">\(b-a\)</span> is the volume <span class="math notranslate nohighlight">\(V\)</span>.</p>
<!-- !split -->
</div>
<div class="section" id="slow-convergence">
<h3><span class="section-number">8.1.2. </span>Slow convergence<a class="headerlink" href="#slow-convergence" title="Permalink to this headline">¶</a></h3>
<p>The main uncertainty lies in assuming that a Gaussian approximation is valid. Note the dependence on <span class="math notranslate nohighlight">\(a/\sqrt{n}\)</span>, which means that you can get a more precise answer by increasing <span class="math notranslate nohighlight">\(n\)</span>. However, the result only gets better very slowly. Each additional decimal point accuracy costs you a factor of 100 in <span class="math notranslate nohighlight">\(n\)</span>.</p>
<!-- !split -->
<p>The key problem is that too much time is wasted in sampling regions where <span class="math notranslate nohighlight">\(p( \boldsymbol{\theta} |D,I )\)</span> is very small. Consider the situation where the significant fraction for one parameter is <span class="math notranslate nohighlight">\(10^{-1}\)</span>. For a <span class="math notranslate nohighlight">\(m\)</span>-parameter problem the significant fraction of the volume is <span class="math notranslate nohighlight">\(10^{-m}\)</span>! This necessitates <em>importance sampling</em> which reweights the integrand to more appropriately distribute points (e.g. the <a class="reference external" href="https://en.wikipedia.org/wiki/VEGAS_algorithm">VEGAS algorithm</a>), but this is difficult to accomplish.</p>
<!-- !split -->
<p>The bottom line is that its not feasible to draw a series of independent random samples from <span class="math notranslate nohighlight">\(p ( \boldsymbol{\theta} | D,I )\)</span> from large <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span> dimensions. Independence means if <span class="math notranslate nohighlight">\(\boldsymbol{\theta}_0, \boldsymbol{\theta}_1, \ldots\)</span> is the series, knowing <span class="math notranslate nohighlight">\(\boldsymbol{\theta}_1\)</span> doesn’t tell us anything about <span class="math notranslate nohighlight">\(\boldsymbol{\theta}_2\)</span>.</p>
<!-- !split -->
<p>However, the samples don’t actually need to be independent. they just need to generate a distribution that is proportional to <span class="math notranslate nohighlight">\(p ( \boldsymbol{\theta} |D,I)\)</span>. E.g., a histogram of the samples should approximate the true distribution.</p>
<!-- !split -->
</div>
</div>
<div class="section" id="markov-chain-monte-carlo">
<h2><span class="section-number">8.2. </span>Markov Chain Monte Carlo<a class="headerlink" href="#markov-chain-monte-carlo" title="Permalink to this headline">¶</a></h2>
<p>A solution is therefore to do a <em>random walk</em> in the parameter space of <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span> so that the probability for being in a region is proportional to <span class="math notranslate nohighlight">\(p( \boldsymbol{\theta} | D,I)\)</span> in that region.</p>
<ul class="simple">
<li><p>The position <span class="math notranslate nohighlight">\(\boldsymbol{\theta}_{i+1}\)</span> follows from <span class="math notranslate nohighlight">\(\boldsymbol{\theta}_i\)</span> by a transition probability (kernel) <span class="math notranslate nohighlight">\(t ( \boldsymbol{\theta}_{i+1} | \boldsymbol{\theta}_i )\)</span>.</p></li>
<li><p>The transition probability is <em>time independent</em>, which means that <span class="math notranslate nohighlight">\(t ( \boldsymbol{\theta}_{i+1} | \boldsymbol{\theta}_i )\)</span> is always the same.</p></li>
</ul>
<p>A sequence of points generated according to these rules is called a <em>Markov Chain</em> and the method is called Markov Chain Monte Carlo (MCMC).</p>
<!-- !split -->
<p>Before describing the most basic implementation of the MCMC, namely the Metropolis and Metropolis-Hastings algorithms, let us list a few state-of-the-art implementations and packages that are available in Python (and often other languages)</p>
<div class="admonition-emcee admonition">
<p class="admonition-title">emcee:</p>
<p><a class="reference external" href="https://emcee.readthedocs.io/en/latest/">emcee</a> is an MIT licensed pure-Python implementation of Goodman &amp; Weare’s <a class="reference external" href="http://msp.berkeley.edu/camcos/2010/5-1/p04.xhtml">Affine Invariant Markov chain Monte Carlo (MCMC) Ensemble sampler</a></p>
</div>
<div class="admonition-pymc3 admonition">
<p class="admonition-title">PyMC3:</p>
<p><a class="reference external" href="https://docs.pymc.io/">PyMC3</a> is a Python package for Bayesian statistical modeling and probabilistic machine learning which focuses on advanced Markov chain Monte Carlo and variational fitting algorithms.</p>
</div>
<div class="admonition-pystan admonition">
<p class="admonition-title">PyStan:</p>
<p><a class="reference external" href="https://pystan.readthedocs.io/en/latest/">PyStan</a> provides an interface to <a class="reference external" href="http://mc-stan.org/">Stan</a>, a package for Bayesian inference using the No-U-Turn sampler, a variant of Hamiltonian Monte Carlo.</p>
</div>
<div class="admonition-pymultinest admonition">
<p class="admonition-title">PyMultiNest:</p>
<p><a class="reference external" href="https://johannesbuchner.github.io/PyMultiNest/">PyMultiNest</a> interacts with <a class="reference external" href="https://github.com/farhanferoz/MultiNest">MultiNest</a>, a Nested Sampling Monte Carlo library.</p>
</div>
<p>We have been using emcee extensively in this course. It is based on ensamble samplers (many MCMC walkers) with affine-invariance. For more details, there is the paper (see above) and some <a class="reference external" href="http://iacs-courses.seas.harvard.edu/courses/am207/blog/lecture-16.html">lecture notes</a></p>
<!-- !split -->
</div>
<div class="section" id="the-metropolis-hastings-algorithm">
<h2><span class="section-number">8.3. </span>The Metropolis Hastings algorithm<a class="headerlink" href="#the-metropolis-hastings-algorithm" title="Permalink to this headline">¶</a></h2>
<p>The basic structure of the Metropolis (and Metropolis-Hastings) algorithm is the following:</p>
<ol class="simple">
<li><p>Initialize the sampling by choosing a starting point <span class="math notranslate nohighlight">\(\boldsymbol{\theta}_0\)</span>.</p></li>
<li><p>Collect samples by repeating the following:</p>
<ol class="simple">
<li><p>Given <span class="math notranslate nohighlight">\(\boldsymbol{\theta}_i\)</span>, <em>propose</em> a new point <span class="math notranslate nohighlight">\(\boldsymbol{\theta}_{i+1}\)</span>, call it <span class="math notranslate nohighlight">\(\boldsymbol{\phi}\)</span>, sampled from a proposal distribution <span class="math notranslate nohighlight">\(q( \boldsymbol{\phi} | \boldsymbol{\theta}_i )\)</span>. This proposal distribution could take many forms. However, for concreteness you can imagine it as a multivariate normal with mean given by <span class="math notranslate nohighlight">\(\boldsymbol{\theta}_i\)</span> and variance <span class="math notranslate nohighlight">\(\boldsymbol{\sigma}^2\)</span>.</p>
<ul class="simple">
<li><p>The transition density will (usually) give a smaller probability for visiting positions that are far from the current position.</p></li>
<li><p>The width <span class="math notranslate nohighlight">\(\boldsymbol{\sigma}\)</span> determines the average step size and is known as the proposal width.</p></li>
</ul>
</li>
<li><p>Compute the Metropolis(-Hastings) ratio <span class="math notranslate nohighlight">\(r\)</span> (defined below). Note that the second ratio is equal to one if the proposal distribution is symmetric. It is then known as the Metropolis algorithm.</p></li>
<li><p>Decide whether or not to accept candidate <span class="math notranslate nohighlight">\(\boldsymbol{\phi}\)</span> for <span class="math notranslate nohighlight">\(\boldsymbol{\theta}_{i+1}\)</span>.</p>
<ul class="simple">
<li><p>If <span class="math notranslate nohighlight">\(r \geq 1\)</span>: accept the proposal position and set <span class="math notranslate nohighlight">\(\boldsymbol{\theta}_{i+1} = \boldsymbol{\phi}\)</span>.</p></li>
<li><p>If <span class="math notranslate nohighlight">\(r &lt; 1\)</span>: accept the position with probability <span class="math notranslate nohighlight">\(r\)</span> (remember that now we have <span class="math notranslate nohighlight">\(0 \leq r &lt; 1\)</span>) by sampling a uniform <span class="math notranslate nohighlight">\(\mathrm{U}(0,1)\)</span> distribution. If <span class="math notranslate nohighlight">\(u \sim \mathrm{U}(0,1) \leq r\)</span>, then <span class="math notranslate nohighlight">\(\boldsymbol{\theta}_{i+1} = \boldsymbol{\phi}\)</span> (accept); else <span class="math notranslate nohighlight">\(\boldsymbol{\theta}_{i+1} = \boldsymbol{\theta}_i\)</span> (reject). Note that the chain always grows (even if the proposed step is rejected in which case you add the current position again.</p></li>
</ul>
</li>
<li><p>Loop until the chain has reached a predetermined length.</p></li>
</ol>
</li>
</ol>
<!-- !split -->
<p>The Metropolis(-Hastings) ratio is</p>
<div class="math notranslate nohighlight">
\[
    
    r = \frac{p( \boldsymbol{\phi} | D,I)}{p( \boldsymbol{\theta}_i | D,I)}
    \frac{q( \boldsymbol{\theta}_i | \boldsymbol{\phi} )}{q( \boldsymbol{\phi} | \boldsymbol{\theta} )}.
    
\]</div>
<ul class="simple">
<li><p>The Metropolis algorithm dates back to the 1950s in physics, but didn’t become widespread in statistics until almost 1980.</p></li>
<li><p>It enabled Bayesian methods to become feasible.</p></li>
<li><p>Note, however, that novadays there are much more sophisticated samplers than the original Metropolis one.</p></li>
</ul>
<!-- !split -->
</div>
<div class="section" id="visualizations-of-mcmc">
<h2><span class="section-number">8.4. </span>Visualizations of MCMC<a class="headerlink" href="#visualizations-of-mcmc" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>There are excellent javascript visualizations of MCMC sampling on the internet.</p></li>
<li><p>A particularly useful set of interactive demos was created by Chi Feng, and is available on the github page: <a class="reference external" href="https://chi-feng.github.io/mcmc-demo/">The Markov-chain Monte Carlo Interactive Gallery</a></p></li>
<li><p>An accessible introduction to MCMC, with simplified versions of Feng’s visualizations, was created by Richard McElreath. It promotes Hamiltonian Monte Carlo and is available in a blog entry called <a class="reference external" href="http://elevanth.org/blog/2017/11/28/build-a-better-markov-chain/">Markov Chains: Why Walk When You Can Flow?</a></p></li>
</ul>
<!-- !split -->
</div>
<div class="section" id="challenges-in-mcmc-sampling">
<h2><span class="section-number">8.5. </span>Challenges in MCMC sampling<a class="headerlink" href="#challenges-in-mcmc-sampling" title="Permalink to this headline">¶</a></h2>
<p>There is much to be written about challenges in performing MCMC sampling and diagnostics that should be made to ascertain that your Markov chain has converged (although it is not really possible to be 100% certain except in special cases.)</p>
<p>We will not focus on these issues here, but just list a few problematic pdfs:</p>
<ul class="simple">
<li><p>Correlated distributions that are very narrow in certain directions. (scaled parameters needed)</p></li>
<li><p>Donut or banana shapes. (very low acceptance ratios)</p></li>
<li><p>Multimodal distributions. (might easily get stuck in local region of high probability and completely miss other regions.)</p></li>
</ul>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
        <div class='prev-next-bottom'>
            
    <a class='left-prev' id="prev-link" href="exercise_BayesianParameterEstimation.html" title="previous page"><span class="section-number">7. </span>Exercise: Bayesian parameter estimation</a>
    <a class='right-next' id="next-link" href="demo-MCMC.html" title="next page"><span class="section-number">9. </span>Metropolis-Hasting MCMC sampling of a Poisson distribution</a>

        </div>
        
        </div>
    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Christian Forssén<br/>
        
            &copy; Copyright 2020.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  
  </body>
</html>