
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>5. Inference With Parametric Models &#8212; Learning from data</title>
    
  <link href="_static/css/theme.css" rel="stylesheet" />
  <link href="_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="_static/sphinx-book-theme.acff12b8f9c144ce68a297486a2fa670.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="_static/js/index.1c5a1a01449ed65a7b51.js">

    <script id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/togglebutton.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="https://unpkg.com/@jupyter-widgets/html-manager@^0.20.0/dist/embed-amd.js"></script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="7. Bayesian parameter estimation demonstration" href="demo-BayesianParameterEstimation.html" />
    <link rel="prev" title="4. Checking the sum and product rules, and their consequences" href="exercise_sum_product_rule.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="index.html">
      
      <img src="_static/logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Learning from data</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="welcome.html">
   Course aim
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Introduction
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="Intro.html">
   1. Learning from data: Inference
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="demo-Intro.html">
   2. Reading Data and fitting
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="exercise_Jupyter_Python_intro_01.html">
   4. Python and Jupyter notebooks: part 01
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="exercise_Jupyter_Python_intro_02.html">
   5. Python and Jupyter notebooks: part 02
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Linear regression
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="LinearRegression.html">
   1. Linear regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="ModelValidation.html">
   2. Model validation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="demo-ModelValidation.html">
   4. Linear Regression and Model Validation demonstration
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="exercise_LinearRegression.html">
   5. Linear Regression exercise
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Bayesian statistics
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="BayesianBasics.html">
   1. Statistical inference
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="BayesianRecipe.html">
   2. The Bayesian recipe
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="demo-BayesianBasics.html">
   3. Interactive Bayesian Coin Tossing
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="exercise_sum_product_rule.html">
   4. Checking the sum and product rules, and their consequences
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   5. Inference With Parametric Models
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="demo-BayesianParameterEstimation.html">
   7. Bayesian parameter estimation demonstration
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="exercise_BayesianParameterEstimation.html">
   8. Exercise: Bayesian parameter estimation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="MCMC.html">
   9. Why MCMC
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="demo-MCMC.html">
   11. Metropolis-Hasting MCMC sampling of a Poisson distribution
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="ErrorPropagation.html">
   12. Why Bayes is Better
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="demo-ErrorPropagation.html">
   13. Learning from Data: Error propagation and nuisance parameters (demonstration)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="MaxEnt.html">
   16. Ignorance pdfs: Indifference and translation groups
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="demo-MaxEnt.html">
   17. Assigning probabilities
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="ModelSelection.html">
   18. Frequentist hypothesis testing
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Machine learning---A Bayesian perspective
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="GaussianProcesses.html">
   1. Inference using Gaussian processes
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="demo-GaussianProcesses.html">
   2. Gaussian processes demonstration
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="exercise_GP.html">
   3. Exercise: Gaussian processes
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="LogReg.html">
   4. Logistic Regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="NeuralNet.html">
   6. Neural networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="demo-NeuralNet.html">
   7. Neural network classifier demonstration
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="exercises_LogReg_NeuralNet.html">
   8. Exercise: Logistic Regression and neural networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="cnn.html">
   9. Convolutional Neural Networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="demo-cnn.html">
   10. Convolutional Neural Network
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="bnn.html">
   13. Bayesian neural networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="demo-bnn.html">
   14. Variational Inference: Bayesian Neural Networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="exercises_BNN.html">
   15. Exercise: Bayesian neural networks
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="_sources/BayesianParameterEstimation.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#">
   5. Inference With Parametric Models
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#parameter-estimation">
     5.1. Parameter estimation
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#bayesian-parameter-estimation">
     5.2. Bayesian parameter estimation
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#example-measured-flux-from-a-star-single-parameter">
   6. Example: Measured flux from a star (single parameter)
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#simple-photon-counts-frequentist-approach">
     6.1. Simple Photon Counts: Frequentist Approach
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#simple-photon-counts-bayesian-approach">
     6.2. Simple Photon Counts: Bayesian Approach
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#a-note-about-priors">
     6.3. A note about priors
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#simple-photon-counts-bayesian-approach-in-practice">
     6.4. Simple Photon Counts: Bayesian approach in practice
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#aside-best-estimates-and-credible-intervals">
     6.5. Aside: Best estimates and credible intervals
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#symmetric-posterior-pdfs">
       6.5.1. Symmetric posterior pdfs
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <p>Copyright 2018-2021, Christian Forssén. Released under CC Attribution-NonCommercial 4.0 license</p>
<!-- !split -->
<!-- <img src="fig/m1m2.png" width=400><p><em>Joint pdf for the masses of two black holes merging obtained from the data analysis of a gravitational wave signal. This representation of a joint pdf is known as a corner plot. <div id="fig:gw"></div></em></p> -->
<p><img alt="Joint pdf for the masses of two black holes merging obtained from the data analysis of a gravitational wave signal. This representation of a joint pdf is known as a corner plot. " src="fig/m1m2.png" /></p>
<!-- !split -->
<div class="section" id="inference-with-parametric-models">
<h1><span class="section-number">5. </span>Inference With Parametric Models<a class="headerlink" href="#inference-with-parametric-models" title="Permalink to this headline">¶</a></h1>
<p>Inductive inference with parametric models is a very important tool in the natural sciences.</p>
<ul class="simple">
<li><p>Consider <span class="math notranslate nohighlight">\(N\)</span> different models <span class="math notranslate nohighlight">\(M_i\)</span> (<span class="math notranslate nohighlight">\(i = 1, \ldots, N\)</span>), each with a parameter vector <span class="math notranslate nohighlight">\(\boldsymbol{\theta}_i\)</span>. The number of parameters (length of <span class="math notranslate nohighlight">\(\boldsymbol{\theta}_i\)</span>) might be different for different models. Each of them implies a sampling distribution for possible data</p></li>
</ul>
<div class="math notranslate nohighlight">
\[p(D|\boldsymbol{\theta}_i, M_i)\]</div>
<ul class="simple">
<li><p>The likelihood function is the pdf of the actual, observed data (<span class="math notranslate nohighlight">\(D_\mathrm{obs}\)</span>) given a set of parameters <span class="math notranslate nohighlight">\(\boldsymbol{\theta}_i\)</span>:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\mathcal{L}_i (\boldsymbol{\theta}_i) \equiv p(D_\mathrm{obs}|\boldsymbol{\theta}_i, M_i)\]</div>
<ul class="simple">
<li><p>We may be uncertain about <span class="math notranslate nohighlight">\(M_i\)</span> (model uncertainty),</p></li>
<li><p>or uncertain about <span class="math notranslate nohighlight">\(\boldsymbol{\theta}_i\)</span> (parameter uncertainty).</p></li>
</ul>
<!-- !split -->
<p>Parameter Estimation:
:<br />
Premise = We have chosen a model (say <span class="math notranslate nohighlight">\(M_1\)</span>)\n
<span class="math notranslate nohighlight">\(\Rightarrow\)</span> What can we say about its parameters <span class="math notranslate nohighlight">\(\boldsymbol{\theta}_1\)</span>?
Model comparison:
:<br />
Premise = We have a set of different models <span class="math notranslate nohighlight">\(\{M_i\}\)</span>\n
<span class="math notranslate nohighlight">\(\Rightarrow\)</span> How do they compare with each other? Do we have evidence to say that, e.g. <span class="math notranslate nohighlight">\(M_1\)</span>, is better than <span class="math notranslate nohighlight">\(M_2\)</span>?
Model adequacy:
:<br />
Premise = We have a model <span class="math notranslate nohighlight">\(M_1\)</span>\n
<span class="math notranslate nohighlight">\(\Rightarrow\)</span> Is <span class="math notranslate nohighlight">\(M_1\)</span> adequate?
Hybrid Uncertainty:
:<br />
Models share some common params: <span class="math notranslate nohighlight">\(\boldsymbol{\theta}_i = \{ \boldsymbol{\varphi}, \boldsymbol{\eta}_i\}\)</span>\n
<span class="math notranslate nohighlight">\(\Rightarrow\)</span> What can we say about <span class="math notranslate nohighlight">\(\boldsymbol{\varphi}\)</span>? (Systematic error is an example)</p>
<!-- !split -->
<div class="section" id="parameter-estimation">
<h2><span class="section-number">5.1. </span>Parameter estimation<a class="headerlink" href="#parameter-estimation" title="Permalink to this headline">¶</a></h2>
<p>Overview comments:</p>
<ul class="simple">
<li><p>In general terms, “parameter estimation” in physics means obtaining values for parameters (constants) that appear in a theoretical model which describes data (exceptions to this general definition exist of course).</p></li>
<li><p>Conventionally this process is known as “parameter fitting” and very often the goal is just to find the “best fit”.</p></li>
<li><p>We will interpret this task from our Bayesian point of view.</p></li>
<li><p>In particular, our ambition is larger as we realize that the strength of inference is best expressed via probabilities (or pdf:s).</p></li>
<li><p>We will also see how familiar ideas like “least-squares optimization” show up from a Bayesian perspective.</p></li>
</ul>
<!-- !split -->
</div>
<div class="section" id="bayesian-parameter-estimation">
<h2><span class="section-number">5.2. </span>Bayesian parameter estimation<a class="headerlink" href="#bayesian-parameter-estimation" title="Permalink to this headline">¶</a></h2>
<p>We will now consider the Bayesian approach to the very important task of model parameter estimation using statistical inference.</p>
<p>Let us first remind ourselves what can go wrong in a fit. We have encountered both <strong>underfitting</strong> (model is not complex enough to describe the variability in the data) and <strong>overfitting</strong> (model tunes to data fluctuations, or terms are underdetermined causing them playing off each other). Bayesian methods can prevent/identify both these situations.</p>
<!-- !split -->
<!-- ===== Example: Measured flux from a star (single parameter) ===== -->
</div>
</div>
<div class="section" id="example-measured-flux-from-a-star-single-parameter">
<h1><span class="section-number">6. </span>Example: Measured flux from a star (single parameter)<a class="headerlink" href="#example-measured-flux-from-a-star-single-parameter" title="Permalink to this headline">¶</a></h1>
<p>Adapted from the blog <a class="reference external" href="http://jakevdp.github.io">Pythonic Perambulations</a> by Jake VanderPlas.</p>
<p>Imagine that we point our telescope to the sky, and observe the light coming from a single star. Our physics model will be that the star’s true flux is constant with time, i.e. that  it has a fixed value <span class="math notranslate nohighlight">\(F_\mathrm{true}\)</span> (we’ll also ignore effects like sky noise and other sources of systematic error). Thus, we have a single model parameter: <span class="math notranslate nohighlight">\(F_\mathrm{true}\)</span>.</p>
<p>We’ll assume that we perform a series of <span class="math notranslate nohighlight">\(N\)</span> measurements with our telescope, where the i:th measurement reports an observed photon flux <span class="math notranslate nohighlight">\(F_i\)</span> and is accompanied by an error model given by <span class="math notranslate nohighlight">\(e_i\)</span><a class="footnote-reference brackets" href="#errors" id="id1">1</a>.
The question is, given this set of measurements <span class="math notranslate nohighlight">\(D = \{F_i\}_{i=0}^{N-1}\)</span>, and the statistical model <span class="math notranslate nohighlight">\(F_i = F_\mathrm{true} + e_i\)</span>, what is our best estimate of the true flux <span class="math notranslate nohighlight">\(F_\mathrm{true}\)</span>?</p>
<p>Because the measurements are number counts, a Poisson distribution is a good approximation to the measurement process:</p>
<p>Now let’s make a simple visualization of the “observed” data, see Fig. <a class="reference external" href="#fig:flux">fig:flux</a>.</p>
<!-- <img src="fig/singlephotoncount_fig_1.png" width=400><p><em>Single photon counts (flux measurements). <div id="fig:flux"></div></em></p> -->
<p><img alt="Single photon counts (flux measurements). " src="fig/singlephotoncount_fig_1.png" /></p>
<p>These measurements each have a different error <span class="math notranslate nohighlight">\(e_i\)</span> which is estimated from Poisson statistics using the standard square-root rule. In this toy example we know the true flux that was used to generate the data, but the question is this: given our measurements and statistical model, what is our best estimate of <span class="math notranslate nohighlight">\(F_\mathrm{true}\)</span>?</p>
<p>Let’s take a look at the frequentist and Bayesian approaches to solving this.</p>
<div class="section" id="simple-photon-counts-frequentist-approach">
<h2><span class="section-number">6.1. </span>Simple Photon Counts: Frequentist Approach<a class="headerlink" href="#simple-photon-counts-frequentist-approach" title="Permalink to this headline">¶</a></h2>
<p>We’ll start with the classical frequentist maximum likelihood approach. Given a single observation <span class="math notranslate nohighlight">\(D_i = F_i\)</span>, we can compute the probability distribution of the measurement given the true flux <span class="math notranslate nohighlight">\(F_\mathrm{true}\)</span> and our assumption of Gaussian errors
$<span class="math notranslate nohighlight">\(
\begin{equation}
p(D_i | F_\mathrm{true}, I) = \frac{1}{\sqrt{2\pi e_i^2}} \exp \left( \frac{-(F_i-F_\mathrm{true})^2}{2e_i^2} \right).
\end{equation}
\)</span><span class="math notranslate nohighlight">\(
This should be read &quot;the probability of \)</span>D_i<span class="math notranslate nohighlight">\( given \)</span>F_\mathrm{true}<span class="math notranslate nohighlight">\(
equals ...&quot;. You should recognize this as a normal distribution with mean \)</span>F_\mathrm{true}<span class="math notranslate nohighlight">\( and standard deviation \)</span>e_i$.</p>
<p>We construct the <em>likelihood function</em> by computing the product of the probabilities for each data point
$<span class="math notranslate nohighlight">\(
\begin{equation}
\mathcal{L}(F_\mathrm{true}) = \prod_{i=1}^N p(D_i | F_\mathrm{true}, I),
\end{equation}
\)</span><span class="math notranslate nohighlight">\(
here \)</span>D = {D_i}$ represents the entire set of measurements. Because the value of the likelihood can become very small, it is often more convenient to instead compute the log-likelihood.</p>
<p><em>Notice.</em>
In the following we will use <span class="math notranslate nohighlight">\(\log\)</span> to denote the natural logarithm. We will write <span class="math notranslate nohighlight">\(\log_{10}\)</span> if we specifically mean the logarithm with base 10.</p>
<p>Combining the previous two equations and computing the log, we have
$<span class="math notranslate nohighlight">\(
\begin{equation}
\log\mathcal{L} = -\frac{1}{2} \sum_{i=1}^N \left[ \log(2\pi e_i^2) +  \frac{(F_i-F_\mathrm{true})^2}{e_i^2} \right].
\end{equation}
\)</span>$</p>
<p>In this approach we will determine <span class="math notranslate nohighlight">\(F_\mathrm{true}\)</span> such that the likelihood is maximized. At this pont we can note that that problem of maximizing the likelihood is equivalent to the minimization of the sum
$<span class="math notranslate nohighlight">\(
\begin{equation}
\sum_{i=1}^N \frac{(F_i-F_\mathrm{true})^2}{e_i^2},
\end{equation}
\)</span>$
which you should recognize as the chi-squared function encountered in the linear regression model.</p>
<p>Therefore, it is not surprising that this particular maximization problem can be solved analytically (i.e. by setting <span class="math notranslate nohighlight">\(d\log\mathcal{L}/d F_\mathrm{true} = 0\)</span>). This results in the following observed estimate of <span class="math notranslate nohighlight">\(F_\mathrm{true}\)</span>
$<span class="math notranslate nohighlight">\(
\begin{equation}
F_\mathrm{est} = \frac{ \sum_{i=1}^N w_i F_i }{ \sum_{i=1}^N w_i}, \quad w_i = 1/e_i^2.
\end{equation}
\)</span><span class="math notranslate nohighlight">\(
Notice that in the special case of all errors \)</span>e_i<span class="math notranslate nohighlight">\( being equal, this reduces to
\)</span><span class="math notranslate nohighlight">\(
\begin{equation}
F_\mathrm{est} = \frac{1}{N} \sum_{i=1} F_i.
\end{equation}
\)</span><span class="math notranslate nohighlight">\(
That is, in agreement with intuition, \)</span>F_\mathrm{est}$ is simply the mean of the observed data when errors are equal.</p>
<p>We can go further and ask what the error of our estimate is. In the frequentist approach, this can be accomplished by fitting a Gaussian approximation to the likelihood curve at maximum; in this simple case this can also be solved analytically (the sum of Gaussians is also a Gaussian). It can be shown that the standard deviation of this Gaussian approximation is <span class="math notranslate nohighlight">\(\sigma_\mathrm{est}\)</span>, which is given by
$<span class="math notranslate nohighlight">\(
\begin{equation}
\frac{ 1 } {\sigma_\mathrm{est}^2} = \sum_{i=1}^N w_i .
\end{equation}
\)</span>$
These results are fairly simple calculations; let’s evaluate them for our toy dataset:</p>
<p><code class="docutils literal notranslate"><span class="pre">F_true</span> <span class="pre">=</span> <span class="pre">1000</span></code> \n
<code class="docutils literal notranslate"><span class="pre">F_est</span> <span class="pre">=</span> <span class="pre">998</span> <span class="pre">+/-</span> <span class="pre">4</span> <span class="pre">(based</span> <span class="pre">on</span> <span class="pre">50</span> <span class="pre">measurements)</span></code> \n</p>
<p>We find that for 50 measurements of the flux, our estimate has an error of about 0.4% and is consistent with the input value.</p>
</div>
<div class="section" id="simple-photon-counts-bayesian-approach">
<h2><span class="section-number">6.2. </span>Simple Photon Counts: Bayesian Approach<a class="headerlink" href="#simple-photon-counts-bayesian-approach" title="Permalink to this headline">¶</a></h2>
<p>The Bayesian approach, as you might expect, begins and ends with probabilities. Our hypothesis is that the star has a constant flux <span class="math notranslate nohighlight">\(F_\mathrm{true}\)</span>. It recognizes that what we fundamentally want to compute is our knowledge of the parameter in question given the data and other information (such as our knowledge of uncertainties for the observed values), i.e. in this case, <span class="math notranslate nohighlight">\(p(F_\mathrm{true} | D,I)\)</span>.
Note that this formulation of the problem is fundamentally contrary to the frequentist philosophy, which says that probabilities have no meaning for model parameters like <span class="math notranslate nohighlight">\(F_\mathrm{true}\)</span>. Nevertheless, within the Bayesian philosophy this is perfectly acceptable.</p>
<p>To compute this pdf, Bayesians next apply Bayes’ Theorem.
If we set the prior <span class="math notranslate nohighlight">\(p(F_\mathrm{true}|I) \propto 1\)</span> (a flat prior), we find
<span class="math notranslate nohighlight">\(p(F_\mathrm{true}|D,I) \propto p(D | F_\mathrm{true},I) \equiv \mathcal{L}(F_\mathrm{true})\)</span>
and the Bayesian probability is maximized at precisely the same value as the frequentist result! So despite the philosophical differences, we see that (for this simple problem at least) the Bayesian and frequentist point estimates are equivalent.</p>
</div>
<div class="section" id="a-note-about-priors">
<h2><span class="section-number">6.3. </span>A note about priors<a class="headerlink" href="#a-note-about-priors" title="Permalink to this headline">¶</a></h2>
<p>The prior allows inclusion of other information into the computation, which becomes very useful in cases where multiple measurement strategies are being combined to constrain a single model. The necessity to specify a prior, however, is one of the more controversial pieces of Bayesian analysis.
A frequentist will point out that the prior is problematic when no true prior information is available. Though it might seem straightforward to use a noninformative prior like the flat prior mentioned above, there are some [surprisingly subtleties](<a class="reference external" href="http://normaldeviate.wordpress.com/2013/07/13/lost-causes-in-statistics-ii-noninformative-">http://normaldeviate.wordpress.com/2013/07/13/lost-causes-in-statistics-ii-noninformative-</a> priors/comment-page-1/) involved. It turns out that in many situations, a truly noninformative prior does not exist! Frequentists point out that the subjective choice of a prior which necessarily biases your result has no place in statistical data analysis.
A Bayesian would counter that frequentism doesn’t solve this problem, but simply skirts the question. Frequentism can often be viewed as simply a special case of the Bayesian approach for some (implicit) choice of the prior: a Bayesian would say that it’s better to make this implicit choice explicit, even if the choice might include some subjectivity.</p>
</div>
<div class="section" id="simple-photon-counts-bayesian-approach-in-practice">
<h2><span class="section-number">6.4. </span>Simple Photon Counts: Bayesian approach in practice<a class="headerlink" href="#simple-photon-counts-bayesian-approach-in-practice" title="Permalink to this headline">¶</a></h2>
<p>Leaving these philosophical debates aside for the time being, let’s address how Bayesian results are generally computed in practice. For a one parameter problem like the one considered here, it’s as simple as computing the posterior probability <span class="math notranslate nohighlight">\(p(F_\mathrm{true} | D,I)\)</span> as a function of <span class="math notranslate nohighlight">\(F_\mathrm{true}\)</span>: this is the distribution reflecting our knowledge of the parameter <span class="math notranslate nohighlight">\(F_\mathrm{true}\)</span>.
But as the dimension of the model grows, this direct approach becomes increasingly intractable. For this reason, Bayesian calculations often depend on sampling methods such as Markov Chain Monte Carlo (MCMC). For this practical example, let us apply an MCMC approach using Dan Foreman-Mackey’s <a class="reference external" href="http://dan.iel.fm/emcee/current/">emcee</a> package. Keep in mind here that the goal is to generate a set of points drawn from the posterior probability distribution, and to use those points to determine the answer we seek.
To perform this MCMC, we start by defining Python functions for the prior <span class="math notranslate nohighlight">\(p(F_\mathrm{true} | I)\)</span>, the likelihood <span class="math notranslate nohighlight">\(p(D | F_\mathrm{true},I)\)</span>, and the posterior <span class="math notranslate nohighlight">\(p(F_\mathrm{true} | D,I)\)</span>, noting that none of these need be properly normalized. Our model here is one-dimensional, but to handle multi-dimensional models we’ll define the model in terms of an array of parameters <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span>, which in this case is <span class="math notranslate nohighlight">\(\boldsymbol{\theta} = [F_\mathrm{true}]\)</span></p>
<p>Now we set up the problem, including generating some random starting guesses for the multiple chains of points.</p>
<p>If this all worked correctly, the array sample should contain a series of 50,000 points drawn from the posterior. Let’s plot them and check. See results in Fig. <a class="reference external" href="#fig:flux-bayesian">fig:flux-bayesian</a>.</p>
<!-- <img src="fig/singlephotoncount_fig_2.png" width=400><p><em>Bayesian posterior pdf (represented by a histogram of MCMC samples) from flux measurements. <div id="fig:flux-bayesian"></div></em></p> -->
<p><img alt="Bayesian posterior pdf (represented by a histogram of MCMC samples) from flux measurements. " src="fig/singlephotoncount_fig_2.png" /></p>
<!-- !split -->
</div>
<div class="section" id="aside-best-estimates-and-credible-intervals">
<h2><span class="section-number">6.5. </span>Aside: Best estimates and credible intervals<a class="headerlink" href="#aside-best-estimates-and-credible-intervals" title="Permalink to this headline">¶</a></h2>
<p>The posterior distribution from our Bayesian data analysis is the key quantity that encodes our inference about the values of the model parameters, given the data and the relevant background information. Often, however, we wish to summarize this result with just a few numbers: the best estimate and a measure of its reliability.</p>
<p>There are a few different options for this. The choice of the most appropriate one depends mainly on the shape of the posterior distribution:</p>
<!-- !split -->
<div class="section" id="symmetric-posterior-pdfs">
<h3><span class="section-number">6.5.1. </span>Symmetric posterior pdfs<a class="headerlink" href="#symmetric-posterior-pdfs" title="Permalink to this headline">¶</a></h3>
<p>Since the probability (density) associated with any particular value of the parameter is a measure of how much we believe that it lies in the neighbourhood of that point, our best estimate is given by the maximum of the posterior pdf. If we denote the quantity of interest by <span class="math notranslate nohighlight">\(\theta\)</span>, with a posterior pdf <span class="math notranslate nohighlight">\(P =p(\theta|D,I)\)</span>, then the best estimate of its value <span class="math notranslate nohighlight">\(\theta_0\)</span> is given by the condition <span class="math notranslate nohighlight">\(dP/d\theta|_{\theta=\theta_0}=0\)</span>. Strictly speaking, we should also check the sign of the second derivative to ensure that <span class="math notranslate nohighlight">\(\theta_0\)</span> represents a maximum.</p>
<p>To obtain a measure of the reliability of this best estimate, we need to look at the width or spread of the posterior pdf about <span class="math notranslate nohighlight">\(\theta_0\)</span>. When considering the behaviour of any function in the neighbourhood of a particular point, it is often helpful to carry out a Taylor series expansion; this is simply a standard tool for (locally) approximating a complicated function by a low-order polynomial. The linear term is zero at the maximum and the quadratic term is often the dominating one determining the width of the posterior pdf. Ignoring all the higher-order terms we arrive at the Gaussian approximation (see more details below)
$<span class="math notranslate nohighlight">\(
\begin{equation}
p(\theta|D,I) \approx \frac{1}{\sigma\sqrt{2\pi}} \exp \left[ -\frac{(\theta-\mu)^2}{2\sigma^2} \right],
\end{equation}
\)</span><span class="math notranslate nohighlight">\(
where the mean \)</span>\mu = \theta_0<span class="math notranslate nohighlight">\( and the variance \)</span>\sigma = \left( - \left. \frac{d^2L}{d\theta^2} \right|_{\theta_0} \right)^{-1/2}<span class="math notranslate nohighlight">\(, where \)</span>L<span class="math notranslate nohighlight">\( is the logarithm of the posterior \)</span>P<span class="math notranslate nohighlight">\(. Our inference about the quantity of interest is conveyed very concisely, therefore, by the 67% Bayesian credible interval \)</span>\theta = \theta_0 \pm \sigma<span class="math notranslate nohighlight">\(, and 
\)</span>$</p>
<p>p(\theta_0-\sigma &lt; \theta &lt; \theta_0+\sigma | D,I) = \int_{\theta_0-\sigma}^{\theta_0+\sigma} p(\theta|D,I) d\theta \approx 0.67.</p>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}&lt;!-- !split --&gt;
#### Asymmetric posterior pdfs\\While the maximum (mode) of the posterior ($\theta_0$) can still be regarded as giving the best estimate, the integrated probability mass is larger on one side of this rather than the other. Alternatively one can compute the mean value, $\langle \theta \rangle = \int \theta p(\theta|D,I) d\theta$, although this tends to overemphasise very long tails. The best option is probably a compromise that can be employed when having access to a large sample from the posterior (as provided by an MCMC), namely to give the median of this ensemble.\\Furthermore, the concept of a single error bar does not seem appropriate in this case, as it implicitly entails the idea of symmetry. A good way of expressing the reliability with which a parameter can be inferred, for an asymmetric posterior pdf, is rather through a *credible interval*. Since the area under the posterior pdf between $\theta_1$ and $\theta_2$ is proportional to how much we believe that $\theta$ lies in that range, the shortest interval that encloses 67% of the area represents a sensible measure of the uncertainty of the estimate. Obviously we can choose to provide some other degree-of-belief that we think is relevant for the case at hand. Assuming that the posterior pdf has been normalized, to have unit area, we need to find $\theta_1$ and $\theta_2$ such that: 
\end{aligned}\end{align} \]</div>
<p>p(\theta_1 &lt; \theta &lt; \theta_2 | D,I) = \int_{\theta_1}^{\theta_2} p(\theta|D,I) d\theta \approx 0.67,</p>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}
where the difference $\theta_2 - \theta_1$ is as small as possible. The region $\theta_1 &lt; \theta &lt; \theta_2$ is then called a 67% credible interval. \\&lt;!-- !split --&gt;
#### Multimodal posterior pdfs\\We can sometimes obtain posteriors which are multimodal; i.e. contains several disconnected regions with large probabilities. There is no difficulty when one of the maxima is very much larger than the others: we can simply ignore the subsidiary solutions, to a good approximation, and concentrate on the global maximum. The problem arises when there are several maxima of comparable magnitude. What do we now mean by a best estimate, and how should we quantify its reliability? The idea of a best estimate and an error-bar, or even a credible interval, is merely an attempt to summarize the posterior with just two or three numbers; sometimes this just can’t be done, and so these concepts are not valid. For the bimodal case we might be able to characterize the posterior in terms of a few numbers: two best estimates and their associated error-bars, or disjoint credible intervals. For a general multimodal pdf, the most honest thing we can do is just display the posterior itself.\\Two options for assigning credible intervals to asymmetric and multimodal pdfs:
* Equal-tailed interval: the probability area above and below the interval are equal.
* Highest posterior density (HPD) interval: The posterior density for any point within the interval is larger than the posterior density for any point outside the interval.\\
&lt;!-- !split --&gt;
#### Different views on credible/confidence intervals\\A Bayesian credible interval, or degree-of-belief (DOB) interval, is the following: Given this data and other information there is $d \%$ probability that this interval contains the true value of the parameter. E.g. a 95% DOB interval implies that the Baysian data analyser would bet 20-to-1 that the true result is inside the interval.\\&lt;!-- !split --&gt;
A frequentist 95% *confidence interval* should be understood as follows: 
&quot;There is a 95% probability that when I compute a confidence interval from data of this sort that he true value of the parameter will fall within the (hypothetical) space of observations&quot;. So the parameter is fixed (no pdf) and the confidence interval is based on random sampling of data. \\Let's try again to understand this: If we make a large number of repeated samples, then 95% of the intervals extracted in this way will include the true value of the parameter.\\
#### Simple Photon Counts: Best estimates and credible intervals\\To compute these numbers for our example, you would run:\\
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~{.Python}
sampper=np.percentile(samples, [2.5, 16.5, 50, 83.5, 97.5],axis=0).flatten()
print(f&quot;&quot;&quot;
F_true = {F_true}
Based on {N} measurements the posterior point estimates are:
...F_est = { np.mean(samples):.0f} +/- { np.std(samples):.0f}
or using credibility intervals:
...F_est = {sampper[2]:.0f}          (posterior median) 
...F_est in [{sampper[1]:.0f}, {sampper[3]:.0f}] (67% credibility interval) 
...F_est in [{sampper[0]:.0f}, {sampper[4]:.0f}] (95% credibility interval) &quot;&quot;&quot;)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\\`F_true = 1000` \n
`Based on 50 measurements the posterior point estimates are:` \n
`...F_est = 998 +/- 4` \n
`or using credibility intervals:` \n
`...F_est = 998          (posterior median)`  \n
`...F_est in [993, 1002] (67% credibility interval)`  \n
`...F_est in [989, 1006] (95% credibility interval)`  \n\\In this particular example, the posterior pdf is actually a Gaussian (since it is constructed as a product of Gaussians), and the mean and variance from the quadratic approximation will agree exactly with the frequentist approach.\\From this final result you might come away with the impression that the Bayesian method is unnecessarily complicated, and in this case it certainly is. Using an MCMC sampler to characterize a one-dimensional normal distribution is a bit like using the Death Star to destroy a beach ball, but we did this here because it demonstrates an approach that can scale to complicated posteriors in many, many dimensions, and can provide nice results in more complicated situations where an analytic likelihood approach is not possible.\\Furthermore, as data, prior information, and models grow in complexity, the two approaches can diverge greatly. \\&lt;!-- !split --&gt;
## Example: Gaussian noise and averages
The example in the demonstration notebook is from Sivia's book. How do we infer the mean and standard deviation from $M$ measurements $D \in \{ x_k \}_{k=0}^{M-1}$ that should be distributed according to a normal distribution $p( D | \mu,\sigma,I)$?\\&lt;!-- !split --&gt;
Start from Bayes theorem
\end{aligned}\end{align} \]</div>
<p>p(\mu,\sigma | D, I) = \frac{p(D|\mu,\sigma,I) p(\mu,\sigma|I)}{p(D|I)}</p>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}
* Remind yourself about the names of the different terms.
* It should become intuitive what the different probabilities (pdfs) describe.
* Bayes theorem tells you how to flip from (hard-to-compute) $p(\mu,\sigma | D, I) \Leftrightarrow p(D|\mu,\sigma,I)$ (easier-to-compute).\\&lt;!-- !split --&gt;
Aside on the denominator, which is known as the &quot;data probability&quot; or &quot;marginalized likelihood&quot; or &quot;evidence&quot;. 
* With $\theta$ denoting a general vector of parameters we must have\end{aligned}\end{align} \]</div>
<p>p(D|I) = \int d\theta p(D|\theta,I) p(\theta|I).</p>
<div class="math notranslate nohighlight">
\[
* This integration (or marginalization) over all parameters is often difficult to perform.
* Fortunately, for **parameter estimation** we don't need $p(D|I)$ since it doesn't depend on $\theta$. We usually only need relative probabilities, or we can determine the normalization $N$ after we have computed the unnormalized posterior \]</div>
<p>p(\theta | D,I) = \frac{1}{N} p(D|\theta,I) p(\theta|I).</p>
<div class="math notranslate nohighlight">
\[&lt;!-- !split --&gt;
If we use a uniform prior $p(\theta | I ) \propto 1$ (in a finite volume), then the posterior is proportional to the **likelihood**
\]</div>
<p>p(\theta | D,I) \propto p(D|\theta,I) = \mathcal{L}(\theta)</p>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}
In this particular situation, the mode of the likelihood (which would correspond to the point estimate of maximum likelihood) is equivalent to the mode of the posterior pdf in the Bayesian analysis.\\&lt;!-- !split --&gt;
The real use of the prior, however, is to include into the analysis any additional information that you might have. The prior statement makes such additional assumptions and information very explicit.\\But how do we actually compute the posterior in practice. Most often we won't be able to get an analytical expression, but we can sample the distribution using a method known as Markov Chain Monte Carlo (MCMC).\\&lt;!-- !split --&gt;
## Example: Fitting a straight line
The next example that we will study is the well known fit of a straight line.\\* Here the theoretical model is\end{aligned}\end{align} \]</div>
<p>y_\mathrm{th}(x; \theta) = m x + b,</p>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}
with parameters $\theta = [b,m]$.\\* The statistical model for the data is\end{aligned}\end{align} \]</div>
<p>y_{\mathrm{exp},i} = y_{\mathrm{th},i} + \delta y_{\mathrm{exp},i},</p>
<div class="math notranslate nohighlight">
\[
where we often assume that the experimental errors are independent and normally distributed so that
\]</div>
<p>y_i = \mathcal{N} \left( y_\mathrm{th}(x_i; \theta), e_i^2 \right).</p>
<div class="math notranslate nohighlight">
\[* Are independent errors always a good approximation?
* An even better statistical model for theoretical models with a quantified, finite resolution would be\]</div>
<p>y_\mathrm{exp} = y_\mathrm{th} + \delta y_\mathrm{exp} + \delta y_\mathrm{th}.</p>
<div class="math notranslate nohighlight">
\[&lt;!-- !split --&gt;
### Linear regression revisited
At this point it is instructive to revisit the linear regression method that we started out with. It corresponds to models that are linear in the parameters such that
\]</div>
<p>y_\mathrm{th} = \sum_{j=0}^{p-1} \theta_j g_j(x),</p>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}
with $p$ parameters and $g_j(x)$ denoting the basis functions.\\With a likelihood as before
\end{aligned}\end{align} \]</div>
<p>p(D|\theta,I) = \prod_{i=0}^{N-1} \exp \left[ -\frac{\left(y_i - y_\mathrm{th}(x_i;\theta) \right)^2}{2\sigma_i^2} \right],</p>
<div class="math notranslate nohighlight">
\[
and assuming a Gaussian prior with a single width $\sigma_\theta$ on the parameters
\]</div>
<p>p(\theta|I) \propto \prod_{j=0}^{p-1} \exp \left[ -\frac{\theta_j^2}{2\sigma_\theta^2} \right].</p>
<div class="math notranslate nohighlight">
\[
We note that the prior can be written $\exp\left( -|\theta|^2 / 2 \sigma_\theta^2\right)$, such that the log (unnormalized) posterior becomes
\]</div>
<p>\log \left[ p(\theta|D,I) \right] = -\frac{1}{2} \left[ \sum_{i=0}^{N-1} \left( \frac{ y_i - y_\mathrm{th}(x_i;\theta)}{\sigma_i}\right)^2 + \frac{|\theta|^2}{\sigma_\theta^2} \right].</p>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}
The mode of the posterior pdf occurs at the minimum of this log-posterior function. You might recognise it as the modified cost function that we introduced in a rather *ad hoc* fashion when implementing linear regression with Ridge regularisation.  From our Bayesian perspective, linear regression with Ridge regularisation corresponds to the maximum a posteriori (MAP) estimate with a Gaussian prior on the parameters.\\
&lt;!-- !split --&gt;
### Why normal distributions?
Let us give a quick motivation why Gaussian distributions show up so often. Say that we have a pdf $p(\theta | D,I)$. Our best estimate from this pdf will be $\theta_0$ where
\end{aligned}\end{align} \]</div>
<p>\left.
\frac{ \partial p }{ \partial \theta }
\right|<em>{\theta_0} = 0, \qquad
\left. \frac{ \partial^2 p }{ \partial \theta^2 }
\right|</em>{\theta_0} &lt; 0.</p>
<div class="math notranslate nohighlight">
\[
The distribution usually varies very rapidly so we study $L(\theta) \equiv \log p$ instead.
Near the peak, it behaves as
\]</div>
<p>L(\theta) = L(\theta_0) + \frac{1}{2} \left. \frac{\partial^2 L}{\partial \theta^2} \right|_{\theta_0} \left( \theta - \theta_0 \right)^2 + \ldots,</p>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}
where the first-order term is zero since we are expanding around a maximum and $\partial L / \partial\theta = 0$.\\&lt;!-- !split --&gt;
If we neglect higher-order terms we find that 
\end{aligned}\end{align} \]</div>
<p>p(\theta|D,I) \approx A \exp \left[ \frac{1}{2} \left. \frac{\partial^2 L}{\partial \theta^2} \right|_{\theta_0} \left( \theta - \theta_0 \right)^2  \right],</p>
<div class="math notranslate nohighlight">
\[
which is a Gaussian $\mathcal{N}(\mu,\sigma^2)$ with
\]</div>
<p>\mu = \theta_0, \qquad \sigma^2 = \left( - \left. \frac{\partial^2 L}{\partial \theta^2} \right|_{\theta_0} \right)^{-1/2}.</p>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}&lt;!-- !split --&gt;
### Correlations
In the &quot;fitting a straight-line&quot; example you should find that the joint pdf for the slope and the intercept $[m, b]$ corresponds to a slanted ellipse. That result implies that the model parameters are **correlated**.\\* Try to understand the correlation that you find in this example.\\Let us explore correlations by studying the behavior of the pdf at the maximum.
A Taylor expansion for a bivariate pdf $p(x,y)$ around the mode $(x_0,y_0)$ gives
\end{aligned}\end{align} \]</div>
<p>p(x,y) \approx p(x_0,y_0) + \frac{1}{2} \begin{pmatrix} x-x_0 &amp; y-y_0 \end{pmatrix}
H
\begin{pmatrix} x-x_0 \ y-y_0 \end{pmatrix},</p>
<div class="math notranslate nohighlight">
\[
where $H$ is the symmetric Hessian matrix
\]</div>
<p>\begin{pmatrix}
A &amp; C \ C &amp; B
\end{pmatrix},</p>
<div class="math notranslate nohighlight">
\[
with elements
\]</div>
<p>A = \left. \frac{\partial^2 p}{\partial x^2} \right|<em>{x_0,y_0}, \quad
B = \left. \frac{\partial^2 p}{\partial y^2} \right|</em>{x_0,y_0}, \quad
C = \left. \frac{\partial^2 p}{\partial x \partial y} \right|_{x_0,y_0}.</p>
<p>$$</p>
<!-- !split -->
<ul class="simple">
<li><p>So in this quadratic approximation the contour is an ellipse centered at <span class="math notranslate nohighlight">\((x_0,y_0)\)</span> with orientation and eccentricity determined by <span class="math notranslate nohighlight">\(A,B,C\)</span>.</p></li>
<li><p>The principal axes are found from the eigenvectors of <span class="math notranslate nohighlight">\(H\)</span>.</p></li>
<li><p>Depending on the skewness of the ellipse, the parameters are either (i) not correlated, (ii) correlated, or (iii) anti-correlated.</p></li>
<li><p>Take a minute to consider what that implies.</p></li>
</ul>
<hr class="footnotes docutils" />
<dl class="footnote brackets">
<dt class="label" id="errors"><span class="brackets"><a class="fn-backref" href="#id1">1</a></span></dt>
<dd><p>We’ll make the reasonable assumption that errors are Gaussian. In a Frequentist perspective, <span class="math notranslate nohighlight">\(e_i\)</span> is the standard deviation of the results of a single measurement event in the limit of repetitions of <em>that event</em>. In the Bayesian perspective, <span class="math notranslate nohighlight">\(e_i\)</span> is the standard deviation of the (Gaussian) probability distribution describing our knowledge of that particular measurement given its observed value.</p>
</dd>
</dl>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
        <div class='prev-next-bottom'>
            
    <a class='left-prev' id="prev-link" href="exercise_sum_product_rule.html" title="previous page"><span class="section-number">4. </span>Checking the sum and product rules, and their consequences</a>
    <a class='right-next' id="next-link" href="demo-BayesianParameterEstimation.html" title="next page"><span class="section-number">7. </span>Bayesian parameter estimation demonstration</a>

        </div>
        
        </div>
    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Christian Forssén<br/>
        
            &copy; Copyright 2020.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  
  </body>
</html>