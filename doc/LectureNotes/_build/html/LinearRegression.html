
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>1. Linear regression &#8212; Learning from data</title>
    
  <link href="_static/css/theme.css" rel="stylesheet" />
  <link href="_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="_static/sphinx-book-theme.acff12b8f9c144ce68a297486a2fa670.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="_static/js/index.1c5a1a01449ed65a7b51.js">

    <script id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/togglebutton.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="https://unpkg.com/@jupyter-widgets/html-manager@^0.20.0/dist/embed-amd.js"></script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="2. Model validation" href="ModelValidation.html" />
    <link rel="prev" title="5. Python and Jupyter notebooks: part 02" href="exercise_Jupyter_Python_intro_02.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="index.html">
      
      <img src="_static/logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Learning from data</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="welcome.html">
   Course aim
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Introduction
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="Intro.html">
   1. Learning from data: Inference
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="demo-Intro.html">
   2. Reading Data and fitting
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="exercise_Jupyter_Python_intro_01.html">
   4. Python and Jupyter notebooks: part 01
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="exercise_Jupyter_Python_intro_02.html">
   5. Python and Jupyter notebooks: part 02
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Linear regression
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   1. Linear regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="ModelValidation.html">
   2. Model validation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="demo-ModelValidation.html">
   4. Linear Regression and Model Validation demonstration
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="exercise_LinearRegression.html">
   5. Linear Regression exercise
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Bayesian statistics
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="BayesianBasics.html">
   1. Statistical inference
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="BayesianRecipe.html">
   2. The Bayesian recipe
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="demo-BayesianBasics.html">
   3. Interactive Bayesian Coin Tossing
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="exercise_sum_product_rule.html">
   4. Checking the sum and product rules, and their consequences
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="BayesianParameterEstimation.html">
   5. Inference With Parametric Models
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="demo-BayesianParameterEstimation.html">
   7. Bayesian parameter estimation demonstration
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="exercise_BayesianParameterEstimation.html">
   8. Exercise: Bayesian parameter estimation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="MCMC.html">
   9. Why MCMC
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="demo-MCMC.html">
   11. Metropolis-Hasting MCMC sampling of a Poisson distribution
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="ErrorPropagation.html">
   12. Why Bayes is Better
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="demo-ErrorPropagation.html">
   13. Learning from Data: Error propagation and nuisance parameters (demonstration)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="MaxEnt.html">
   16. Ignorance pdfs: Indifference and translation groups
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="demo-MaxEnt.html">
   17. Assigning probabilities
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="ModelSelection.html">
   18. Frequentist hypothesis testing
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Machine learning---A Bayesian perspective
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="GaussianProcesses.html">
   1. Inference using Gaussian processes
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="demo-GaussianProcesses.html">
   2. Gaussian processes demonstration
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="exercise_GP.html">
   3. Exercise: Gaussian processes
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="LogReg.html">
   4. Logistic Regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="NeuralNet.html">
   6. Neural networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="demo-NeuralNet.html">
   7. Neural network classifier demonstration
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="exercises_LogReg_NeuralNet.html">
   8. Exercise: Logistic Regression and neural networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="cnn.html">
   9. Convolutional Neural Networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="demo-cnn.html">
   10. Convolutional Neural Network
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="bnn.html">
   13. Bayesian neural networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="demo-bnn.html">
   14. Variational Inference: Bayesian Neural Networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="exercises_BNN.html">
   15. Exercise: Bayesian neural networks
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="_sources/LinearRegression.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#why-linear-regression-aka-ordinary-least-squares">
   1.1. Why Linear Regression (aka Ordinary Least Squares)
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#regression-analysis-overarching-aims">
     1.1.1. Regression analysis, overarching aims
    </a>
   </li>
  </ul>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <p>Copyright 2018-2021, Christian Forssén. Released under CC Attribution-NonCommercial 4.0 license</p>
<!-- !split -->
<div class="section" id="linear-regression">
<h1><span class="section-number">1. </span>Linear regression<a class="headerlink" href="#linear-regression" title="Permalink to this headline">¶</a></h1>
<!-- !split -->
<div class="section" id="why-linear-regression-aka-ordinary-least-squares">
<h2><span class="section-number">1.1. </span>Why Linear Regression (aka Ordinary Least Squares)<a class="headerlink" href="#why-linear-regression-aka-ordinary-least-squares" title="Permalink to this headline">¶</a></h2>
<p>Fitting a continuous function with linear parameterization in terms of the parameters  <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span>.</p>
<ul class="simple">
<li><p>Often used for fitting a continuous function!</p></li>
<li><p>Gives an excellent introduction to central Machine Learning features with <strong>understandable pedagogical</strong> links to other methods like <strong>Neural Networks</strong>, <strong>Support Vector Machines</strong> etc</p></li>
<li><p>Analytical expression for the fitting parameters <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span></p></li>
<li><p>Analytical expressions for statistical propertiers like mean values, variances, confidence intervals and more</p></li>
<li><p>Analytical relation with probabilistic interpretations</p></li>
<li><p>Easy to introduce basic concepts like bias-variance tradeoff, cross-validation, resampling and regularization techniques and many other ML topics</p></li>
<li><p>Easy to code! And links well with classification problems and logistic regression and neural networks</p></li>
<li><p>Allows for <strong>easy</strong> hands-on understanding of gradient descent methods</p></li>
</ul>
<!-- !split -->
<div class="section" id="regression-analysis-overarching-aims">
<h3><span class="section-number">1.1.1. </span>Regression analysis, overarching aims<a class="headerlink" href="#regression-analysis-overarching-aims" title="Permalink to this headline">¶</a></h3>
<p>Regression modeling deals with the description of a <strong>response</strong> variable(s) <span class="math notranslate nohighlight">\(y\)</span> and how it varies as function of some <strong>predictor</strong> variable(s) <span class="math notranslate nohighlight">\(x\)</span>. The first variable is also often called the <strong>dependent</strong>, or the <strong>outcome</strong> variable while the second one can be called the <strong>independent</strong> variable, or the <strong>explanatory</strong> variable. Note also that each of these might be a vector of variables, meaning that there could be more than one response variable and more than one predictor variable.</p>
<p>In general we will try to find a model <span class="math notranslate nohighlight">\(M\)</span> that corresponds to a function <span class="math notranslate nohighlight">\(f_\theta(x)\)</span> such that
$$</p>
<p>y \approx f_\theta(x)</p>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}
In **linear regression** the dependence on the model parameters is **linear**, and this fact will make it possible to find an analytical expression for the optimal set of model parameters (as we will see below).\\&lt;!-- !split --&gt;\\When performing a regression analysis we will have access to a set of data $\mathcal{D}$ that consists of:
* $n$ cases $i = 0, 1, 2, \dots, n-1$ \\For each case there is a
* (vector of) response variable(s) $y_i$ (observations);
* (vector of) independent variable(s) $x_i$.\\Below, we will use boldface to denote the set of data, i.e., $\boldsymbol{y} = (y_0, y_1,\ldots, y_{n-1})$ and $\boldsymbol{x} = (x_0, x_1,\ldots, x_{n-1})$.\\The independent variables can be turned into a number of **features**, and the key to a successful regression analysis is to identify the most relevant features. In physics, these would correspond to a set of **basis functions**.\\Assume that there are $p$ features and we will use the (possibly confusing) notation
* $x_i=[x_{i0}, x_{i1}, \dots, x_{ip-1}]$ and from now on let $x$ denote the vector of features. See below for more explicit examples. \\As our model will (in general) not predict the observations perfectly, we will write the relationship as
\end{aligned}\end{align} \]</div>
<p>y_i = f_\theta(x_i) + \epsilon_i,</p>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}
where $\epsilon_i$ is the error (or the **residual**).\\&lt;!-- !split --&gt;\\A regression analysis aims at finding the model parameters $\theta$ of a specified model $M$ such that the vector of errors $\boldsymbol{\epsilon}$ is minimized. You might ask the very relevant question what is specifically meant by minimizing a vector, and you will find that this is often achieved by minimizing a **cost** function that has been introduced without much motivation. This function might also be called a **loss** function or an **objective** function.\\Alternatively, we could introduce the likelihood function $p(\boldsymbol{y} \vert \boldsymbol{x}, M(\theta))$.  It is the conditional distribution for the probability of making the observations $\boldsymbol{y}$ given the independent variable $\boldsymbol{x}$ and a model $M$, where $\boldsymbol{y}$ and $\boldsymbol{x}$ are contained in our data set $\mathcal{D}$. The parameters $\theta$ that maximizes this likelihood function is then our optimal set. We will later discuss likelihood functions in much more detail.\\Having access to this &quot;optimal&quot; model, we have extracted a relationship between $\boldsymbol{y}$ and $\boldsymbol{x}$ that we can exploit to infer causal dependencies, make predictions, and many other things.\\&lt;!-- !split --&gt;
The $p$ explanatory variables for the $n$ cases in the data set are normally represented by a matrix $\mathbf{X}$. The matrix $\mathbf{X}$ is called the *design
matrix*.\\&lt;!-- !split --&gt;
#### Example: Liquid-drop model for nuclear binding energies\\In order to understand the relation among the predictors $p$, the set of data $\mathcal{D}_n$ and the target (outcome, output etc) $\boldsymbol{y}$,
consider the model we discussed for describing nuclear binding energies. \\There we assumed that we could parametrize the data using a polynomial approximation based on the liquid drop model.
Assuming 
\end{aligned}\end{align} \]</div>
<p>BE(A,N,Z) = a_0+a_1A+a_2A^{2/3}+a_3 Z^2 A^{-1/3}+a_4 (N-Z)^2 A^{-1},</p>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}
we have five features, that is the intercept (constant term, aka bias), the $A$ dependent term, the $A^{2/3}$ term and the $Z^2 A^{-1/3}$ and $(N-Z)^2 A^{-1}$ terms. Although the features are somewhat complicated functions of the independent variables $A,N,Z$, we note that the $p=5$ regression parameters $\theta = (a_0, a_1, a_2, a_3, a_4)$ enter linearly. Furthermore we have $n$ cases. It means that our design matrix is a 
$p\times n$ matrix $\boldsymbol{X}$.\\
&lt;!-- !split --&gt;
### Polynomial basis functions
The perhaps simplest linear-regression approach is to assume we can parametrize our function in terms of a polynomial $f(x)$ of degree $p-1$. I.e.
\end{aligned}\end{align} \]</div>
<p>y(x_i)=f({x}<em>i)+\epsilon_i=\sum</em>{j=0}^{p-1} \theta_j x_i^j+\epsilon_i,</p>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}
where $\epsilon_i$ is the error in our approximation.\\
&lt;!-- !split --&gt;
For every set of values $y_i,x_i$ we have thus the corresponding set of equations
\end{aligned}\end{align} \]</div>
<p>\begin{align*}
y_0&amp;=\theta_0+\theta_1x_0^1+\theta_2x_0^2+\dots+\theta_{p-1}x_0^{p-1}+\epsilon_0\
y_1&amp;=\theta_0+\theta_1x_1^1+\theta_2x_1^2+\dots+\theta_{p-1}x_1^{p-1}+\epsilon_1\
y_2&amp;=\theta_0+\theta_1x_2^1+\theta_2x_2^2+\dots+\theta_{p-1}x_2^{p-1}+\epsilon_2\
\dots &amp; \dots \
y_{n-1}&amp;=\theta_0+\theta_1x_{n-1}^1+\theta_2x_{n-1}^2+\dots+\theta_{p-1}x_{n-1}^{p-1}+\epsilon_{n-1}.\
\end{align*}
$$</p>
<!-- !split -->
<p>Defining the vectors
$$</p>
<p>\boldsymbol{y} = [y_0,y_1, y_2,\dots, y_{n-1}]^T,</p>
<div class="math notranslate nohighlight">
\[
and
\]</div>
<p>\boldsymbol{\theta} = [\theta_0,\theta_1, \theta_2,\dots, \theta_{p-1}]^T,</p>
<div class="math notranslate nohighlight">
\[
and
\]</div>
<p>\boldsymbol{\epsilon} = [\epsilon_0,\epsilon_1, \epsilon_2,\dots, \epsilon_{n-1}]^T,</p>
<div class="math notranslate nohighlight">
\[
and the design matrix
\]</div>
<p>\boldsymbol{X}=
\begin{bmatrix}
1&amp; x_{0}^1 &amp;x_{0}^2&amp; \dots &amp; \dots &amp;x_{0}^{p-1}\
1&amp; x_{1}^1 &amp;x_{1}^2&amp; \dots &amp; \dots &amp;x_{1}^{p-1}\
1&amp; x_{2}^1 &amp;x_{2}^2&amp; \dots &amp; \dots &amp;x_{2}^{p-1}\<br />
\dots&amp; \dots &amp;\dots&amp; \dots &amp; \dots &amp;\dots\
1&amp; x_{n-1}^1 &amp;x_{n-1}^2&amp; \dots &amp; \dots &amp;x_{n-1}^{p-1}\
\end{bmatrix}</p>
<div class="math notranslate nohighlight">
\[
we can rewrite our equations as
\]</div>
<p>\boldsymbol{y} = \boldsymbol{X}\boldsymbol{\theta}+\boldsymbol{\epsilon}.</p>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}
The above design matrix is called a [Vandermonde matrix](https://en.wikipedia.org/wiki/Vandermonde_matrix).\\
&lt;!-- !split --&gt;
#### General basis functions\\
We are obviously not limited to the above polynomial expansions.  We
could replace the various powers of $x$ with elements of Fourier
series or instead of $x_i^j$ we could have $\cos{(j x_i)}$ or $\sin{(j
x_i)}$, or time series or other orthogonal functions.  For every set
of values $y_i,x_i$ we can then generalize the equations to\end{aligned}\end{align} \]</div>
<p>\begin{align*}
y_0&amp;=\theta_0x_{00}+\theta_1x_{01}+\theta_2x_{02}+\dots+\theta_{p-1}x_{0p-1}+\epsilon_0\
y_1&amp;=\theta_0x_{10}+\theta_1x_{11}+\theta_2x_{12}+\dots+\theta_{p-1}x_{1p-1}+\epsilon_1\
y_2&amp;=\theta_0x_{20}+\theta_1x_{21}+\theta_2x_{22}+\dots+\theta_{p-1}x_{2p-1}+\epsilon_2\
\dots &amp; \dots \
y_{i}&amp;=\theta_0x_{i0}+\theta_1x_{i1}+\theta_2x_{i2}+\dots+\theta_{p-1}x_{ip-1}+\epsilon_i\
\dots &amp; \dots \
y_{n-1}&amp;=\theta_0x_{n-1,0}+\theta_1x_{n-1,2}+\theta_2x_{n-1,2}+\dots+\theta_{p-1}x_{n-1,p-1}+\epsilon_{n-1}.\
\end{align*}
$$</p>
<!-- !split -->
<p>We redefine in turn the matrix <span class="math notranslate nohighlight">\(\boldsymbol{X}\)</span> as
$$</p>
<p>\boldsymbol{X}=
\begin{bmatrix}
x_{00}&amp; x_{01} &amp;x_{02}&amp; \dots &amp; \dots &amp;x_{0,p-1}\
x_{10}&amp; x_{11} &amp;x_{12}&amp; \dots &amp; \dots &amp;x_{1,p-1}\
x_{20}&amp; x_{21} &amp;x_{22}&amp; \dots &amp; \dots &amp;x_{2,p-1}\<br />
\dots&amp; \dots &amp;\dots&amp; \dots &amp; \dots &amp;\dots\
x_{n-1,0}&amp; x_{n-1,1} &amp;x_{n-1,2}&amp; \dots &amp; \dots &amp;x_{n-1,p-1}\
\end{bmatrix}</p>
<div class="math notranslate nohighlight">
\[
and without loss of generality we rewrite again our equations as
\]</div>
<p>\boldsymbol{y} = \boldsymbol{X}\boldsymbol{\theta}+\boldsymbol{\epsilon}.</p>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}
The left-hand side of this equation is kwown. The error vector $\boldsymbol{\epsilon}$ and the parameter vector $\boldsymbol{\theta}$ are unknown quantities. How can we obtain the optimal set of $\theta_i$ values?\\
&lt;!-- !split --&gt;
We have defined the matrix $\boldsymbol{X}$ via the equations
\end{aligned}\end{align} \]</div>
<p>\begin{align*}
y_0&amp;=\theta_0x_{00}+\theta_1x_{01}+\theta_2x_{02}+\dots+\theta_{p-1}x_{0p-1}+\epsilon_0\
y_1&amp;=\theta_0x_{10}+\theta_1x_{11}+\theta_2x_{12}+\dots+\theta_{p-1}x_{1p-1}+\epsilon_1\
y_2&amp;=\theta_0x_{20}+\theta_1x_{21}+\theta_2x_{22}+\dots+\theta_{p-1}x_{2p-1}+\epsilon_1\
\dots &amp; \dots \
y_{i}&amp;=\theta_0x_{i0}+\theta_1x_{i1}+\theta_2x_{i2}+\dots+\theta_{p-1}x_{ip-1}+\epsilon_1\
\dots &amp; \dots \
y_{n-1}&amp;=\theta_0x_{n-1,0}+\theta_1x_{n-1,2}+\theta_2x_{n-1,2}+\dots+\theta_{p-1}x_{n-1,p-1}+\epsilon_{n-1}.\
\end{align*}
$$</p>
<p>Note that the design matrix
<span class="math notranslate nohighlight">\(\boldsymbol{X}\in {\mathbb{R}}^{n\times p}\)</span>, with the predictors refering to the column numbers and the entries <span class="math notranslate nohighlight">\(n\)</span> being the row elements.</p>
<!-- !split -->
<p>With the above we use the design matrix to define the approximation <span class="math notranslate nohighlight">\(\boldsymbol{\tilde{y}}\)</span> via the unknown quantity <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span> as
$$</p>
<p>\boldsymbol{\tilde{y}}= \boldsymbol{X}\boldsymbol{\theta},</p>
<div class="math notranslate nohighlight">
\[
and in order to find the optimal parameters $\theta_i$ instead of solving the above linear algebra problem, we define a function which gives a measure of the spread between the values $y_i$ (which represent hopefully the exact values) and the parameterized values $\tilde{y}_i$, namely
\]</div>
<p>C(\boldsymbol{\theta})=\frac{1}{n}\sum_{i=0}^{n-1}\left(y_i-\tilde{y}_i\right)^2=\frac{1}{n}\left{\left(\boldsymbol{y}-\boldsymbol{\tilde{y}}\right)^T\left(\boldsymbol{y}-\boldsymbol{\tilde{y}}\right)\right},</p>
<div class="math notranslate nohighlight">
\[
or using the matrix $\boldsymbol{X}$ and in a more compact matrix-vector notation as
\]</div>
<p>C(\boldsymbol{\theta})=\frac{1}{n}\left{\left(\boldsymbol{y}-\boldsymbol{X}\boldsymbol{\theta}\right)^T\left(\boldsymbol{y}-\boldsymbol{X}\boldsymbol{\theta}\right)\right}.</p>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}
This function is one possible way to define the so-called **cost function**.\\It is also common to define
the cost function as\end{aligned}\end{align} \]</div>
<p>C(\boldsymbol{\theta})=\frac{1}{2n}\sum_{i=0}^{n-1}\left(y_i-\tilde{y}_i\right)^2,</p>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}
since when taking the first derivative with respect to the unknown parameters $\theta$, the factor of $2$ cancels out.\\
&lt;!-- !split --&gt;\\The function 
\end{aligned}\end{align} \]</div>
<p>C(\boldsymbol{\theta})=\frac{1}{n}\left{\left(\boldsymbol{y}-\boldsymbol{X}\boldsymbol{\theta}\right)^T\left(\boldsymbol{y}-\boldsymbol{X}\boldsymbol{\theta}\right)\right},</p>
<div class="math notranslate nohighlight">
\[
can be linked to the variance of the quantity $y_i$ if we interpret the latter as the mean value. 
When linking (see the discussion below) with the maximum likelihood approach, we will indeed interpret $y_i$ as a mean value
\]</div>
<p>y_{i}=\langle y_i \rangle = \theta_0x_{i,0}+\theta_1x_{i,1}+\theta_2x_{i,2}+\dots+\theta_{n-1}x_{i,n-1}+\epsilon_i,</p>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}where $\langle y_i \rangle$ is the mean value. Keep in mind also that
till now we have treated $y_i$ as the exact value. Normally, the
response (dependent or outcome) variable $y_i$ the outcome of a
numerical experiment or another type of experiment and is thus only an
approximation to the true value. It is then always accompanied by an
error estimate, often limited to a statistical error estimate. For now, we
will treat $y_i$ as our exact value for the response variable.\\In order to find the parameters $\theta_i$ we will then minimize the spread of $C(\boldsymbol{\theta})$, that is we are going to solve the problem
\end{aligned}\end{align} \]</div>
<p>{\displaystyle \min_{\boldsymbol{\theta}\in
{\mathbb{R}}^{p}}}\frac{1}{n}\left{\left(\boldsymbol{y}-\boldsymbol{X}\boldsymbol{\theta}\right)^T\left(\boldsymbol{y}-\boldsymbol{X}\boldsymbol{\theta}\right)\right}.</p>
<div class="math notranslate nohighlight">
\[
In practical terms it means we will require
\]</div>
<p>\frac{\partial C(\boldsymbol{\theta})}{\partial \theta_j} = \frac{\partial }{\partial \theta_j}\left[ \frac{1}{n}\sum_{i=0}^{n-1}\left(y_i-\theta_0x_{i,0}-\theta_1x_{i,1}-\theta_2x_{i,2}-\dots-\theta_{n-1}x_{i,n-1}\right)^2\right]=0,</p>
<div class="math notranslate nohighlight">
\[
which results in
\]</div>
<p>\frac{\partial C(\boldsymbol{\theta})}{\partial \theta_j} = -\frac{2}{n}\left[ \sum_{i=0}^{n-1}x_{ij}\left(y_i-\theta_0x_{i,0}-\theta_1x_{i,1}-\theta_2x_{i,2}-\dots-\theta_{n-1}x_{i,n-1}\right)\right]=0,</p>
<div class="math notranslate nohighlight">
\[
or in a matrix-vector form as
\]</div>
<p>\frac{\partial C(\boldsymbol{\theta})}{\partial \boldsymbol{\theta}} = 0 = \boldsymbol{X}^T\left( \boldsymbol{y}-\boldsymbol{X}\boldsymbol{\theta}\right).</p>
<div class="math notranslate nohighlight">
\[
&lt;!-- !split --&gt;
We can rewrite
\]</div>
<p>\frac{\partial C(\boldsymbol{\theta})}{\partial \boldsymbol{\theta}} = 0 = \boldsymbol{X}^T\left( \boldsymbol{y}-\boldsymbol{X}\boldsymbol{\theta}\right),</p>
<div class="math notranslate nohighlight">
\[
as
\]</div>
<p>\boldsymbol{X}^T\boldsymbol{y} = \boldsymbol{X}^T\boldsymbol{X}\boldsymbol{\theta},</p>
<div class="math notranslate nohighlight">
\[
and if the matrix $\boldsymbol{X}^T\boldsymbol{X}$ is invertible we have the solution
\]</div>
<p>\boldsymbol{\theta} =\left(\boldsymbol{X}^T\boldsymbol{X}\right)^{-1}\boldsymbol{X}^T\boldsymbol{y}.</p>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}We note also that since our design matrix is defined as $\boldsymbol{X}\in
{\mathbb{R}}^{n\times p}$, the product $\boldsymbol{X}^T\boldsymbol{X} \in
{\mathbb{R}}^{p\times p}$.  In the liquid drop model example from the Intro lecture, we had $p=5$ ($p \ll n$) meaning that we end up with inverting a small
$5\times 5$ matrix. This is a rather common situation, in many cases we end up with low-dimensional
matrices to invert, which
allow for the usage of direct linear algebra methods such as **LU** decomposition or **Singular Value Decomposition** (SVD) for finding the inverse of the matrix
$\boldsymbol{X}^T\boldsymbol{X}$.\\**Small question**: What kind of problems can we expect when inverting the matrix  $\boldsymbol{X}^T\boldsymbol{X}$?\\&lt;!-- !split --&gt;
### Training scores\\We can easily test our fit by computing various **training scores**. Several such measures are used in machine learning applications. First we have the **Mean-Squared Error** (MSE)
\end{aligned}\end{align} \]</div>
<p>\mathrm{MSE}(\boldsymbol{\theta}) = \frac{1}{n} \sum_{i=1}^n \left( y_{\mathrm{data},i} - y_{\mathrm{model},i}(\boldsymbol{\theta}) \right)^2,</p>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}
where we have $n$ training data and our model is a function of the parameter vector $\boldsymbol{\theta}$.\\Furthermore, we have the **mean absolute error** (MAE) defined as.
\end{aligned}\end{align} \]</div>
<p>\mathrm{MAE}(\boldsymbol{\theta}) = \frac{1}{n} \sum_{i=1}^n \left| y_{\mathrm{data},i} - y_{\mathrm{model},i}(\boldsymbol{\theta}) \right|,</p>
<div class="math notranslate nohighlight">
\[And the $R2$ score, also known as *coefficient of determination* is
\]</div>
<p>\mathrm{R2}(\boldsymbol{\theta}) = 1 - \frac{\sum_{i=1}^n \left( y_{\mathrm{data},i} - y_{\mathrm{model},i}(\boldsymbol{\theta}) \right)^2}{\sum_{i=1}^n \left( y_{\mathrm{data},i} - \bar{y}_\mathrm{model}(\boldsymbol{\theta}) \right)^2},</p>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}
where $\bar{y}_\mathrm{model}(\boldsymbol{\theta}) = \frac{1}{n} \sum_{i=1}^n y_{\mathrm{model},i} (\boldsymbol{\theta})$ is the mean of the model predictions.\\
&lt;!-- !split --&gt;
### The $\chi^2$ function\\Normally, the response (dependent or outcome) variable $y_i$ is the
outcome of a numerical experiment or another type of experiment and is
thus only an approximation to the true value. It is then always
accompanied by an error estimate, often limited to a statistical error
estimate given by a **standard deviation**. \\Introducing the standard deviation $\sigma_i$ for each measurement
$y_i$ (assuming uncorrelated errors), we define the so called $\chi^2$ function as\end{aligned}\end{align} \]</div>
<p>\chi^2(\boldsymbol{\theta})=\frac{1}{n}\sum_{i=0}^{n-1}\frac{\left(y_i-\tilde{y}_i\right)^2}{\sigma_i^2}=\frac{1}{n}\left{\left(\boldsymbol{y}-\boldsymbol{\tilde{y}}\right)^T \boldsymbol{\Sigma}^{-1}\left(\boldsymbol{y}-\boldsymbol{\tilde{y}}\right)\right},</p>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}
where the matrix $\boldsymbol{\Sigma}$ is a diagonal $n \times n$ matrix with $\sigma_i^2$ as matrix elements.\\&lt;!-- !split --&gt;\\In order to find the parameters $\theta_i$ we will then minimize the $\chi^2(\boldsymbol{\theta})$ function by requiring
\end{aligned}\end{align} \]</div>
<p>\frac{\partial \chi^2(\boldsymbol{\theta})}{\partial \theta_j} = \frac{\partial }{\partial \theta_j}\left[ \frac{1}{n}\sum_{i=0}^{n-1}\left(\frac{y_i-\theta_0x_{i,0}-\theta_1x_{i,1}-\theta_2x_{i,2}-\dots-\theta_{n-1}x_{i,n-1}}{\sigma_i}\right)^2\right]=0,</p>
<div class="math notranslate nohighlight">
\[
which results in
\]</div>
<p>\frac{\partial \chi^2(\boldsymbol{\theta})}{\partial \theta_j} = -\frac{2}{n}\left[ \sum_{i=0}^{n-1}\frac{x_{ij}}{\sigma_i}\left(\frac{y_i-\theta_0x_{i,0}-\theta_1x_{i,1}-\theta_2x_{i,2}-\dots-\theta_{n-1}x_{i,n-1}}{\sigma_i}\right)\right]=0,</p>
<div class="math notranslate nohighlight">
\[
or in a matrix-vector form as
\]</div>
<p>\frac{\partial \chi^2(\boldsymbol{\theta})}{\partial \boldsymbol{\theta}} = 0 = \boldsymbol{A}^T\left( \boldsymbol{b}-\boldsymbol{A}\boldsymbol{\theta}\right).</p>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}
where we have defined the matrix $\boldsymbol{A} =\boldsymbol{X} \boldsymbol{\Sigma}^{-1/2}$ with matrix elements $a_{ij} = x_{ij}/\sigma_i$ and the vector $\boldsymbol{b}$ with elements $b_i = y_i/\sigma_i$.\\&lt;!-- !split --&gt;\\We can rewrite
\end{aligned}\end{align} \]</div>
<p>\frac{\partial \chi^2(\boldsymbol{\theta})}{\partial \boldsymbol{\theta}} = 0 = \boldsymbol{A}^T\left( \boldsymbol{b}-\boldsymbol{A}\boldsymbol{\theta}\right),</p>
<div class="math notranslate nohighlight">
\[
as
\]</div>
<p>\boldsymbol{A}^T\boldsymbol{b} = \boldsymbol{A}^T\boldsymbol{A}\boldsymbol{\theta},</p>
<div class="math notranslate nohighlight">
\[
and if the matrix $\boldsymbol{A}^T\boldsymbol{A}$ is invertible we have the solution
\]</div>
<p>\boldsymbol{\theta} =\left(\boldsymbol{A}^T\boldsymbol{A}\right)^{-1}\boldsymbol{A}^T\boldsymbol{b}.</p>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}&lt;!-- !split --&gt;\\If we then introduce the matrix
\end{aligned}\end{align} \]</div>
<p>\boldsymbol{H} =  \left(\boldsymbol{A}^T\boldsymbol{A}\right)^{-1},</p>
<div class="math notranslate nohighlight">
\[
we have then the following expression for the parameters $\theta_j$ (the matrix elements of $\boldsymbol{H}$ are $h_{ij}$)
\]</div>
<p>\theta_j = \sum_{k=0}^{p-1}h_{jk}\sum_{i=0}^{n-1}\frac{y_i}{\sigma_i}\frac{x_{ik}}{\sigma_i} = \sum_{k=0}^{p-1}h_{jk}\sum_{i=0}^{n-1}b_ia_{ik}</p>
<div class="math notranslate nohighlight">
\[
We state without proof the expression for the uncertainty  in the parameters $\theta_j$ as (we leave this as an exercise)
\]</div>
<p>\sigma^2(\theta_j) = \sum_{i=0}^{n-1}\sigma_i^2\left( \frac{\partial \theta_j}{\partial y_i}\right)^2,</p>
<div class="math notranslate nohighlight">
\[
resulting in 
\]</div>
<p>\sigma^2(\theta_j) = \left(\sum_{k=0}^{p-1}h_{jk}\sum_{i=0}^{n-1}a_{ik}\right)\left(\sum_{l=0}^{p-1}h_{jl}\sum_{m=0}^{n-1}a_{ml}\right) = h_{jj}!</p>
<p>$$</p>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
        <div class='prev-next-bottom'>
            
    <a class='left-prev' id="prev-link" href="exercise_Jupyter_Python_intro_02.html" title="previous page"><span class="section-number">5. </span>Python and Jupyter notebooks: part 02</a>
    <a class='right-next' id="next-link" href="ModelValidation.html" title="next page"><span class="section-number">2. </span>Model validation</a>

        </div>
        
        </div>
    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Christian Forssén<br/>
        
            &copy; Copyright 2020.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  
  </body>
</html>