<div class='main_box'>
  <h3>Course literature and extra resources</h3>
  <p>
  	<ol>
		<li>Lecture notes including demonstrations:
		<ul>
	  		<li>
    		Christian Forssén, <a href="https://cforssen.gitlab.io/tif285-book">Learning from data</a>.
    		Published under a <a href="https://creativecommons.org/licenses/by-nc/4.0/">Creative Commons BY-NC</a> license (2021).
      		</li>
      		<li>The book format is powered by <a href="https://jupyterbook.org/">Jupyter Book</a>.</li>
      		<li>The static html book version is not interactive. However, the source files are available in the git repository. It is recommended to download all the course material using git clone (and to use git pull for possible updates). Alternatively, you can download single notebooks via the jupyter book by clicking on the download button in the upper right corner.</li>
		</ul>
		</li>
   		<li>Course textbook:
		<ul>
	  		<li>
    		Phil Gregory, 
    		<a href="https://doi-org.proxy.lib.chalmers.se/10.1017/CBO9780511791277">
    		<i>"Bayesian Logical Data Analysis for the Physical Sciences"</i></a>, 
    		Cambridge University Press (2005).
      		</li>
      		<li>An electronic version of the book is available via Chalmers library (login required)</li>
		</ul>
		</li>
   		<li>Additional reading: 
		<ul>
	  		<li>
    		David J.C. MacKay, 
    		<a href="http://www.inference.org.uk/mackay/itila/">
    		<i>"Information Theory, Inference, and Learning Algorithms"</i></a>,
    		Cambridge University Press (2005).
      		</li>
	  		<li>
    		D.S. Sivia, 
    		<a href="http://search.ebscohost.com/login.aspx?direct=true&db=cat07472a&AN=clec.EBC430582&site=eds-live&scope=site">
    		<i>"Data Analysis : A Bayesian Tutorial"</i></a>,
    		Oxford University Press (2006).
      		</li>
	  		<li>
    		Trevor Hastie, Robert Tibshirani, Jerome H. Friedman, 
    		<a href="http://search.ebscohost.com/login.aspx?direct=true&db=cat07472a&AN=clec.SPRINGERLINK9780387848587&site=eds-live&scope=site">
    		<i>"The Elements of Statistical Learning"</i></a>,
    		Springer (2009).
      		</li>
	  		<li>
    		Andrew Gelman et al., 
    		<a href="http://search.ebscohost.com/login.aspx?direct=true&db=cat06296a&AN=clc.b1573576&site=eds-live&scope=site">
    		<i>"Bayesian Data Analysis"</i></a>,
    		CRC Press, 3rd edition (2014). e-book not available.
      		</li>
	  		<li>
    		 Aurelien Geron,
    		<a href="http://search.ebscohost.com/login.aspx?direct=true&db=cat07472a&AN=clec.EBC4822582&site=eds-live&scope=site">
    		<i>"Hands‑On Machine Learning with Scikit‑Learn and TensorFlow"</i></a>,
    		O'Reilly (2017).
      		</li>
		</ul>
    	Electronic versions of some of these books are available via the author or via Chalmers library
    	(login required). See links above.
		</li>	
	</ol>
	</p>
</div>

<div class='main_box'>
  <h3>Weekly schedule</h3>
	<p>
  Themes:
  <ul>
    <li><a href="#lv1" target="_self">Week 1 - Introduction, linear regression</a></li>
	<li><a href="#lv2" target="_self">Week 2 - Model validation; Probability theory and Bayesian statistics</a></li>
	<li><a href="#lv3" target="_self">Week 3 - Bayesian parameter estimation</a></li>
	<li><a href="#lv4" target="_self">Week 4 - Markov Chain Monte Carlo; Why Bayes is better</a></li>
	<li><a href="#lv5" target="_self">Week 5 - Priors; Model selection</a></li> 
	<li><a href="#lv6" target="_self">Week 6 - Gaussian processes</a></li>
	<li><a href="#lv7" target="_self">Week 7 - Classification, Neural networks</a></li>
	<li><a href="#lv8" target="_self">Week 8 - Bayesian neural networks, Deep neural networks</a></li>
  </ul>
	The course planning is preliminary. Some topics might be
  shifted around, and we might find that we need to spend more time in-class on certain parts.
	</p>
</div>

<!-- ================ Week 1 ================ -->
<div class='main_box'>
  <hr/>
  <h4><a name="lv1">Week 1 - Introduction, linear regression</a></h4>

    <ul>
    <li>Introduction (lecture notes)</li>
    <li>Getting started (demonstration)</li>
    <li>Linear regression (lecture notes)</li>
    <li>Introduction to problem set 1</li>
    <li>Extra reading: Geron ch. 4; Hastie ch. 3</li>
  </ul>
 
  <h4>Computer lab</h4> 
  <ul>
  	<li>Demonstration Monday (Christian): Getting started.</li>
    <li>    
    <script>
	document.write('<a href="' + gettingstarted + '">Getting started</a>');
	</script>
	<noscript>
		Getting started
	</noscript>
	instructions.
	</li>
  	<li>Demonstration Thursday (Oliver): Introduction to problem set 1.</li>
    <li>Exercise: Jupyter/Python/NumPy intro</li> 
    <li>Exercise: Linear Regression</li>
    <li>Problem set 1</li>
  </ul>
</div>

<!-- ================ Week 2 ================ -->
<div class='main_box'>
  <hr/>
  <h4><a name="lv2">Week 2 - Model validation; Probability theory and Bayesian statistics</a></h4>

    <ul>
    <li>Linear regression (lecture notes)</li>
    <li>Model validation (lecture notes)</li>
    <li>Probability theory and Bayesian statistics (Gregory chs 1,2)</li>
    <li>Extra reading: Geron ch. 4; Hastie ch. 3</li>
    <li>Extra reading: Sivia ch. 1</li>
  </ul>
 
  <h4>Computer lab</h4> 
  <ul>
  	<li>Demonstration (Isak): Introduction to project 1.</li>
    <li>Exercise: Linear Regression</li>
    <li>Exercise: Probability sum and product rules</li>
    <li>Problem set 1</li>
    <li>Project 1</li>
  </ul>
</div>

<!-- ================ Week 3 ================ -->
<div class='main_box'>
  <hr/>
  <h4><a name="lv3">Week 3 - Bayesian parameter estimation</a></h4>

    <ul>
    <li>Parameter estimation (lecture notes; Gregory ch 3)</li>
    <li>Extra reading: Sivia chs. 2,3</li>
    <li>Extra reading: MacKay ch. 3</li>
  </ul>
 
  <h4>Computer lab</h4> 
  <ul>
  	<li>Demonstration (Noemi): Introduction to problem set 2.</li>
    <li>Exercise: Parameter Estimation</li>
    <li>Problem set 2</li>
    <li>Project 1</li>
  </ul>
</div>

<!-- ================ Week 4 ================ -->
<div class='main_box'>
  <hr/>
  <h4><a name="lv4">Week 4 - Markov Chain Monte Carlo; Error propagation</a></h4>

    <ul>
    <li>MCMC (lecture notes; Gregory ch 12)</li>
    <li>Error propagation and nuisance parameters (lecture notes; Gregory chs 3, 9)</li>
    <li>Extra reading: Sivia chs. 3, 4</li>
    <li>Extra reading: MacKay ch. 29</li>
  </ul>
 
  <h4>Computer lab</h4> 
  <ul>
  	<li>Demonstration (Christian): Feedback on problem set 1.</li>
    <li>Exercise: MCMC</li>
    <li>Problem set 2</li>
    <li>Project 1</li>
  </ul>
</div>

<!-- ================ Week 5 ================ -->
<div class='main_box'>
  <hr/>
  <h4><a name="lv5">Week 5 - Priors; Hypothesis testing and Bayesian model selection</a></h4>

    <ul>
    <li>Assigning probabilities (lecture notes; Gregory chs 4, 8)</li>
    <li>Hypothesis testing and Bayesian model selection (lecture notes; Gregory chs 3, 7)</li>
    <li>Extra reading: Sivia chs. 4, 5</li>
    <li>Extra reading: MacKay chs. 3, 28</li>
  </ul>
 
  <h4>Computer lab</h4> 
  <ul>
  	<li>Demonstration (Noemi): Feedback on problem set 2.</li>
    <li>Exercise: Assigning probabilities (follow the lecture demonstration)</li>
    <li>Project 1</li>
  </ul>
</div>


<!-- ================ Week 6 ================ -->
<div class='main_box'>
  <hr/>
  <h4><a name="lv6">Week 6 - Gaussian processes</a></h4>

    <ul>
    <li>Gaussian processes, part 1 (lecture notes; MacKay ch 45)</li>
    <li>Gaussian processes, part 2 (lecture notes; MacKay ch 45)</li>
    <li>Extra reading: </li>
  </ul>
 
  <h4>Computer lab</h4> 
  <ul>
  	<li>Demonstration (Isak): Feedback on project 1.</li>
  	<li>Demonstration (Oliver): Introduction to problem set 3.</li>
    <li>Exercise: Gaussian processes</li>
    <li>Problem set 3</li>
  </ul>
</div>

<!-- ================ Week 7 ================ -->
<div class='main_box'>
  <hr/>
  <h4><a name="lv7">Week 7 - Classification, Neural networks</a></h4>

    <ul>
    <li>Classification, logistic regression (lecture notes; MacKay chs 38,39)</li>
    <li>Neural networks (lecture notes; MacKay chs 38,39)</li>
    <li>Extra reading: </li>
  </ul>
 
  <h4>Computer lab</h4> 
  <ul>
  	<li>Demonstration (Shahnawaz): Introduction to project 2.</li>
    <li>Exercise: Neural networks</li>
    <li>Problem set 3</li>
    <li>Project 2</li>
  </ul>
</div>

<!-- ================ Week 8 ================ -->
<div class='main_box'>
  <hr/>
  <h4><a name="lv8">Week 8 - Bayesian neural networks, Deep neural networks</a></h4>

    <ul>
    <li>Bayesian neural networks (lecture notes; MacKay ch 41)</li>
    <li>Deep neural networks (lecture notes; MacKay ch 44)</li>
  </ul>
 
  <h4>Computer lab</h4> 
  <ul>
  	<li>Demonstration: TBD.</li>
    <li>Project 2</li>
  </ul>
</div>
