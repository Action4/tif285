<div class="main_box">
    <h3>Course literature and extra resources</h3>
    <p>&nbsp;</p>
    <ol>
        <li>Lecture notes including demonstrations:
            <ul>
                <li>Christian Forss&eacute;n, <a href="https://cforssen.gitlab.io/tif285-book">Learning from data</a>. Published under a <a href="https://creativecommons.org/licenses/by-nc/4.0/">Creative Commons BY-NC</a> license (2021).</li>
                <li>The book format is powered by <a href="https://jupyterbook.org/">Jupyter Book</a>.</li>
                <li>The static html book version is not interactive. However, the source files are available in the git repository. It is recommended to download all the course material using git clone (and to use git pull for possible updates). Alternatively, you can download single notebooks via the jupyter book by clicking on the download button in the upper right corner.</li>
            </ul>
        </li>
        <li>Additional reading:
            <ul>
                <li>Phil Gregory, <a href="https://doi-org.proxy.lib.chalmers.se/10.1017/CBO9780511791277"> <i>"Bayesian Logical Data Analysis for the Physical Sciences"</i></a>, Cambridge University Press (2005).</li>
                <li>David J.C. MacKay, <a href="http://www.inference.org.uk/mackay/itila/"> <i>"Information Theory, Inference, and Learning Algorithms"</i></a>, Cambridge University Press (2005).</li>
                <li>D.S. Sivia, <a href="http://search.ebscohost.com/login.aspx?direct=true&amp;db=cat07472a&amp;AN=clec.EBC430582&amp;site=eds-live&amp;scope=site"> <i>"Data Analysis : A Bayesian Tutorial"</i></a>, Oxford University Press (2006).</li>
                <li>Trevor Hastie, Robert Tibshirani, Jerome H. Friedman, <a href="http://search.ebscohost.com/login.aspx?direct=true&amp;db=cat07472a&amp;AN=clec.SPRINGERLINK9780387848587&amp;site=eds-live&amp;scope=site"> <i>"The Elements of Statistical Learning"</i></a>, Springer (2009).</li>
                <li>Andrew Gelman et al., <a href="http://search.ebscohost.com/login.aspx?direct=true&amp;db=cat06296a&amp;AN=clc.b1573576&amp;site=eds-live&amp;scope=site"> <i>"Bayesian Data Analysis"</i></a>, CRC Press, 3rd edition (2014). e-book not available.</li>
                <li>Aurelien Geron, <a href="http://search.ebscohost.com/login.aspx?direct=true&amp;db=cat07472a&amp;AN=clec.EBC4822582&amp;site=eds-live&amp;scope=site"> <i>"Hands‑On Machine Learning with Scikit‑Learn and TensorFlow"</i></a>, O'Reilly (2017).</li>
            </ul>
            Electronic versions of some of these books are available via the author or via Chalmers library (login required). See links above.
        </li>
    </ol>
    <p>&nbsp;</p>
</div>
<div class="main_box">
    <h3>Weekly schedule</h3>
    <p>Week-by-week themes and important dates:</p>
    <ul>
        <li><a href="#lv1" target="_self">Week 1 - Introduction, linear and nonlinear models, model optimization</a></li>
        <li><a href="#lv2" target="_self">Week 2 - Probability theory and Bayesian statistics, model discrepancy</a>
            <ul>
                <li>Deadline: Problem set 1 (Friday)</li>
            </ul>
        </li>
        <li><a href="#lv3" target="_self">Week 3 - Bayesian parameter estimation and workflow</a>
            <ul>
                <li>Face-to-face discussions: Problem set 1 (Monday 15-17)</li>
            </ul>
        </li>
        <li><a href="#lv4" target="_self">Week 4 - Markov Chain Monte Carlo</a>
            <ul>
                <li>Lecture: Ethics and data analysis (Tuesday 15-17)</li>
                <li>Deadline: Problem set 2 (Friday)</li>
            </ul>
        </li>
        <li><a href="#lv5" target="_self">Week 5 - Assigning probabilities; Model selection</a>
            <ul>
                <li>Face-to-face discussions: Problem set 2 (Monday 15-17)</li>
                <li>Seminar: Ethics and data analysis (Tuesday 15-17); Compulsory for Chalmers TIF285</li>
            </ul>
        </li>
        <ul>
            <li>Deadline: Project 1 (Friday)</li>
        </ul>
        <li><a href="#lv6" target="_self">Week 6 - Gaussian processes</a></li>
        <li><a href="#lv7" target="_self">Week 7 - Neural networks</a>
            <ul>
                <li>Deadline: Problem set 3 (Friday)</li>
            </ul>
        </li>
        <li><a href="#lv8" target="_self">Week 8 - Bayesian neural networks</a>
            <ul>
                <li>Face-to-face discussions: Problem set 3 (Monday 15-17)</li>
            </ul>
        </li>
        <li>Exam week
            <ul>
                <li>Deadline: Project 2 (Wednesday)</li>
            </ul>
        </li>
    </ul>
</div>
<div class="main_box">
    <hr />The course planning is preliminary. Some topics might be shifted around. Sections in the lecture notes that are listed as "Recommended reading" will not be covered at lectures, but might provide relevant background.
    <hr />
    <h4><a name="lv1"></a>Week 1 - Introduction, linear and nonlinear models, model optimization</h4>
    <ul>
        <li>Introduction (lecture notes)</li>
        <li>Getting started (demonstration)</li>
        <li>Overview of modeling (lecture notes)</li>
        <li>Linear models (lecture notes)</li>
        <li>Mathematical optimization (lecture notes)</li>
        <li>Introduction to problem set 1</li>
        <li>Extra reading: Geron ch. 4; Hastie ch. 3</li>
    </ul>
    <h4>Computer lab</h4>
    <ul>
        <li>Demonstration Monday: Getting started.</li>
        <li>Demonstration Thursday: Introduction to problem set 1.</li>
        <li>Exercise: Jupyter/Python/NumPy intro</li>
        <li>Problem set 1</li>
    </ul>
</div>
<div class="main_box">
    <hr />
    <h4><a name="lv2"></a>Week 2 - Probability theory and Bayesian statistics, model discrepancy</h4>
    <ul>
        <li>Linear models (lecture notes) and confidence intervals</li>
        <li>Data likelihood (lecture notes, ch. 20.2)</li>
        <li>Bayes' theorem (lecture notes)</li>
        <li>Recommended reading: Statistics concepts and notation (lecture notes)</li>
        <li>Extra reading: Sivia ch. 1</li>
        <li>Extra reading: Gregory chs. 5, 6</li>
    </ul>
    <h4>Computer lab</h4>
    <ul>
        <li>Demonstration: Introduction to project 1.</li>
        <li>Exercise: Probability sum and product rules</li>
        <li>Problem set 1</li>
        <li>Project 1</li>
    </ul>
</div>
<div class="main_box">
    <hr />
    <h4><a name="lv3"></a>Week 3 - Bayesian parameter estimation and workflow</h4>
    <ul>
        <li>Data, models, and predictions (lecture notes)</li>
        <li>Advantages of the Bayesian approach (lecture notes; we will not cover Secs. 19.3 and 19.4.)</li>
        <li>Demonstration: Bayesian parameter estimation</li>
        <li>Recommended reading: Bayesian Linear Regression (lecture notes)</li>
        <li>Recommended reading: Bayesian research workflow (lecture notes)</li>
        <li>Extra reading: Gregory ch. 3</li>
        <li>Extra reading: Sivia chs. 2, 3</li>
        <li>Extra reading: MacKay ch. 3</li>
    </ul>
    <h4>Computer lab</h4>
    <ul>
        <li>Demonstration: Introduction to problem set 2.</li>
        <li>Problem set 2</li>
        <li>Project 1</li>
    </ul>
</div>
<div class="main_box">
    <hr />
    <h4><a name="lv4"></a>Week 4 - Markov Chain Monte Carlo</h4>
    <ul>
        <li>Markov chain Monte Carlo sampling (lecture notes)</li>
        <li>Advanced Markov chain Monte Carlo sampling (lecture notes; we will focus on MCMC convergence tests.)</li>
        <li>Recommended reading: Stochastic processes (lecture notes)</li>
        <li>Recommended reading: Markov chains (lecture notes)</li>
        <li>Extra reading: Gregory ch. 12</li>
        <li>Extra reading: Sivia chs. 3, 4</li>
        <li>Extra reading: MacKay ch. 29</li>
    </ul>
    <h4>Computer lab</h4>
    <ul>
        <li>Demonstration: Feedback on problem set 1.</li>
        <li>Go through the Demonstration: Metropolis-Hasting MCMC sampling of a Poisson distribution (lecture notes)</li>
        <li>Problem set 2</li>
        <li>Project 1</li>
    </ul>
</div>
<div class="main_box">
    <hr />
    <h4><a name="lv5"></a>Week 5 - Assigning probabilities; model selection</h4>
    <ul>
        <li>Assigning probabilities (I) (lecture notes)</li>
        <li>Assigning probabilities (II) (lecture notes)</li>
        <li>Model Selection (lecture notes)</li>
        <li>Extra reading: Gregory chs. 4, 8 and 3, 7</li>
        <li>Extra reading: Sivia chs. 4, 5</li>
        <li>Extra reading: MacKay chs. 3, 28</li>
    </ul>
    <h4>Computer lab</h4>
    <ul>
        <li>Demonstration: Feedback on problem set 2.</li>
        <li>Go through the Demonstration: Assigning probabilities (lecture notes)</li>
        <li>Project 1</li>
    </ul>
</div>
<div class="main_box">
    <hr />
    <h4><a name="lv6"></a>Week 6 - Gaussian processes</h4>
    <ul>
        <li>Machine learning: Overview and notation (lecture notes)</li>
        <li>Gaussian processes (lecture notes)</li>
        <li>Extra reading: MacKay ch. 45</li>
    </ul>
    <h4>Computer lab</h4>
    <ul>
        <li>Demonstration: Feedback on project 1.</li>
        <li>Demonstration: Introduction to problem set 3.</li>
        <li>Go through the Demonstration: Gaussian processes (lecture notes)</li>
        <li>Problem set 3</li>
    </ul>
</div>
<div class="main_box">
    <hr />
    <h4><a name="lv7"></a>Week 7 -Neural networks</h4>
    <ul>
        <li>Model validation, Logistic regression (lecture notes; )</li>
        <li>Neural networks (lecture notes)</li>
        <li>Demonstration: Neural networks with Tensorflow</li>
        <li>Extra reading: MacKay chs. 38, 39, 44</li>
    </ul>
    <h4>Computer lab</h4>
    <ul>
        <li>Demonstration: Introduction to project 2.</li>
        <li>Exercise: Logistic Regression and neural networks</li>
        <li>Problem set 3</li>
        <li>Project 2</li>
    </ul>
</div>
<div class="main_box">
    <hr />
    <h4><a name="lv8"></a>Week 8 - Bayesian neural networks</h4>
    <ul>
        <li>Bayesian neural networks (lecture notes)</li>
        <li>Variational inference (preliminary)</li>
        <li>Extra reading: MacKay chs. 33, 41</li>
    </ul>
    <h4>Computer lab</h4>
    <ul>
        <li>Demonstration: Not planned.</li>
        <li>Project 2</li>
    </ul>
</div>